
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <title>Agent-Based Models</title>

    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/color-brewer.min.css">
    <link rel="stylesheet" href="http://ai-code.tech/ai_notes_html/css/custom.css">
</head>

<body>

    <h1>Agent-Based Models</h1>
<p>Agent-based models include:</p>
<ul>
<li>individual agents model intelligent behavior, usually with a simple set of rules</li>
<li>the agents are situated in some space or a network and interact with each other locally</li>
<li>the agents usually have imperfect, local information</li>
<li>there is usually variability between agents</li>
<li>often there are random elements, either among the agents or in the world</li>
</ul>
<h2>Agents</h2>
<ul>
<li>An <strong>agent</strong> is an entity that <em>perceives</em> and <em>acts</em>.</li>
<li>A <strong>rational agent</strong> selects actions that maximize its (expected) <em>utility</em>.</li>
<li>Characteristics of the <em>percepts</em>, <em>environment</em>, and <em>action space</em> dictate techniques for selecting rational actions.</li>
</ul>
<p><strong>Reflex</strong> agents choose actions based on the current percept (and maybe memory). They are concerned almost exclusively with the <em>current</em> state of the world - they do not consider the future consequences of their actions, and they don't have a goal that their are working towards. Rather, they just operate off of simple "reflexes".</p>
<p>Agents that <strong>plan</strong> consider long(er) term consequences of their actions, have a model of how the world changes based on their actions, and work towards a particular goal (or goals), and can find an optimal solution (plan) for achieving its goal or goals.</p>
<h3>Brownian agents</h3>
<p>A <strong>Brownian agent</strong> is described by a set of state variables <mathjax>$u_i^{(k)}$</mathjax> where <mathjax>$i \in [1, \dots, N]$</mathjax> refers to the individual agent <mathjax>$i$</mathjax> and <mathjax>$k$</mathjax> indicates the different variables.</p>
<p>These state variables may be <em>external</em>, which are observable from outside the agent, or <em>internal degrees of freedom</em> that must be inferred from observable actions.</p>
<p>The state variables can change over time due to the environment or internal dynamics. We can generally express the dynamics of the state variables as follows:</p>
<p><mathjax>$$
\frac{d u_i^{(k)}}{dt} = f_i^{(k)} + \mathcal F_i^{\text{stoch}}
$$</mathjax></p>
<p>The <em>principle of causality</em> is represented here: any <em>effect</em> such as a temporal change of variable <mathjax>$u$</mathjax> has some <em>causes</em> on the right-hand side of the equation; such causes are described as a <em>superposition</em> of deterministic and stochastic influences imposed on the agent <mathjax>$i$</mathjax>.</p>
<p>In this formulation, <mathjax>$f_i^{(k)}$</mathjax> is a deterministic term representing influences that can be specified on the time and length scale of the agent, whereas <mathjax>$F_i^{\text{stoch}}$</mathjax> is a stochastic term which represents influences that exist, but not observable on the time and length scale of the agent.</p>
<p>The deterministic term <mathjax>$f_i^{(k)}$</mathjax> captures all specified influences that cause changes to the state variable <mathjax>$u_i^{(k)}$</mathjax>, including interactions with other agents <mathjax>$j \in N$</mathjax>, so it could be a function of the state variables of other agents in addition to external conditions.</p>
<h2>Multi-task and multi-scale problems</h2>
<p>A <strong>multi-task</strong> domain is an environment where an agent performs two or more separate tasks.</p>
<p>A <strong>multi-scale</strong> domain is a multi-task domain that satisfies the following:</p>
<ul>
<li>multiple structural scales: actions are performed across multiple levels of coordination</li>
<li>interrelated tasks: there is not a strict separation across tasks and the performance in each tasks impacts other tasks</li>
<li>actions are performed in real-time</li>
</ul>
<p>More generally, multi-scale problems involve working at many different levels of detail.</p>
<p>For example, an AI for an RTS game must manage many simultaneous goals at the micro and macro level, and these goals and their tasks are often interwoven, and all this must be done in real-time.</p>
<h2>Utilities</h2>
<p>We encode <em>preferences</em> for an agent, e.g. <mathjax>$A \succ B$</mathjax> means the agent prefers <mathjax>$A$</mathjax> over <mathjax>$B$</mathjax> (on the other hand, <mathjax>$A \sim B$</mathjax> means the agent is indifferent about either).</p>
<p>A <em>lottery</em> represents these preferences under uncertainty, e.g. <mathjax>$[p, A; 1-p B]$</mathjax>.</p>
<p><em>Rational</em> preferences must obey the axioms of rationality:</p>
<ul>
<li><em>orderability</em>: <mathjax>$(A \succ B) \lor (B \succ A) \lor (A \sim B)$</mathjax>. You either have to like <mathjax>$A$</mathjax> better than <mathjax>$B$</mathjax>, <mathjax>$B$</mathjax> better than <mathjax>$A$</mathjax>, or be indifferent.</li>
<li><em>transitivity</em>: <mathjax>$(A \succ B) \land (B \succ C) \implies (A \sim C)$</mathjax></li>
<li><em>continuity</em>: <mathjax>$A \succ B \succ C \implies \exists p [p, A; 1 - p, C] \sim B$</mathjax>. That is, if <mathjax>$B$</mathjax> is somewhere between <mathjax>$A$</mathjax> and <mathjax>$C$</mathjax>, there is some lottery between <mathjax>$A$</mathjax> and <mathjax>$C$</mathjax> that is equivalent to <mathjax>$B$</mathjax>.</li>
<li><em>substitutability</em>: <mathjax>$A \sim B \implies [p, A;, 1 -p, C] \sim [p, B; 1 - p, C]$</mathjax>. If you're indifferent to <mathjax>$A$</mathjax> and <mathjax>$B$</mathjax>, you are indifferent to them in lotteries.</li>
<li><em>monotonicity</em>: <mathjax>$A \succ B \implies (p \geq q \Leftrightarrow [p, A;, 1 - p, B] \succeq [q, A; 1-q, B])$</mathjax>. If you prefer <mathjax>$A$</mathjax> over <mathjax>$B$</mathjax>, when given lotteries between <mathjax>$A$</mathjax> and <mathjax>$B$</mathjax>, you prefer the lottery that is biased towards <mathjax>$A$</mathjax>.</li>
</ul>
<p>When preferences are rational, they imply behavior that maximizes expected utility, which implies we can come up with a utility function to represent these preferences.</p>
<p>That is, there exists a real-valued function <mathjax>$U$</mathjax> such that:</p>
<p><mathjax>$$
\begin{aligned}
U(A) \geq U(B) &amp;\Leftrightarrow A \succeq B \\
U([p1, S_1; \dots ; p_n, S_n]) &amp;= \sum_i p_i U(S_i)
\end{aligned}
$$</mathjax></p>
<p>The second equation says that the utility of a lottery is the expected value of that lottery.</p>
<h2>References</h2>
<ul>
<li><a href="http://www.greenteapress.com/compmod/">Think Complexity</a>. Version 1.2.3. Allen B. Downey. 2012.</li>
<li><a href="http://arxiv.org/abs/1006.5305">An Agent-Based Model of Collective Emotions in Online Communities</a>. Frank Schweitzer, David Garcia. Swiss Federal Institute of Technology Zurich. 2008.</li>
<li><a href="http://alumni.soe.ucsc.edu/~bweber/bweber-dissertation.pdf">Integrating Learning in a Multi-Scale Agent</a>. Ben G. Weber. 2012.</li>
<li><a href="https://www.edx.org/course/artificial-intelligence-uc-berkeleyx-cs188-1x">CS188: Artificial Intelligence</a>. Dan Klein, Pieter Abbeel. University of California, Berkeley (edX).</li>
</ul>
    
    <script src="http://cdn.bootcss.com/jquery/1.11.1/jquery.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script>
        $(document).ready(function() {
            $('pre').each(function(i, e) {hljs.highlightBlock(e)});
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$","$"]],
                    displayMath: [['$$','$$']],
                    processEscapes: true
                },
                "HTML-CSS": {
                    linebreaks: { automatic: true }
                }
            });
            MathJax.Hub.Startup.onload();
        });
    </script>
    <script src="http://ai-code.tech/ai_notes_html/js/custom.js"></script>


</body>
</html>
