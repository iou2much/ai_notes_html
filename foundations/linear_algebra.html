
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <title>Linear Algebra</title>

    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/color-brewer.min.css">
    <link rel="stylesheet" href="http://ai-code.tech/ai_notes_html/css/custom.css">
</head>

<body>

    <p><mathjax>$$
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Det}{det}
\DeclareMathOperator{\Tr}{tr}
\DeclareMathOperator{\Adj}{adj}
\DeclareMathOperator{\Dim}{dim}
\DeclareMathOperator{\Rank}{rank}
\DeclareMathOperator{\Ker}{ker}
\DeclareMathOperator{\Img}{im}
\DeclareMathOperator{\Diag}{diag}
\DeclareMathOperator{\Proj}{Proj}
$$</mathjax></p>
<h1>线性代数</h1>
<h1>Linear Algebra</h1>
<p>当我们处理数据时, 通常要面对由很多个<strong>维度</strong>构成的数据点, 代表一个数据点有多个组成部分, 例如, 如果人是你的数据集, 那他们可以用身高,体重,年龄这三个维度来表示.这种数据叫<strong>向量</strong>, 而单个值的数据叫<strong>标量</strong>.</p>
<p>When working with data, we typically deal with many data points consisting of many <strong>dimensions</strong>. That is, each data point may have a several <strong>components</strong>; e.g. if people are your data points, they may be represented by their height, weight, and age, which constitutes three dimensions all together. These data points of many components are called <strong>vectors</strong>. These are contrasted with individual values, which are called <strong>scalars</strong>.</p>
<p>而多个向量又组成了<strong>矩阵</strong>.</p>
<p>We deal with these data points - vectors - simultaneously in aggregates known as <a href="#matrices"><em>matrices</em></a>.</p>
<p><strong>线性代数</strong>这门知识主要是一系列关于向量与矩阵的操作.</p>
<p>Linear algebra provides the tools needed to manipulate vectors and matrices.</p>
<h2>向量</h2>
<h2>Vectors</h2>
<p>向量有<strong>大小</strong>与<strong>方向</strong>, 例如, 以5公里/小时的速度往东. 大小在某种意义上可以看作"向量的长度".</p>
<p>Vectors have <strong>magnitude</strong> and <strong>direction</strong>, e.g. 5 miles/hour going east. The magnitude can be thought of, in some sense, as the "length" of the vector (this isn't quite right however, as there are many concepts of "length" - see <a href="#norm">norms</a>).</p>
<figure><img alt="A vector" src="../assets/vector.svg" /><figcaption>A vector</figcaption>
</figure>
<p>这个例子中的向量表示为:</p>
<p>Formally, this example would be represented:</p>
<p><mathjax>$$ \vec{v} = \begin{bmatrix} 5 \\ 0 \end{bmatrix} $$</mathjax></p>
<p>我们在x轴上移动了5个单位,在y轴上移动了0个单位.</p>
<p>since we are "moving" 5 on x-axis and 0 on the y-axis.</p>
<p>字母上的箭头经常被省略,也就是说,表示为<mathjax>$v$</mathjax>.</p>
<p>Note that often the arrow is dropped, i.e. the vector is notated as just <mathjax>$v$</mathjax>.</p>
<h3>实空间</h3>
<h3>Real coordinate spaces</h3>
<p>Vectors are plotted and manipulated in <em>space</em>. A two-dimensional vector, such as the previous example, may be represented in a two-dimensional space.</p>
<p>A vector with three components would be represented in a three-dimensional space, and so on for any arbitrary <mathjax>$n$</mathjax> dimensions.</p>
<p>A <strong>real coordinate space</strong> (that is, a space consisting of real numbers) of <mathjax>$n$</mathjax> dimensions is notated <mathjax>$\mathbb R^n$</mathjax>. Such a space encapsulates all possible vectors of that dimensionality, i.e. all possible vectors of the form <mathjax>$[v_1, v_2, \dots, v_n]$</mathjax>.</p>
<p>To denote a vector of <mathjax>$n$</mathjax> dimensions, we write <mathjax>$x \in \mathbb R^n$</mathjax>.</p>
<p>For example: the notation for the two-dimensional real coordinate space is notated <mathjax>$\mathbb R^2$</mathjax>, which is all possible real-valued 2-tuples (i.e. all 2D vectors whose components are real numbers, e.g. <mathjax>$[1, 2], [-0.4, 21.4], \dots$</mathjax>). If we wanted to describe an arbitrary two-dimensional vector, we could do so with <mathjax>$\vec v \in \mathbb R^2$</mathjax>.</p>
<h3>列向量与行向量</h3>
<h3>Column and row vectors</h3>
<p>A vector <mathjax>$x \in \mathbb R^n$</mathjax> typically denotes a <strong>column vector</strong>, i.e. with <mathjax>$n$</mathjax> rows and 1 column.</p>
<p>A <strong>row vector</strong> <mathjax>$x^T \in \mathbb R^n$</mathjax> has 1 row and <mathjax>$n$</mathjax> columns. The notation <mathjax>$x^T$</mathjax> is described below.</p>
<h3>转置向量</h3>
<h3>Transposing a vector</h3>
<p><strong>Transposing</strong> a vector means turning its rows into columns:</p>
<p><mathjax>$$
\vec{a} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix}, \\
\vec{a}^T = \begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; x_4 \end{bmatrix}
$$</mathjax></p>
<p>So a column vector <mathjax>$x$</mathjax> can be represented as a row vector with <mathjax>$x^T$</mathjax>.</p>
<h3>向量操作</h3>
<h3>Vectors operations</h3>
<h4>向量加法</h4>
<h4>Vector addition</h4>
<p>Vectors are added by adding the individual corresponding components:</p>
<p><mathjax>$$ \begin{bmatrix} 6 \\ 2 \end{bmatrix} + \begin{bmatrix} -4 \\ 4 \end{bmatrix} = \begin{bmatrix} 6 + -4 \\ 2 + 4 \end{bmatrix} = \begin{bmatrix} 2 \\ 6 \end{bmatrix} $$</mathjax></p>
<h4>标量乘以向量</h4>
<h4>Multiplying a vector by a scalar</h4>
<figure><img alt="Example: The red vector is before multiplying a scalar, blue is after." src="../assets/scaled_vector.svg" /><figcaption>Example: The red vector is before multiplying a scalar, blue is after.</figcaption>
</figure>
<p>To multiply a vector with a scalar, you just multiply the individual components of the vector by the scalar:</p>
<p><mathjax>$$ 3\begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \times 2 \\ 3 \times 1
\end{bmatrix} = \begin{bmatrix} 6 \\ 3 \end{bmatrix} $$</mathjax></p>
<p>This changes the <em>magnitude</em> of the vector, but <em>not the direction</em>.</p>
<h4>向量点积</h4>
<h4>Vector dot products</h4>
<p>The <strong>dot product</strong> (also called <strong>inner product</strong>) of two vectors <mathjax>$\vec{a}, \vec{b} \in \mathbb R^n$</mathjax> (note that this implies they must be of the same dimension) is notated:</p>
<p><mathjax>$$ \vec{a} \cdot \vec{b} $$</mathjax></p>
<p>It is calculated:</p>
<p><mathjax>$$
\vec{a} \cdot \vec{b} = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} \cdot \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} = a_1b_1 + a_2b_2 + \dots + a_nb_n = \sum^n_{i=1} a_i b_i
$$</mathjax></p>
<p>Which results in a scalar value.</p>
<p>Note that sometimes the dot operator is dropped, so a dot product may be notated as just <mathjax>$\vec{a} \vec{b}$</mathjax>.</p>
<p>Also note that the dot product of <mathjax>$x \cdot y$</mathjax> is equivalent to the matrix multiplication <mathjax>$x^Ty$</mathjax>.</p>
<p>Properties of vector dot products:</p>
<ul>
<li><strong>Commuative property</strong>: The order of the dot product doesn't matter: <mathjax>$\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$</mathjax></li>
<li><strong>Distributive property</strong>: You can distribute terms in dot products: <mathjax>$(\vec{v} + \vec{w}) \cdot \vec{x} = (\vec{v} \cdot \vec{x} + \vec{w} \cdot \vec{x})$</mathjax></li>
<li><strong>Associative property</strong>: <mathjax>$(c\vec{v}) \cdot \vec{w} = c(\vec{v} \cdot \vec{w})$</mathjax></li>
</ul>
<h3>范数</h3>
<h3>Norms</h3>
<p>The <strong>norm</strong> of a vector <mathjax>$x \in \mathbb R^n$</mathjax>, denoted <mathjax>$||x||$</mathjax>, is the "length" of the vector. That is, norms are a generalization of "distance" or "length".</p>
<p>There are many different norms, the most common of which is the <strong>Euclidean norm</strong> (also known as the <mathjax>$\ell_2$</mathjax> norm), denoted <mathjax>$||x||_2$</mathjax>, computed:</p>
<p><mathjax>$$ ||x||_2 = \sqrt{ \sum^n_{i=1} x^2_i } = \sqrt{x^Tx}$$</mathjax></p>
<p>This is the "as-the-crow-flies" distance that we are all familiar with.</p>
<p>Generally, a norm is just any function <mathjax>$f : \mathbb R^n \to \mathbb R$</mathjax> which satisfies the following properties:</p>
<ol>
<li><em>non-negativity</em>: For all <mathjax>$x \in \mathbb R^n$</mathjax>, <mathjax>$f(x) \geq 0$</mathjax></li>
<li><em>definiteness</em>: <mathjax>$f(x) = 0$</mathjax> if and only if <mathjax>$x=0$</mathjax></li>
<li><em>homogeneity</em>: For all <mathjax>$x \in \mathbb R^n, t \in \mathbb R, f(tx) = |t|f(x)$</mathjax></li>
<li><em>triangle inequality</em>: For all <mathjax>$x,y \in \mathbb R^n, f(x+y) \leq f(x) + f(y)$</mathjax></li>
</ol>
<p>Another norm is the <mathjax>$\ell_1$</mathjax> norm:</p>
<p><mathjax>$$
||x||_1 = \sum^n_{i=1} |x_i|
$$</mathjax></p>
<p>and the <mathjax>$\ell_{\infty}$</mathjax> norm:</p>
<p><mathjax>$$
||x||_{\infty} = \max_i|x_i|
$$</mathjax></p>
<p>These three norms are part of the family of <mathjax>$\ell_p$</mathjax> norms, which are parameterized by a real number <mathjax>$p \geq 1$</mathjax> and defined as:</p>
<p><mathjax>$$
||x||_p = (\sum^n_{i=1} |x_i|^p)^{\frac{1}{p}}
$$</mathjax></p>
<p>There are also norms for [matrices], the most common of which is the <strong>Frobenius norm</strong>, analogous to the Euclidean (<mathjax>$\ell_2$</mathjax>) norm for vectors:</p>
<p><mathjax>$$
||A||_{\text{Fro}} = \sqrt{\sum_{n=1}^N \sum_{m=1}^M A_{n,m}^2}
$$</mathjax></p>
<h4>长度与点积</h4>
<h4>Lengths and dot products</h4>
<p>You may notice that the dot product of a vector with itself is the square of that vector's length:</p>
<p><mathjax>$$ \vec{a} \cdot \vec{a} = a_1^2 + a_2^2 + \dots + a_n^2 = ||\vec{a}||^2 $$</mathjax></p>
<p>So the length (Euclidean norm) of a vector can be written:</p>
<p><mathjax>$$ ||\vec{a}|| = \sqrt{\vec{a} \cdot \vec{a}} $$</mathjax></p>
<h3>单位向量</h3>
<h3>Unit vectors</h3>
<p>Each dimension in a space has a <strong>unit vector</strong>, generally denoted with a hat, e.g. <mathjax>$\hat u$</mathjax>, which is a vector constrained to that dimension (that is, it has 0 magnitude in all other dimensions), with length 1, e.g. <mathjax>$||\hat u|| = 1$</mathjax>.</p>
<p>Unit vectors exists for all <mathjax>$\mathbb R^n$</mathjax>.</p>
<p>The unit vector is also called a <strong>normalized vector</strong> (which is not to be confused with a <em>normal</em> vector, which is something else entirely.)</p>
<p>The unit vector in the same direction as some vector<br />
<mathjax>$\vec{v}$</mathjax> is found by computing:</p>
<p><mathjax>$$ \hat u = \frac{\vec{v}}{||\vec{v}||} $$</mathjax></p>
<p>For instance, in <mathjax>$\mathbb R^2$</mathjax> space, we would have two unit vectors:</p>
<p><mathjax>$$ \hat{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} ,\;
\hat{j} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} $$</mathjax></p>
<p>In <mathjax>$\mathbb R^3$</mathjax> space, we would have three unit vectors:</p>
<p><mathjax>$$ \hat{i} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} ,\;
\hat{j} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} ,\;
\hat{k} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} $$</mathjax></p>
<p>But you can have unit vectors in any direction. Say you have a vector:</p>
<p><mathjax>$$ \vec{a} = \begin{bmatrix} 5 \\ -6 \end{bmatrix} $$</mathjax></p>
<p>You can find a unit vector <mathjax>$\hat{u}$</mathjax> in the direction of this vector like so:</p>
<p><mathjax>$$ \hat{u} = \frac{\vec{a}}{||\vec{a}||} $$</mathjax></p>
<p>so, with our example:</p>
<p><mathjax>$$ \hat{u} = \frac{1}{||\vec{a}||}\vec{a} = \frac{1}{\sqrt{61}}\begin{bmatrix}5 \\
        -6\end{bmatrix} = \begin{bmatrix} \frac{5}{\sqrt{61}} \\ \frac{-6}{\sqrt{61}}
        \end{bmatrix}$$</mathjax></p>
<h3>向量间角度</h3>
<h3>Angles between vectors</h3>
<p>Say you have two non-zero vectors, <mathjax>$\vec{a}, \vec{b} \in \mathbb R^n$</mathjax>.</p>
<p>We often notate the angle between two vectors as <mathjax>$\theta$</mathjax>.</p>
<figure><img alt="Angle between two vectors." src="../assets/vector_angle.svg" /><figcaption>Angle between two vectors.</figcaption>
</figure>
<p>The <strong>law of cosine</strong> tells us, that for a triangle:</p>
<p><mathjax>$$ C^2 = A^2 + B^2 - 2AB\cos\theta $$</mathjax></p>
<p>Using this law, we can get the angle between our two vectors:</p>
<p><mathjax>$$ ||\vec{a} - \vec{b}||^2 = ||\vec{b}||^2 + ||\vec{a}||^2 - 2||\vec{a}||||\vec{b}||\cos\theta $$</mathjax></p>
<p>which simplifies to:</p>
<p><mathjax>$$ \vec{a} \cdot \vec{b} = ||\vec{a}||||\vec{b}||\cos\theta $$</mathjax></p>
<p>There are two special cases if the vectors are <strong>collinear</strong>, that is if <mathjax>$\vec{a} = c\vec{b}$</mathjax>:</p>
<ul>
<li>If <mathjax>$c &gt; 0$</mathjax>, then <mathjax>$\theta = 0$</mathjax>.</li>
<li>If <mathjax>$c &lt; 0$</mathjax>, then <mathjax>$\theta = 180^{\circ}$</mathjax></li>
</ul>
<h3>正交向量</h3>
<h3>Perpendicular vectors</h3>
<p>With the above angle calculation, you can see that if <mathjax>$\vec{a}$</mathjax> and <mathjax>$\vec{b}$</mathjax> are non-zero, and their dot product is 0, that is, <mathjax>$\vec{a} \cdot \vec{b} = \vec{0}$</mathjax>, then they are perpendicular to each other.</p>
<p>Whenever a pair of vectors satisfies this condition <mathjax>$\vec{a} \cdot \vec{b} = \vec{0}$</mathjax>, it is said that the two vectors are <strong>orthogonal</strong> (i.e. perpendicular).</p>
<p>Note that because any vector times the zero vector equals the zero vector: <mathjax>$\vec{0} \cdot \vec{x} = \vec{0}$</mathjax>.</p>
<p>Thus the zero vector is orthogonal to <em>everything</em>.</p>
<p>Technical detail: So if the vectors are both non-zero and orthogonal, then the vectors are <em>both</em> perpendicular and orthogonal. But of course, since the zero vector is not non-zero, it cannot be perpendicular to anything, but it is orthogonal to everything.</p>
<h3>法向量</h3>
<h3>Normal vectors</h3>
<p>A <strong>normal</strong> vector is one which is perpendicular to all the points/vectors on a plane.</p>
<p>That is for any vector <mathjax>$\vec{a}$</mathjax> on the plane, and a normal vector, <mathjax>$\vec{n}$</mathjax>, to that plane, we have:</p>
<p><mathjax>$$ \vec{n} \cdot \vec{a} = \vec{0} $$</mathjax></p>
<p>For example: given an equation of a plane, <mathjax>$Ax + By + Cz = D$</mathjax>, the normal vector is simply:</p>
<p><mathjax>$$ \vec{n} = A\hat{i} + B\hat{j} + C\hat{k} $$</mathjax></p>
<h3>正交向量</h3>
<h3>Orthonormal vectors</h3>
<p>Given <mathjax>$V = \{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_k} \}$</mathjax> where:</p>
<ul>
<li><mathjax>$||\vec{v_i}|| = 1$</mathjax> for <mathjax>$i=1,2,\dots,k$</mathjax>. That is, the length of each vector in <mathjax>$V$</mathjax> is 1 (that is, they have all been normalized).</li>
<li><mathjax>$\vec{v_i} \cdot \vec{v_j} = 0$</mathjax> for <mathjax>$i \neq j$</mathjax>. That is, these vectors are all orthogonal to each other.</li>
</ul>
<p>This can be summed up as:</p>
<p><mathjax>$$ \vec{v_i} \cdot \vec{v_j} = \begin{cases} 0 &amp; i \neq j \\ 1 &amp; i = j \end{cases} $$</mathjax></p>
<p>This is an <strong>orthonormal</strong> set. The term comes from the fact that these vectors are all<br />
orthogonal to each other, and they have all been normalized.</p>
<h3>其他向量操作</h3>
<h3>Additional vector operations</h3>
<p>These vector operations are less common, but included for reference.</p>
<h4>外积</h4>
<h4>Vector outer products</h4>
<p>For the <strong>outer product</strong>, the two vectors do not need to be of the same dimension (i.e. <mathjax>$x \in \mathbb R^n, y \in \mathbb R^m$</mathjax>), and the result is a matrix instead of a scalar:</p>
<p><mathjax>$$
x \otimes y \in \mathbb R^{n \times m} = \begin{bmatrix} x_1y_1 &amp; \cdots &amp; x_1y_m \\ \vdots &amp; \ddots &amp; \vdots \\ x_ny_1 &amp; \cdots &amp; x_ny_m \end{bmatrix}
$$</mathjax></p>
<p>Note that the outer product <mathjax>$x \otimes y$</mathjax> is equivalent to the matrix multiplication <mathjax>$xy^T$</mathjax>.</p>
<h4>叉积</h4>
<h4>Vector cross products</h4>
<p><strong>Cross products</strong> are much more limited than dot products. Dot products can be calculated for any <mathjax>$\mathbb R^n$</mathjax>. Cross products are only defined in <mathjax>$\mathbb R^3$</mathjax>.</p>
<p>Unlike the dot product, which results in a scalar, the cross product results in a vector which is orthogonal to the original vectors (i.e. it is orthogonal to the plane defined by the two original vectors).</p>
<p><mathjax>$$ \vec{a} = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix}, \vec{b} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} $$</mathjax></p>
<p><mathjax>$$ \vec{a} \times \vec{b} = \begin{bmatrix} a_2b_3 - a_3b_2 \\ a_3b_1 - a_1b_3 \\ a_1b_2 - a_2b_1 \end{bmatrix} $$</mathjax></p>
<p>For example:</p>
<p><mathjax>$$ \begin{bmatrix} 1 \\ -7 \\ 1 \end{bmatrix} \times \begin{bmatrix} 5 \\ 2 \\ 4
\end{bmatrix} = \begin{bmatrix} -7\times4 - 1\times2 \\ 1\times5 - 1\times4 \\ 1\times2 - -7\times5 \end{bmatrix} =
\begin{bmatrix} -30 \\ 1 \\ 37 \end{bmatrix} $$</mathjax></p>
<h2>线性组合</h2>
<h2>Linear Combinations</h2>
<h3>线的参数化表示</h3>
<h3>Parametric representations of lines</h3>
<p>Any line in an <mathjax>$n$</mathjax>-dimensional space can be represented using vectors.</p>
<p>Say you have a vector <mathjax>$\vec{v}$</mathjax> and a set <mathjax>$S$</mathjax> consisting of all scalar multiplications of that vector (where the scalar <mathjax>$c$</mathjax> is any real number):</p>
<p><mathjax>$$ S = \{  c\vec{v} \,|\, c \in \mathbb R \} $$</mathjax></p>
<p>This set <mathjax>$S$</mathjax> represents a line, since multiplying a vector with scalars does not changes its direction, only its magnitudes, so that set of vectors covers the entirety of the line.</p>
<figure><img alt="A few of the infinite scalar multiplications which define the line." src="../assets/vector_line.svg" /><figcaption>A few of the infinite scalar multiplications which define the line.</figcaption>
</figure>
<p>But that line is around the origin. If you wanted to shift it, you need only to add a vector, which we'll call <mathjax>$\vec{x}$</mathjax>. So we could define a line as:</p>
<p><mathjax>$$ L = \{  \vec{x} + c\vec{v} \,|\, c \in \mathbb R \} $$</mathjax></p>
<figure><img alt="Calculating the intersecting line for two vectors." src="../assets/intersecting_line.svg" /><figcaption>Calculating the intersecting line for two vectors.</figcaption>
</figure>
<p>For example: say you are given two vectors:</p>
<p><mathjax>$$ \vec{a} = \begin{bmatrix} 2 \\ 1 \end{bmatrix} ,\; \vec{b} = \begin{bmatrix} 0 \\ 3
\end{bmatrix} $$</mathjax></p>
<p>Say you want to find the line that goes through them. First you need to find the vector along that intersecting line, which is just <mathjax>$\vec{b} - \vec{a}$</mathjax>.</p>
<p>Although in standard form, that vector originates at the origin.</p>
<p>Thus you still need to shift it by finding the appropriate vector <mathjax>$\vec{x}$</mathjax> to add to it. But as you can probably see, we can use our <mathjax>$\vec{a}$</mathjax> to shift it, giving us:</p>
<p><mathjax>$$ L = \{  \vec{a} + c(\vec{b} - \vec{a}) \,|\, c \in \mathbb R \} $$</mathjax></p>
<p>And this works for any arbitrary <mathjax>$n$</mathjax> dimensions! (Although in other spaces, this wouldn't really be a "line". In <mathjax>$\mathbb R^3$</mathjax> space, for instance, this would define a plane.)</p>
<p>You can convert this form to a <strong>parametric</strong> equation, where the equation for a dimension of the vector <mathjax>$a_i$</mathjax> looks like:</p>
<p><mathjax>$$ a_i + (b_i  - a_i)c $$</mathjax></p>
<p>Say you are in <mathjax>$\mathbb R^2$</mathjax> so, you might have:</p>
<p><mathjax>$$ L = \left\lbrace  \begin{bmatrix} 0 \\ 3 \end{bmatrix} + c \begin{bmatrix} -2 \\ 2 \end{bmatrix} \,|\, c \in \mathbb R \right\rbrace $$</mathjax></p>
<p>you can write it as the following parametric equation:</p>
<p><mathjax>$$
\begin{aligned}
x &amp;= 0 + -2c = -2c \\
y &amp;= 3 + 2c = 2c + 3
\end{aligned}
$$</mathjax></p>
<h3>线性组合</h3>
<h3>Linear combinations</h3>
<p>Say you have the following vectors in <mathjax>$\mathbb R^m$</mathjax> :</p>
<p><mathjax>$$ \vec{v_1}, \vec{v_2}, \dotsc, \vec{v_n} $$</mathjax></p>
<p>A <strong>linear combination</strong> is just some sum of the combination of these vectors, scaled by arbitrary constants (<mathjax>$c_1 \to c_n \in \mathbb R$</mathjax>):</p>
<p><mathjax>$$ c_1\vec{v_1} + c_2\vec{v_2}, + \dotsc + c_n\vec{v_n} $$</mathjax></p>
<p>For example:</p>
<p><mathjax>$$ \vec{a} = \begin{bmatrix} 2 \\ 1 \end{bmatrix} ,\; \vec{b} = \begin{bmatrix} 0 \\ 3 \end{bmatrix} $$</mathjax></p>
<p>A linear combination would be:</p>
<p><mathjax>$$ 0\vec{a} + 0\vec{b} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} $$</mathjax></p>
<p><em>Any</em> vector in the space <mathjax>$\mathbb R^2$</mathjax> can represented by some linear combination of these two vectors.</p>
<h3>张成</h3>
<h3>Spans</h3>
<p>The set of all linear combinations for some vectors is called the <strong>span</strong>.</p>
<p>The span of some vectors can define an entire space. For instance, using our previously-defined vectors:</p>
<p><mathjax>$$ \Span(\vec{a}, \vec{b}) = \mathbb R^2 $$</mathjax></p>
<p>But this is not always true for the span of any arbitrary set of vectors. For instance, this does <em>not</em> represent all the vectors in <mathjax>$\mathbb R^2$</mathjax> :</p>
<p><mathjax>$$ \Span(\begin{bmatrix} -2 \\ -2 \end{bmatrix}, \begin{bmatrix} 2 \\ 2 \end{bmatrix}) $$</mathjax></p>
<p>These two vectors are <strong>collinear</strong> (that is, they lie along the same line), so combinations of them will only yield other vectors along that line.</p>
<p>As another example, the span of the zero vector, <mathjax>$\Span(\vec{0})$</mathjax>, cannot represent all vectors in a space.</p>
<p>Formally, the span is defined as:</p>
<p><mathjax>$$ \Span(\vec{v_1}, \vec{v_2}, \dotsc, \vec{v_n}) = \{ c_1\vec{v_1} + c_2\vec{v_2}, + \dotsc + c_n\vec{v_n} \,|\, c_i \in \mathbb R \, \forall \, 1 \le i \le n \} $$</mathjax></p>
<h3>线性无关</h3>
<h3>Linear independence</h3>
<p>The set of vectors in the previous collinear example:</p>
<p><mathjax>$$ \left\lbrace \begin{bmatrix} -2 \\ -2 \end{bmatrix}, \begin{bmatrix} 2 \\ 2 \end{bmatrix} \right\rbrace $$</mathjax></p>
<p>are called a <strong>linearly dependent set</strong>, which means that some vector in the set can be represented as the linear combination of some of the other vectors in the set.</p>
<p>In this example, we could represent <mathjax>$\begin{bmatrix} -2 , -2 \end{bmatrix}$</mathjax> using the linear combination of the other vector, i.e. <mathjax>$-1\begin{bmatrix} 2 , 2 \end{bmatrix}$</mathjax>.</p>
<p>You can think of a linearly dependent set as one that contains a redundant vector - one that doesn't add any more information to the set.</p>
<p>As another example:</p>
<p><mathjax>$$ \left\lbrace \begin{bmatrix} 2 \\ 3 \end{bmatrix}, \begin{bmatrix} 7 \\ 2 \end{bmatrix}, \begin{bmatrix} 9 \\ 5 \end{bmatrix} \right\rbrace $$</mathjax></p>
<p>is linearly dependent because <mathjax>$\vec{v_1} + \vec{v_2} = \vec{v_3}$</mathjax>.</p>
<p>Naturally, a set that is <em>not</em> linearly dependent is called a <strong>linearly independent set</strong>.</p>
<p>For a more formal definition of linear dependence, a set of vectors:</p>
<p><mathjax>$$ S = \{ \vec{v_1}, \vec{v_2}, \dotsc, \vec{v_n} \} $$</mathjax></p>
<p>is linearly dependent iff (if and only if)</p>
<p><mathjax>$$ c_1\vec{v_1} + c_2\vec{v_2} + \dotsc + c_n\vec{v_n} = \vec{0} = \begin{bmatrix} 0 \\
        \vdots \\ 0 \end{bmatrix} $$</mathjax></p>
<p>for some <mathjax>$c_i$</mathjax>'s where <em>at least one is non-zero</em>.</p>
<p>To put the previous examples in context, if you can show that at least one of the vectors can be described by the linear combination of the other vectors in the set, that is:</p>
<p><mathjax>$$ \vec{v_1} = a_2\vec{v_2} + a_3\vec{v_3} + \dots + a_n\vec{v_n} $$</mathjax></p>
<p>then you have a linearly dependent set because that can be reduced to show:</p>
<p><mathjax>$$ \vec{0} = -1\vec{v_1} + a_2\vec{v_2} + a_3\vec{v_3} + \dots + a_n\vec{v_n} $$</mathjax></p>
<p>Thus you can calculate the zero vector as a linear combination of the vectors where at least one constant is non-zero, which satifies the definition for linear dependence.</p>
<p>So then a set is <strong>linearly independent</strong> if, to calculate the zero vector as a linear combination of the vectors, the coefficients must all be zero.</p>
<p>Going back to spans, the span of set of size <mathjax>$n$</mathjax> which is <em>linearly independent</em> can describe that set's entire space (e.g. <mathjax>$\mathbb R^n$</mathjax>).</p>
<h4>示例</h4>
<h4>An example problem:</h4>
<p>Say you have the set:</p>
<p><mathjax>$$ S = \left\lbrace \begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}, \begin{bmatrix} 2 \\ 1
\\ 3 \end{bmatrix}, \begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix} \right\rbrace $$</mathjax></p>
<p>and you want to know:</p>
<ul>
<li>does <mathjax>$\Span(S) = \mathbb R^3$</mathjax>?</li>
<li>is <mathjax>$S$</mathjax> linearly independent?</li>
</ul>
<p>For the first question, you want to see if any linear combination of the set yields any arbitrary vector in <mathjax>$\mathbb R^3$</mathjax> :</p>
<p><mathjax>$$  c_1\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}, c_2\begin{bmatrix} 2 \\ 1
\\ 3 \end{bmatrix}, c_3\begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix} = \begin{bmatrix} a \\
        b \\ c \end{bmatrix} $$</mathjax></p>
<p>You can distribute the coefficients:</p>
<p><mathjax>$$  \begin{bmatrix} 1c_1 \\ -1c_1 \\ 2c_1 \end{bmatrix}, \begin{bmatrix} 2c_2 \\ 1c_2
\\ 3c_2 \end{bmatrix}, \begin{bmatrix} -1c_3 \\ 0c_3 \\ 2c_3 \end{bmatrix} = \begin{bmatrix} a \\
        b \\ c \end{bmatrix} $$</mathjax></p>
<p>So you can break that out into a system of equations:</p>
<p><mathjax>$$
\begin{aligned}
c_1 + 2c_2 - c_3 &amp;= a \\
-c_1 + c_2 + 0 &amp;= b \\
2c_1 + 3c_2 + 2c_3 &amp;= c
\end{aligned}
$$</mathjax></p>
<p>And solve it, which gives you:</p>
<p><mathjax>$$
\begin{aligned}
c_3 &amp;= \frac{1}{11}(3c - 5a + b) \\
c_2 &amp;= \frac{1}{3}(b + a + c_3) \\
c_1 &amp;= a - 2c_2 + c_3
\end{aligned}
$$</mathjax></p>
<p>So it looks like you can get these coefficients from any <mathjax>$a, b, c$</mathjax>, so we can say <mathjax>$\Span(S) = \mathbb R^3$</mathjax>.</p>
<p>For the second question, we want to see if all of the coefficients have to be non-zero for this to be true:</p>
<p><mathjax>$$  c_1\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}, c_2\begin{bmatrix} 2 \\ 1
\\ 3 \end{bmatrix}, c_3\begin{bmatrix} -1 \\ 0 \\ 2 \end{bmatrix} = \begin{bmatrix} 0 \\
        0 \\ 0 \end{bmatrix} $$</mathjax></p>
<p>We can just reuse the previous equations we derived for the coefficients, substituting <mathjax>$a=0, b=0, c=0$</mathjax>, which gives us:</p>
<p><mathjax>$$
c_1 = c_2 = c_3 = 0
$$</mathjax></p>
<p>So we know this set is linearly independent.</p>
<h2>矩阵</h2>
<h2>Matrices</h2>
<p>A <strong>matrix</strong> can, in some sense, be thought of as a vector of vectors.</p>
<p>The notation <mathjax>$m \times n$</mathjax> in terms of matrices mean there are <mathjax>$m$</mathjax> rows and <mathjax>$n$</mathjax> columns.</p>
<p>So that matrix would look like:</p>
<p><mathjax>$$
\mathbf A =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn}
\end{bmatrix}
$$</mathjax></p>
<p>A matrix of these dimensions may also be notated <mathjax>$\mathbb R^{m \times n}$</mathjax> to indicate its membership in that set.</p>
<p>We refer to the entry in the <mathjax>$i$</mathjax>th row and <mathjax>$j$</mathjax>th column with the notation <mathjax>$\mathbf A_{ij}$</mathjax>.</p>
<h3>矩阵操作</h3>
<h3>Matrix operations</h3>
<h4>加法</h4>
<h4>Matrix addition</h4>
<p>Matrices must have the same dimensions in order to be added (or subtracted).</p>
<p><mathjax>$$
\mathbf A =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn}
\end{bmatrix},
\mathbf B =
\begin{bmatrix}
b_{11} &amp; b_{12} &amp; \dots &amp; b_{1n} \\
b_{21} &amp; b_{22} &amp; \dots &amp; b_{2n} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
b_{m1} &amp; b_{m2} &amp; \dots &amp; b_{mn}
\end{bmatrix}
$$</mathjax></p>
<p><mathjax>$$
\mathbf A + \mathbf B =
\begin{bmatrix}
a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \dots &amp; a_{1n} + b_{1n} \\
a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \dots &amp; a_{2n} + b_{2n} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \dots &amp; a_{mn} + b_{mn}
\end{bmatrix}
$$</mathjax></p>
<p><mathjax>$$ \mathbf A + \mathbf B = \mathbf B + \mathbf A $$</mathjax></p>
<h4>矩阵-标量乘法</h4>
<h4>Matrix-scalar multiplication</h4>
<p>Just distribute the scalar:</p>
<p><mathjax>$$
\mathbf A =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn}
\end{bmatrix},
c\mathbf A =
\begin{bmatrix}
ca_{11} &amp; ca_{12} &amp; \dots &amp; ca_{1n} \\
ca_{21} &amp; ca_{22} &amp; \dots &amp; ca_{2n} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
ca_{m1} &amp; ca_{m2} &amp; \dots &amp; ca_{mn}
\end{bmatrix}
$$</mathjax></p>
<h4>矩阵-向量积</h4>
<h4>Matrix-vector products</h4>
<p>To multiply a <mathjax>$m \times n$</mathjax> matrix with a vector, the vector must have <mathjax>$n$</mathjax> components (that is, the same number of components as there are columns in the matrix, i.e. <mathjax>$\vec{x} \in \mathbb R^n$</mathjax>):</p>
<p><mathjax>$$
\vec{x} =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
$$</mathjax></p>
<p>The product would be:</p>
<p><mathjax>$$
\mathbf A \vec{x} =
\begin{bmatrix}
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n \\
a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n \\
\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n
\end{bmatrix}
$$</mathjax></p>
<p>This results in a <mathjax>$m \times 1$</mathjax> matrix.</p>
<h5>矩阵-向量乘积,看作线性组合</h5>
<h5>Matrix-vector products as linear combinations</h5>
<p>If you interpret each column in a matrix <mathjax>$\mathbf A$</mathjax> as its own vector <mathjax>$\vec{v_i}$</mathjax>, such that:</p>
<p><mathjax>$$ \mathbf A = \begin{bmatrix} \vec{v_1} &amp; \vec{v_2} &amp; \dots &amp; \vec{v_n} \end{bmatrix} $$</mathjax></p>
<p>Then the product of a matrix and vector can be rewritten simply as a linear combination of those vectors:</p>
<p><mathjax>$$
\mathbf A \vec{x} = x_1\vec{v_1} + x_2\vec{v_2} + \dots + x_n\vec{v_n}
$$</mathjax></p>
<h5>矩阵-向量乘积,看作线性转换</h5>
<h5>Matrix-vector products as linear transformations</h5>
<p>A matrix-vector product can also be seen as a linear transformation. You can describe it as a transformation:</p>
<p><mathjax>$$
\begin{aligned}
\mathbf A &amp;= \begin{bmatrix} \vec{v_1} &amp; \vec{v_2} &amp; \dots &amp; \vec{v_n} \end{bmatrix} \\
T: \mathbb R^n &amp;\to \mathbb R^m \\
T(\vec{x}) &amp;= \mathbf A \vec{x}
\end{aligned}
$$</mathjax></p>
<p>It satisfies the conditions for a linear transformation (not shown here), so a matrix-vector product is always a linear transformation.</p>
<p>Just to be clear: the transformation of a vector can always be expressed as that vector's product with some matrix; that matrix is referred to as the <strong>transformation matrix</strong>.</p>
<p>So in the equations above, <mathjax>$\mathbf A$</mathjax> is the transformation matrix.</p>
<p>To reiterate:</p>
<ul>
<li>any matrix-vector product is a linear transformation</li>
<li>any linear transformation can be expressed in terms of a matrix-vector product</li>
</ul>
<h4>矩阵相乘</h4>
<h4>Matrix-matrix products</h4>
<p>To multiply two matrices, one must have the same number of columns as the other has rows. That is, you can only multiply an <mathjax>$m \times n$</mathjax> matrix with an <mathjax>$n \times p$</mathjax> matrix. The resulting matrix will be of <mathjax>$m \times p$</mathjax> dimensions.</p>
<p>That is, if <mathjax>$A \in \mathbb R^{m \times n}, B \in \mathbb R^{n \times p}$</mathjax>, then <mathjax>$C = AB \in \mathbb R^{m \times p}$</mathjax>.</p>
<p>The resulting matrix is defined as such:</p>
<p><mathjax>$$
C_{ij} = \sum^{n}_{k=1} A_{ik} B_{kj}
$$</mathjax></p>
<p>You can break the terms out into individual matrix-vector products. Then you combine the resulting vectors to get the final matrix.</p>
<p>More formally, the <mathjax>$i$</mathjax>th column of the resulting product matrix is obtained by multiplying <mathjax>$\mathbf A$</mathjax> with the <mathjax>$i$</mathjax>th column of <mathjax>$\mathbf B$</mathjax> for <mathjax>$i=1,2,\dots,k$</mathjax>.</p>
<p><mathjax>$$
\begin{bmatrix}
1 &amp; 3 &amp; 2 \\
4 &amp; 0 &amp; 1
\end{bmatrix}
\times
\begin{bmatrix}
1 &amp; 3 \\
0 &amp; 1 \\
5 &amp; 2
\end{bmatrix}
$$</mathjax></p>
<p>The product would be:</p>
<p><mathjax>$$
\begin{aligned}
\begin{bmatrix}
1 &amp; 3 &amp; 2 \\
4 &amp; 0 &amp; 1
\end{bmatrix}
\times
\begin{bmatrix}
1 \\
0 \\
5
\end{bmatrix}
&amp;=
\begin{bmatrix}
11 \\
9
\end{bmatrix}
\\
\begin{bmatrix}
1 &amp; 3 &amp; 2 \\
4 &amp; 0 &amp; 1
\end{bmatrix}
\times
\begin{bmatrix}
3 \\
1 \\
2
\end{bmatrix}
&amp;=
\begin{bmatrix}
10 \\
14
\end{bmatrix}
\\
\begin{bmatrix}
1 &amp; 3 &amp; 2 \\
4 &amp; 0 &amp; 1
\end{bmatrix}
\times
\begin{bmatrix}
1 &amp; 3 \\
0 &amp; 1 \\
5 &amp; 2
\end{bmatrix}
&amp;=
\begin{bmatrix}
11 &amp; 10 \\
9 &amp; 14
\end{bmatrix}
\end{aligned}
$$</mathjax></p>
<h5>矩阵相乘的属性</h5>
<h5>Properties of matrix multiplication</h5>
<p>Matrix multiplication is <em>not commutative</em>. That is, for matrices <mathjax>$\mathbf A$</mathjax> and <mathjax>$\mathbf B$</mathjax>, in general  <mathjax>$\mathbf A \times \mathbf B \neq \mathbf B \times \mathbf A$</mathjax>. They may not even be of the same dimension.</p>
<p>Matrix multiplication is <em>associative</em>. For example, for matrices <mathjax>$\mathbf A, \mathbf B, \mathbf C$</mathjax>, we can say that:</p>
<p><mathjax>$$ \mathbf A \times \mathbf B \times \mathbf C = \mathbf A \times (\mathbf B \times \mathbf C) = (\mathbf A \times \mathbf B) \times \mathbf C $$</mathjax></p>
<p>There is also an <em>identity</em> matrix <mathjax>$\mathbf I$</mathjax>. For any matrix <mathjax>$\mathbf A$</mathjax>, we can say that:</p>
<p><mathjax>$$ \mathbf A \times \mathbf I = \mathbf I \times \mathbf A = \mathbf A $$</mathjax></p>
<h3>Hadamard积</h3>
<h3>Hadamard product</h3>
<p>The <strong>Hadamard product</strong>, sometimes called the <strong>element-wise product</strong>, is another way of multiplying matrices, but only for matrices of the same size. It is usually denoted with <mathjax>$\odot$</mathjax>. It is simply:</p>
<p><mathjax>$$
(A \odot B)_{n,m} = A_{n,m}B_{n,m}
$$</mathjax></p>
<p>It returns a matrix of the same size as the input matrices.</p>
<p>Haramard multiplication has the following propertise:</p>
<ul>
<li><em>commutativity</em>: <mathjax>$A \odot B = B \odot A$</mathjax></li>
<li><em>associativity</em>: <mathjax>$A \odot (B \odot C) = (A \odot B) \odot C$</mathjax></li>
<li><em>distributivity</em>: <mathjax>$A \odot (B+C) = A \odot B + A \odot C$</mathjax></li>
</ul>
<h3>单位矩阵</h3>
<h3>The identity matrix</h3>
<p>The <strong>identity matrix</strong> is an <mathjax>$n \times n$</mathjax> matrix where every component is 0, except for those along the diagonal:</p>
<p><mathjax>$$ \mathbf I_n = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \dots &amp; 1
\end{bmatrix}
$$</mathjax></p>
<p>When you multiply the identity matrix by any vector:</p>
<p><mathjax>$$ \mathbf I_n \vec{x} \, | \, \vec{x} \in \mathbb R^n = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\
        x_n \end{bmatrix} = \vec{x} $$</mathjax></p>
<p>That is, a vector multiplied by the identity matrix equals itself.</p>
<h3>对角矩阵</h3>
<h3>Diagonal matrices</h3>
<p>A <strong>diagonal matrix</strong> is a matrix where all non-diagonal elements are 0, typically denoted <mathjax>$\Diag(x_1, x_2, \dots, x_n)$</mathjax>, where</p>
<p><mathjax>$$
D_{ij} =
\begin{cases}
d_i &amp; i=j \\
0 &amp; i \neq j
\end{cases}
$$</mathjax></p>
<p>So the identity matrix is <mathjax>$I = \Diag(1, 1, \dots, 1)$</mathjax>.</p>
<h3>三角矩阵</h3>
<h3>Triangular matrices</h3>
<p>We say that a matrix is <strong>upper-triangular</strong> if all of its elements below the diagonal are zero.</p>
<p>Similarly, a matrix is <strong>lower-triangular</strong> if all of its elements above the diagonal are zero.</p>
<p>A matrix is <strong>diagonal</strong> if it is both upper-triangular and lower-triangular.</p>
<h3>矩阵的性质</h3>
<h3>Some properties of matrices</h3>
<h4>结合律</h4>
<h4>Associative property</h4>
<p><mathjax>$$ (AB)C = A(BC) $$</mathjax></p>
<p>i.e. it doesn't matter where the parentheses are.</p>
<p>This applies to compositions as well:</p>
<p><mathjax>$$ (h \circ f) \circ g = h \circ (f \circ g) $$</mathjax></p>
<h4>分配律</h4>
<h4>Distributive property</h4>
<p><mathjax>$$
\begin{aligned}
A(B+C) &amp;= AB + AC \\
(B+C)A &amp;= BA + CA
\end{aligned}
$$</mathjax></p>
<h3>逆矩阵</h3>
<h3>Matrix inverses</h3>
<p>If <mathjax>$\mathbf A$</mathjax> is an <mathjax>$m \times m$</mathjax> matrix, and if it has an inverse, then:</p>
<p><mathjax>$$ \mathbf A \mathbf A^{-1} = \mathbf A^{-1} \mathbf A = \mathbf I $$</mathjax></p>
<p>Only square matrices can have inverses. An inverse does not exist for all square matrices, but those that have one are called <strong>invertible</strong> or <strong>non-singular</strong>, otherwise they are <strong>non-invertible</strong> or <strong>singular</strong>.</p>
<p>The inverse exists if and only if <mathjax>$A$</mathjax> is full rank.</p>
<p>The invertible matrices <mathjax>$A,B \in \mathbb R^{n \times n}$</mathjax> have the following properties:</p>
<ul>
<li><mathjax>$(A^{-1})^{-1} = A$</mathjax></li>
<li>If <mathjax>$Ax = B$</mathjax>, we can multiply by <mathjax>$A^{-1}$</mathjax> on both sides to obtain <mathjax>$x = A^{-1}b$</mathjax></li>
<li><mathjax>$(AB)^{-1} = B^{-1}A^{-1}$</mathjax></li>
<li><mathjax>$(A^{-1})^T = (A^T)^{-1}$</mathjax>; this matrix is often denoted <mathjax>$A^{-T}$</mathjax></li>
</ul>
<h4>伪逆</h4>
<h4>Pseudo-inverses</h4>
<p><mathjax>$A^{\dagger}$</mathjax> is a <strong>pseudo-inverse</strong> (sometimes called a <strong>Moore-Penrose inverse</strong>) of <mathjax>$A$</mathjax>, which may be non-square, if the following are satisfied:</p>
<ul>
<li><mathjax>$AA^{\dagger}A = A$</mathjax></li>
<li><mathjax>$A^{\dagger}AA^{\dagger} = A$</mathjax></li>
<li><mathjax>$(AA^{\dagger})^T = AA^{\dagger}$</mathjax></li>
<li><mathjax>$(A^{\dagger}A)^T = A^{\dagger}A$</mathjax></li>
</ul>
<p>A pseudo-inverse exists and is unique for any matrix <mathjax>$A$</mathjax>. If <mathjax>$A$</mathjax> is invertible, <mathjax>$A^{-1} = A^{\dagger}$</mathjax>.</p>
<h3>行列式</h3>
<h3>Matrix determinants</h3>
<p>The determinant of a square matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax> is a function <mathjax>$\Det : \mathbb R^{n \times n} \to \mathbb R$</mathjax>, denoted <mathjax>$|A|$</mathjax>, <mathjax>$\Det(A)$</mathjax>, or sometimes with the parentheses dropped, <mathjax>$\Det A$</mathjax>.</p>
<p>A more intuitive interpretation:</p>
<p>Say we are in a 2D space and we have some shape. It has some area. Then we apply transformations to that space. The determinant describes how that shape's area has been scaled as a result of the transformation.</p>
<p>This can be extended to 3D, replacing "area" with "volume".</p>
<p>With this interpretation it's clear that a determinant of 0 means scaling area or volume to 0, which indicates that space has been "compressed" to a line or a point (or a plane in the case of 3D).</p>
<h4><mathjax>$2 \times 2$</mathjax> 矩阵的逆和行列式</h4>
<h4>Inverse and determinant for a <mathjax>$2 \times 2$</mathjax> matrix</h4>
<p>Say you have the matrix:</p>
<p><mathjax>$$ \mathbf A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} $$</mathjax></p>
<p>You can calculate the inverse of this matrix as:</p>
<p><mathjax>$$ \mathbf A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix} $$</mathjax></p>
<p>Note that <mathjax>$\mathbf A^{-1}$</mathjax> is undefined if <mathjax>$ad-bc=0$</mathjax>, which means that <mathjax>$\mathbf A$</mathjax> is not invertible.</p>
<p>Intuitively:</p>
<ul>
<li>the inverse of a matrix essentially "undoes" the transformation that matrix represents</li>
<li>a determinant of 0 implies a transformation that squishes everything together in some way (e.g. into a line). This means that some vectors occupy the same position on the line.</li>
<li>by definition, a function takes one input and maps it to one output. So if we have (what used to be) different vectors mapped to the same position, we can't take that one same position and re-map it back to different vectors - that would require a function that gives different outputs for the same input.</li>
</ul>
<p>The denominator <mathjax>$ad-bc$</mathjax> is called the <strong>determinant</strong>. It is notated as:</p>
<p><mathjax>$$ \Det(\mathbf A) = |\mathbf A| = ad-db $$</mathjax></p>
<h4><mathjax>$n \times n$</mathjax> 矩阵的逆和行列式</h4>
<h4>Inverse and determinant for an <mathjax>$n \times n$</mathjax> matrix</h4>
<p>Say we have an <mathjax>$n \times n$</mathjax> matrix <mathjax>$\mathbf A$</mathjax>.</p>
<p>A submatrix of <mathjax>$\mathbf A_{ij}$</mathjax> is an <mathjax>$(n-1)\times(n-1)$</mathjax> matrix constructed from <mathjax>$\mathbf A$</mathjax> by ignoring the <mathjax>$i^{th}$</mathjax> row and the <mathjax>$j^{th}$</mathjax> column of <mathjax>$\mathbf A$</mathjax>, which we denote by <mathjax>$\mathbf A_{\lnot i, \lnot j}$</mathjax>.</p>
<p>You can calculate the determinant of an <mathjax>$n \times n$</mathjax> matrix <mathjax>$\mathbf A$</mathjax> by using some <mathjax>$i^{th}$</mathjax> row  of <mathjax>$\mathbf A$</mathjax>, where <mathjax>$1 \le i \le n$</mathjax>:</p>
<p><mathjax>$$
\Det(\mathbf A) = \sum_{j=1}^n (-1)^{i+j}a_{ij} \Det(\mathbf A_{\lnot i,\lnot j})
$$</mathjax></p>
<p>All the <mathjax>$\Det(\mathbf A_{ij})$</mathjax> eventually reduce to the determinant of a <mathjax>$2 \times 2$</mathjax> matrix.</p>
<h4>行列式的标量乘法</h4>
<h4>Scalar multiplication of determinants</h4>
<p>For an <mathjax>$n \times n$</mathjax> matrix <mathjax>$\mathbf A$</mathjax>,</p>
<p><mathjax>$$ \Det(k\mathbf A) = k^n \Det(\mathbf A) $$</mathjax></p>
<h4>对角矩阵和三角矩阵的行列式</h4>
<h4>Determinant of diagonal or triangular matrix</h4>
<p>The determinant of a diagonal or triangular matrix is simply the product of the elements along its diagonal.</p>
<h4>行列式的性质</h4>
<h4>Properties of determinants</h4>
<table>
<thead>
<tr>
<th>- For <mathjax>$A \in \mathbb R^{n \times n}, t \in \mathbb R$</mathjax>, multiplying a single row by the scalar <mathjax>$t$</mathjax> yields a new matrix <mathjax>$B$</mathjax>, for which  $</th>
<th>B</th>
<th>= t</th>
<th>A</th>
<th>$.</th>
</tr>
</thead>
<tbody>
<tr>
<td>- For <mathjax>$A,B \in \mathbb R^{n \times n}$</mathjax>, $</td>
<td>AB</td>
<td>=</td>
<td>A</td>
<td>B</td>
</tr>
<tr>
<td>- For <mathjax>$A,B \in \mathbb R^{n \times n}$</mathjax>, $</td>
<td>A</td>
<td>= 0<mathjax>$ if $</mathjax>A$ is singular (i.e. non-invertible).</td>
<td></td>
<td></td>
</tr>
<tr>
<td>- For <mathjax>$A,B \in \mathbb R^{n \times n}$</mathjax>, $</td>
<td>A</td>
<td>^{-1} = \frac{1}{</td>
<td>A</td>
<td>}<mathjax>$ if $</mathjax>A$ is non-singular (i.e. invertible).</td>
</tr>
<tr>
<td>- For <mathjax>$A,B \in \mathbb R^{n \times n}$</mathjax>, if two rows of <mathjax>$A$</mathjax> are swapped to produce <mathjax>$B$</mathjax>, then <mathjax>$\Det(A) = -\Det(B)$</mathjax></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- The determinant of a matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax> is non-zero if and only if it has full rank; this also means you can check if a <mathjax>$A$</mathjax> is invertible by checking that its determinant is non-zero.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3>矩阵转置</h3>
<h3>Transpose of a matrix</h3>
<p>The <strong>transpose</strong> of a matrix <mathjax>$\mathbf A$</mathjax> is that matrix with its columns and rows swapped, denoted <mathjax>$\mathbf A^T$</mathjax>.</p>
<p>More formally, let <mathjax>$\mathbf A$</mathjax> be an <mathjax>$m \times n$</mathjax> matrix, and let <mathjax>$\mathbf B = \mathbf A^T$</mathjax>. Then <mathjax>$\mathbf B$</mathjax> is an <mathjax>$n \times m$</mathjax> matrix, and <mathjax>$B_{ij} = A_{ji}$</mathjax>.</p>
<ul>
<li><strong>Transpose of determinants</strong>: The determinant of a transpose is the same as the determinant of the original matrix: <mathjax>$\Det(\mathbf A^T) = \Det(\mathbf A)$</mathjax></li>
<li><strong>Transposes of sums</strong>: With matrices <mathjax>$A, B, C$</mathjax> where <mathjax>$C = A + B$</mathjax>, then <mathjax>$C^T = (A+B)^T = A^T + B^T$</mathjax></li>
<li><strong>Transposes of inverses</strong>: The transpose of the inverse is equal to the inverse of the transpose: <mathjax>$(A^{-1})^T = (A^T)^{-1}$</mathjax></li>
<li><strong>Transposes of multiplication</strong>: <mathjax>$(AB)^T = B^TA^T$</mathjax></li>
<li><strong>Transpose of a vector</strong>: for two column vectors <mathjax>$\vec{a}, \vec{b}$</mathjax>, we know that <mathjax>$\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a} = \vec{a}^T\vec{b}$</mathjax>, from which we can derive: <mathjax>$(\mathbf A \vec{x}) \cdot \vec{y} = \vec{x} \cdot (\mathbf A^T \vec{y})$</mathjax> (proof omitted).</li>
</ul>
<h3>对称矩阵</h3>
<h3>Symmetric matrices</h3>
<p>A square matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax> is <strong>symmetric</strong> if <mathjax>$A = A^T$</mathjax>.</p>
<p>It is <strong>anti-symmetric</strong> if <mathjax>$A = -A^T$</mathjax>.</p>
<p>For any square matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax>, the matrix <mathjax>$A + A^T$</mathjax> is symmetric and the matrix <mathjax>$A - A^T$</mathjax> is anti-symmetric. Thus any such <mathjax>$A$</mathjax> can be represented as a sum of a symmetric and an anti-symmetric matrix:</p>
<p><mathjax>$$
A = \frac{1}{2}(A + A^T) + \frac{1}{2}(A - A^T)
$$</mathjax></p>
<p>Symmetric matrices have many nice properties.</p>
<p>The set of all symmetric matrices of dimension <mathjax>$n$</mathjax> is often denoted as <mathjax>$\mathbb S^n$</mathjax>, so you can denote a symmetric <mathjax>$n \times n$</mathjax> matrix <mathjax>$A$</mathjax> as <mathjax>$A \in \mathbb S^n$</mathjax>.</p>
<h4>二次型</h4>
<h4>The quadratic form</h4>
<p>Given a square matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax> and a vector <mathjax>$x \in \mathbb R$</mathjax>, the scalar value <mathjax>$x^TAx$</mathjax> is called a <strong>quadratic form</strong>:</p>
<p><mathjax>$$
x^TAx = \sum^n_{i=1} \sum^n_{j=1} A_{ij}x_ix_j
$$</mathjax></p>
<p>Here <mathjax>$A$</mathjax> is typically assumed to be symmetric.</p>
<h4>对称矩阵的类型</h4>
<h4>Types of symmetric matrices</h4>
<p>Given a symmetric matrix <mathjax>$A \in \mathbb S^n$</mathjax>...</p>
<ul>
<li><mathjax>$A$</mathjax> is <strong>positive definite</strong> (PD) if for all non-zero vectors <mathjax>$x \in \mathbb R^n, x^TAx &gt; 0$</mathjax>.<ul>
<li>This is often denoted <mathjax>$A \succ 0$</mathjax> or <mathjax>$A &gt; 0$</mathjax>.</li>
<li>The set of all positive definite matrices is denoted <mathjax>$\mathbb S^n_{++}$</mathjax>.</li>
</ul>
</li>
<li><mathjax>$A$</mathjax> is <strong>positive semidefinite</strong> (PSD) if for all vectors <mathjax>$x \in \mathbb R^n, x^TAx \geq 0$</mathjax>.<ul>
<li>This is often denoted <mathjax>$A \succeq 0$</mathjax> or <mathjax>$A \geq 0$</mathjax>.</li>
<li>The set of all positive semidefinite matrices is denoted <mathjax>$\mathbb S^n_{+}$</mathjax>.</li>
</ul>
</li>
<li><mathjax>$A$</mathjax> is <strong>negative definite</strong> (ND) if for all non-zero vectors <mathjax>$x \in \mathbb R^n, x^TAx &lt; 0$</mathjax>.<ul>
<li>This is often denoted <mathjax>$A \prec 0$</mathjax> or <mathjax>$A &lt; 0$</mathjax>.</li>
</ul>
</li>
<li><mathjax>$A$</mathjax> is <strong>negative semidefinite</strong> (NSD) if for all vectors <mathjax>$x \in \mathbb R^n, x^TAx \leq 0$</mathjax>.<ul>
<li>This is often denoted <mathjax>$A \preceq 0$</mathjax> or <mathjax>$A \leq 0$</mathjax>.</li>
</ul>
</li>
<li><mathjax>$A$</mathjax> is <strong>indefinite</strong> if it is neither positive semidefinite nor negative semidefinite, that is, if there exists <mathjax>$x_1, x_2 \in \mathbb R^n$</mathjax> such that <mathjax>$x_1^TAx_1 &gt; 0$</mathjax> and <mathjax>$x_2^TAx_2 &lt; 0$</mathjax>.</li>
</ul>
<p>Some other properties of note:</p>
<ul>
<li>If <mathjax>$A$</mathjax> is positive definite, then <mathjax>$-A$</mathjax> is negative definite and vice versa.</li>
<li>If <mathjax>$A$</mathjax> is positive semidefinite, then <mathjax>$-A$</mathjax> is negative semidefinite and vice versa.</li>
<li>If <mathjax>$A$</mathjax> is indefinite, then <mathjax>$-A$</mathjax> is also indefinite and vice versa.</li>
<li>Positive definite and negative definite matrices are always invertible.</li>
<li>For any matrix <mathjax>$A \in R^{m \times n}$</mathjax>, which does not need to be symmetric or square, the matrix <mathjax>$G = A^TA$</mathjax>, called a <strong>Gram matrix</strong>, is always positive semidefinite.<ul>
<li>If <mathjax>$m \geq n$</mathjax> and <mathjax>$A$</mathjax> is full rank, then <mathjax>$G$</mathjax> is positive definite.</li>
</ul>
</li>
</ul>
<p>Essentially, "positive semidefinite" is to matrices as "non-negative" is to scalar values (and "positive definite" as "positive" is to scalar values).</p>
<h3>矩阵迹</h3>
<h3>The Trace</h3>
<p>The <strong>trace</strong> of a square matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax> is denoted <mathjax>$\Tr(A)$</mathjax> and is the sum of the diagonal elements in the matrix:</p>
<p><mathjax>$$
\Tr(A) = \sum^n_{i=1} A_{ii}
$$</mathjax></p>
<p>The trace has the following properties:</p>
<ul>
<li><mathjax>$\Tr(A) = \Tr(A^T)$</mathjax></li>
<li>For <mathjax>$B \in \mathbb R^{n \times n}$</mathjax>, <mathjax>$\Tr(A+B) = \Tr(A) + \Tr(B)$</mathjax></li>
<li>For <mathjax>$t \in \mathbb R$</mathjax>, <mathjax>$\Tr(tA) = t \Tr(A)$</mathjax></li>
<li>If <mathjax>$AB$</mathjax> is square, then <mathjax>$\Tr(AB) = \Tr(BA)$</mathjax></li>
<li>If <mathjax>$ABC$</mathjax> is square, then <mathjax>$\Tr(ABC) = \Tr(BCA) = \Tr(CAB)$</mathjax> and so on for the product of more matrices</li>
</ul>
<h3>正交矩阵</h3>
<h3>Orthogonal matrix</h3>
<p>Say we have a <mathjax>$n \times k$</mathjax> matrix <mathjax>$\mathbf C$</mathjax>, whose column rows form an orthonormal set.</p>
<p>If <mathjax>$k=n$</mathjax> then <mathjax>$\mathbf C$</mathjax> is a square matrix (<mathjax>$n \times n$</mathjax>) and since <mathjax>$\mathbf C$</mathjax>'s columns are linearly independent, <mathjax>$\mathbf C$</mathjax> is invertible.</p>
<p>For an orthonormal matrix:</p>
<p><mathjax>$$
\begin{aligned}[c]
\mathbf C^T \mathbf C &amp;= \mathbf I_n \\
\mathbf C^{-1} \mathbf C &amp;= \mathbf I_n
\end{aligned}
\qquad
\begin{aligned}[c]
\therefore \mathbf C^T = \mathbf C^{-1}
\end{aligned}
$$</mathjax></p>
<p>When <mathjax>$\mathbf C$</mathjax> is an <mathjax>$n \times n$</mathjax> matrix (i.e. square) whose columns form an orthonormal set, we say that <mathjax>$\mathbf C$</mathjax> is an <em>orthogonal matrix</em>.</p>
<p>Orthogonal matrices have the property of <mathjax>$C^TC = I = CC^T$</mathjax>.</p>
<p>Orthogonal matrices also have the property that operating on a vector with an orthogonal matrix will not change its Euclidean norm, i.e. <mathjax>$||Cx||_2 = ||x||_2$</mathjax> for any <mathjax>$x \in \mathbb R^n$</mathjax>.</p>
<h4>正交矩阵角度和长度保留</h4>
<h4>Orthogonal matrices preserve angles and lengths</h4>
<p>For an orthogonal matrix  <mathjax>$\mathbf C$</mathjax>, when you multiply <mathjax>$\mathbf C$</mathjax> by some vector, the length and angle of the vector is preserved:</p>
<p><mathjax>$$
\begin{aligned}
||\vec{x}|| &amp;= ||\mathbf C \vec{x}|| \\
\cos\theta &amp;= \cos\theta_{\mathbf C}
\end{aligned}
$$</mathjax></p>
<h3>伴随矩阵</h3>
<h3>Adjoints</h3>
<p>The <strong>classical adjoint</strong>, often just called the <strong>adjoint</strong> of a matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax> is denoted <mathjax>$\Adj(A)$</mathjax> and defined as:</p>
<p><mathjax>$$
\Adj(A)_{ij} = (-1)^{i+j}|A_{\lnot j, \lnot i}|
$$</mathjax></p>
<p>Note that the indices are switched in <mathjax>$A_{\lnot j, \lnot i}$</mathjax>.</p>
<h2>子空间</h2>
<h2>Subspaces</h2>
<p>Say we have set of vectors <mathjax>$V$</mathjax> which is a subset of <mathjax>$\mathbb R^n$</mathjax>, that is, every vector in the set has <mathjax>$n$</mathjax> components.</p>
<figure><img alt="A subspace" src="../assets/subspace.svg" /><figcaption>A subspace</figcaption>
</figure>
<p><mathjax>$V$</mathjax> is a  <em>linear subspace</em> of <mathjax>$\mathbb R^n$</mathjax> if</p>
<ul>
<li><mathjax>$V$</mathjax> contains the zero vector <mathjax>$\vec{0}$</mathjax></li>
<li>for a vector <mathjax>$\vec{x}$</mathjax> in <mathjax>$V$</mathjax>, <mathjax>$c\vec{x}$</mathjax> (where <mathjax>$c \in \mathbb R$</mathjax>) must also be in <mathjax>$V$</mathjax>, i.e. <em>closure under scalar multiplcation</em>.</li>
<li>for a vector <mathjax>$\vec{a}$</mathjax> in <mathjax>$V$</mathjax> and a vector <mathjax>$\vec{b}$</mathjax> in <mathjax>$V$</mathjax>, <mathjax>$\vec{a} + \vec{b}$</mathjax> must also be in <mathjax>$V$</mathjax>, i.e. <em>closure under addition</em>.</li>
</ul>
<h4>示例:</h4>
<h4>Example:</h4>
<p>Say we have the set of vectors:</p>
<p><mathjax>$$ S = \left\lbrace \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb R^2 \,:\, x_1 \ge 0 \right\rbrace $$</mathjax></p>
<p>which is the shaded area below.</p>
<figure><img alt="Is the shaded set of vectors $S$ a subspace of $\mathbb R^2$?" src="../assets/subspace_p.svg" /><figcaption>Is the shaded set of vectors <mathjax>$S$</mathjax> a subspace of <mathjax>$\mathbb R^2$</mathjax>?</figcaption>
</figure>
<p>Is <mathjax>$S$</mathjax> a subspace of <mathjax>$\mathbb R^2$</mathjax>?</p>
<ul>
<li>It does contain the zero vector</li>
<li>
<p>It is closed under addition:</p>
<p><mathjax>$$ \begin{bmatrix} a \\ b \end{bmatrix} + \begin{bmatrix} c \\ d \end{bmatrix} =
\begin{bmatrix} a + c \\ b + d \end{bmatrix} $$</mathjax></p>
<p>Since a &amp; b are both &gt; 0 (that was a criteria for the set), the a+b will also be<br />
greater than 0, so it will also be in the set (there were no constraints on the<br />
second component so it doesn't matter what that is)</p>
</li>
<li>
<p>It is NOT closed under multiplcation:</p>
<p><mathjax>$$ -1\begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} -a \\ -b \end{bmatrix} $$</mathjax></p>
<p>Since a is &gt;= 0, -a will be &lt;= 0, which falls outside the constraints of the set<br />
and thus is not contained within the set.</p>
</li>
</ul>
<p>So no, this set is not a subspace of <mathjax>$\mathbb R^2$</mathjax>.</p>
<h3>张成和子空间</h3>
<h3>Spans and subspaces</h3>
<p>Let's say we have the set:</p>
<p><mathjax>$$ U = \Span(\vec{v_1}, \vec{v_2}, \vec{v_3}) $$</mathjax></p>
<p>where each vector has <mathjax>$n$</mathjax> components.<br />
Is this a valid subspace of <mathjax>$\mathbb R^n$</mathjax>?</p>
<p>Since the span represents all the linear combinations of those vectors, we can define an arbitrary vector in the set as:</p>
<p><mathjax>$$ \vec{x} = c_1\vec{v_1} + c_2\vec{v_2} + c_3\vec{v_3} $$</mathjax></p>
<ul>
<li>
<p>the set does contain the zero vector:</p>
<p><mathjax>$$ 0\vec{v_1} + 0\vec{v_2} + 0\vec{v_3} = \vec{0} $$</mathjax></p>
</li>
<li>
<p>it is closed under multiplication, since the following is just another linear combination:</p>
<p><mathjax>$$ a\vec{x} = ac_1\vec{v_1} + ac_2\vec{v_2} + ac_3\vec{v_3} $$</mathjax></p>
</li>
<li>
<p>it is closed under addition, since if we take another arbitrary vector in the set:</p>
<p><mathjax>$$ \vec{y} = d_1\vec{v_1} + d_2\vec{v_2} + d_3\vec{v_3} $$</mathjax></p>
<p>and add them:</p>
<p><mathjax>$$ \vec{x} + \vec{y} = (c_1+d_1)\vec{v_1} + (c_2+d_2)\vec{v_2} + (c_3+d_3)\vec{v_3} $$</mathjax></p>
<p>that's also just another linear combination in the set.</p>
</li>
</ul>
<h3>子空间的基</h3>
<h3>Basis of a subspace</h3>
<p>If we have a subspace <mathjax>$V = \Span(S)$</mathjax> where the set of vectors <mathjax>$S = \vec{v_1}, \vec{v_2}, \dots, \vec{v_n}$</mathjax> is linearly independent, then we can say that <mathjax>$S$</mathjax> is a <em>basis</em> for <mathjax>$V$</mathjax>.</p>
<p>A set <mathjax>$S$</mathjax> is the <strong>basis</strong> for a subspace <mathjax>$V$</mathjax> if <mathjax>$S$</mathjax> is linearly independent and its span defines <mathjax>$V$</mathjax>. In other words, the basis is the <em>minimum</em> set of vectors that spans the subspace that it is a basis of.</p>
<p>All bases for a subspace will have the same number of elements.</p>
<p>Intuitively, the basis of a subspace is a set of vectors that can be linearly combined to describe any vector in that subspace. For example, the vectors [0,1] and [1,0] form a basis for <mathjax>$\mathbb R^2$</mathjax>.</p>
<h3>子空间的维度</h3>
<h3>Dimension of a subspace</h3>
<p>The <em>dimension</em> of a subspace is the number of elements in a basis for that subspace.</p>
<h3>矩阵的零空间</h3>
<h3>Nullspace of a matrix</h3>
<p>Say we have:</p>
<p><mathjax>$$
\mathbf A \vec{x} = \vec{0}
$$</mathjax></p>
<p>If you have a set <mathjax>$N$</mathjax> of all <mathjax>$x \in \mathbb R^n$</mathjax> that satisfies this equation, do you have a valid subspace?</p>
<p>Of course if <mathjax>$\vec{x} = \vec{0}$</mathjax> this equation is satisfied. So we know the zero vector is part of this set (which is a requirement for a valid subspace).</p>
<p>The other two properties (closure under multiplication and addition) necessary for a subspace also hold:</p>
<p><mathjax>$$
\begin{aligned}
A(\vec{v_1} + \vec{v_2}) &amp;= A\vec{v_1} + A\vec{v_2} = \vec{0} \\
A(c\vec{v_1}) &amp;= \vec{0}
\end{aligned}
$$</mathjax></p>
<p>and of course <mathjax>$\vec{0}$</mathjax> is in the set <mathjax>$N$</mathjax>.</p>
<p>So yes, the set <mathjax>$N$</mathjax> is a valid subspace, and it is a special subspace: the <strong>nullspace</strong> of <mathjax>$\mathbf A$</mathjax>, notated:</p>
<p><mathjax>$$ N(\mathbf A) $$</mathjax></p>
<p>That is, the nullspace for a matrix <mathjax>$\mathbf A$</mathjax> is the subspace described by the set of vectors which yields the zero vector which multiplied by <mathjax>$\mathbf A$</mathjax>, that is, the set of vectors which are the solutions for <mathjax>$\vec{x}$</mathjax> in:</p>
<p><mathjax>$$
\mathbf A \vec{x} = \vec{0}
$$</mathjax></p>
<p>Or, more formally, if <mathjax>$\mathbf A$</mathjax> is an <mathjax>$m \times n$</mathjax> matrix:</p>
<p><mathjax>$$ N(\mathbf A) = \{ \vec{x} \in \mathbb R^n \, | \, \mathbf A \vec{x} = \vec{0} \} $$</mathjax></p>
<p>The nullspace for a matrix <mathjax>$A$</mathjax> may be notated <mathjax>$\mathcal N(A)$</mathjax>.</p>
<p>To put nullspace (or "kernel") another way, it is space of all vectors that map to the zero vector after applying the transformation the matrix represents.</p>
<h4>零空间与线性无关</h4>
<h4>Nullspace and linear independence</h4>
<p>If you take each column in a matrix <mathjax>$\mathbf A$</mathjax> as a vector <mathjax>$\vec{v_i}$</mathjax>, that set of vectors is linearly independent if the nullspace of <mathjax>$\mathbf A$</mathjax> consists of <em>only</em> the zero vector. That is, if:</p>
<p><mathjax>$$ N(\mathbf A) = \{\vec{0}\} $$</mathjax></p>
<p>The intuition behind this is because, if the linear combination of a set of vectors can only equal the zero vector if all of its coefficients are zero (that is, its coefficients are components of the zero vector), then it is linearly independent:</p>
<p><mathjax>$$ x_1\vec{v_1} + x_2\vec{v_2} + \dots + x_n\vec{v_n} = \vec{0} \; \text{iff} \; \vec{x} =
\begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} $$</mathjax></p>
<h4>零化</h4>
<h4>Nullity</h4>
<p>The <strong>nullity</strong> of a nullspace is its dimension, that is, it is the number of elements in a basis for that nullspace.</p>
<p><mathjax>$$ \Dim(N(\mathbf A)) = \text{nullity}(N(\mathbf A)) $$</mathjax></p>
<h4>左零空间</h4>
<h4>Left nullspace</h4>
<p>The <strong>left nullspace</strong> of a matrix <mathjax>$\mathbf A$</mathjax> is the nullspace of its transpose, that is <mathjax>$N(\mathbf A^T)$</mathjax> :</p>
<p><mathjax>$$ N(\mathbf A^T)  = \{ \vec{x} \, | \, \vec{x}^T \mathbf A = \vec{0}^T \} $$</mathjax></p>
<h3>列空间</h3>
<h3>Columnspace</h3>
<p>Again, a matrix can be represented as a set of column vectors.<br />
The <strong>columnspace</strong> of a matrix (also called the <strong>range</strong> of the matrix) is all the linear combinations (i.e. the span) of these column vectors:</p>
<p><mathjax>$$
\begin{aligned}
\mathbf A &amp;= \begin{bmatrix} \vec{v_1} &amp; \vec{v_2} &amp; \dots &amp; \vec{v_n} \end{bmatrix} \\
C(\mathbf A) &amp;= \Span(\vec{v_1}, \vec{v_2}, \dots, \vec{v_n})
\end{aligned}
$$</mathjax></p>
<p>Because any span is a valid subspace, the columnspace of a matrix is a valid subspace.</p>
<p>So if you expand out the matrix-vector product, you'll see that every matrix-vector product is within that matrix's columnspace:</p>
<p><mathjax>$$
\begin{aligned}
\{ \mathbf A \vec{x} \, &amp;| \, \vec{x} \in \mathbb R^n \} \\
\mathbf A \vec{x} &amp;= x_1\vec{v_1} + x_2\vec{v_2} + \dots + x_n\vec{v_n} \\
\mathbf A \vec{x} &amp;= C(\mathbf A)
\end{aligned}
$$</mathjax></p>
<p>That is, for any vector in the space <mathjax>$\mathbb R^n$</mathjax>, multiplying the matrix by it just yields another linear combination of that matrix's column vectors. Therefore it is also in the columnspace.</p>
<p>The columnspace (range) for a matrix <mathjax>$A$</mathjax> may be notated <mathjax>$\mathcal R(A)$</mathjax>.</p>
<h4>列空间的秩</h4>
<h4>Rank of a columnspace</h4>
<p>The <strong>column rank</strong> of a columnspace is its dimension, that is, it is the number of elements in a basis for that columnspace (i.e. the largest number of columns of the matrix which constitute a linearly independent set):</p>
<p><mathjax>$$ \Dim(C(\mathbf A)) = \Rank(C(\mathbf A)) $$</mathjax></p>
<h4>行空间</h4>
<h4>Rowspace</h4>
<p>The <strong>rowspace</strong> of a matrix <mathjax>$\mathbf A$</mathjax> is the columnspace of <mathjax>$\mathbf A^T$</mathjax>, i.e. <mathjax>$C(\mathbf A^T)$</mathjax>.</p>
<p>The <strong>row rank</strong> of a matrix is similarly the number of elements in a basis for that rowspace.</p>
<h3>秩</h3>
<h3>Rank</h3>
<p>Note that for any matrix <mathjax>$A$</mathjax>, the column rank and the row rank are equal, so they are typically just referred to as <mathjax>$\Rank(A)$</mathjax>.</p>
<p>The rank has some properties:</p>
<ul>
<li>For <mathjax>$A \in \mathbb R^{m \times n}$</mathjax>, <mathjax>$\Rank(A) \leq \min(m, n)$</mathjax>. If <mathjax>$\Rank(A) = \min(m,n)$</mathjax>, then <mathjax>$A$</mathjax> is said to be <strong>full rank</strong>.</li>
<li>For <mathjax>$A \in \mathbb R^{m \times n}, \Rank(A) = \Rank(A^T)$</mathjax></li>
<li>For <mathjax>$A \in \mathbb R^{m \times n}, B \in \mathbb R^{n \times p}, \Rank(AB) \leq \min(\Rank(A), \Rank(B))$</mathjax></li>
<li>For <mathjax>$A,B \in \mathbb R^{m \times n}, \Rank(A+B) \leq \Rank(A) + \Rank(B)$</mathjax></li>
</ul>
<p>The rank of a transformation refers to the number of dimensions in the output.</p>
<p>A matrix is full-rank if it has rank equal to the number of dimensions in its originating space. I.e. they represent a transformation that preserves the dimensionality (it does not collapse it to a lower dimension space).</p>
<h3>标准基</h3>
<h3>The standard basis</h3>
<p>The set of column vectors in an identity matrix <mathjax>$\mathbf I_n$</mathjax> is known as the <strong>standard basis for <mathjax>$\mathbb R^n$</mathjax></strong>.</p>
<p>Each of those column vectors is notated <mathjax>$\vec{e_i}$</mathjax>. E.g., in an identity matrix, the column vector:</p>
<p><mathjax>$$ \begin{bmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} = \vec{e_1} $$</mathjax></p>
<p>For a transformation <mathjax>$T(\vec{x})$</mathjax>, its transformation matrix <mathjax>$\mathbf A$</mathjax> can be expressed as:</p>
<p><mathjax>$$ \mathbf A = \begin{bmatrix} T(\vec{e_1}) &amp; T(\vec{e_2}) &amp; \dots &amp; T(\vec{e_n})
    \end{bmatrix} $$</mathjax></p>
<h3>正交互补</h3>
<h3>Orthogonal complements</h3>
<p>Given that <mathjax>$V$</mathjax> is some subspace of <mathjax>$\mathbb R^n$</mathjax>, the <strong>orthogonal complement of <mathjax>$V$</mathjax></strong>, notated <mathjax>$V^{\perp}$</mathjax> :</p>
<p><mathjax>$$ V^{\perp} = \{ \vec{x} \in \mathbb R^n \, | \, \vec{x} \cdot \vec{v} = 0 \, \forall \, \vec{v} \in V \} $$</mathjax></p>
<p>That is, the orthogonal complement of a subspace <mathjax>$V$</mathjax> is the set of all vectors where the dot product of each vector with each vector from <mathjax>$V$</mathjax> is 0, that is where all vectors in the set are orthogonal to all vectors in <mathjax>$V$</mathjax>.</p>
<p><mathjax>$V^{\perp}$</mathjax> is a subspace (proof omitted).</p>
<h4>列空间, 零空间, 转置</h4>
<h4>Columnspaces, nullspaces, and transposes</h4>
<p><mathjax>$C(\mathbf A)$</mathjax> is the orthogonal complement to <mathjax>$N(\mathbf A^T)$</mathjax>, and vice versa:</p>
<p><mathjax>$$
\begin{aligned}
N(\mathbf A^T) &amp;= C(\mathbf A)^{\perp} \\
N(\mathbf A^T)^{\perp} &amp;= C(\mathbf A)
\end{aligned}
$$</mathjax></p>
<p><mathjax>$C(\mathbf A^T)$</mathjax> is the orthogonal complement to <mathjax>$N(\mathbf A)$</mathjax>, and vice versa.</p>
<p><mathjax>$$
\begin{aligned}
N(\mathbf A) &amp;= C(\mathbf A^T)^{\perp} \\
N(\mathbf A)^{\perp} &amp;= C(\mathbf A^T)
\end{aligned}
$$</mathjax></p>
<p>As a reminder, columnspaces and nullspaces are spans, i.e. sets of linear combinations, i.e. lines, so these lines are orthogonal to each other.</p>
<h4>维度和正交补</h4>
<h4>Dimensionality and orthogonal complements</h4>
<p>For <mathjax>$V$</mathjax>, a subspace of <mathjax>$\mathbb R^n$</mathjax> :</p>
<p><mathjax>$$ \Dim(V) + \Dim(V^{\perp}) = n$$</mathjax></p>
<p>(proof omitted here)</p>
<h4>正交补的交集</h4>
<h4>The intersection of orthogonal complements</h4>
<p>Since the vectors between a subspace and its orthogonal complement are all orthogonal:</p>
<p><mathjax>$$ V \cap V^{\perp} = \{ \vec{0} \} $$</mathjax></p>
<p>That is, the only vector which exists both in a subspace and its orthogonal complement is the zero vector.</p>
<h3>一组基下的坐标</h3>
<h3>Coordinates with respect to a basis</h3>
<p>With a subspace <mathjax>$V$</mathjax> of <mathjax>$\mathbb R^n$</mathjax>, we have <mathjax>$V$</mathjax>'s basis, <mathjax>$B$</mathjax>, as</p>
<p><mathjax>$$ B = \{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_k} \} $$</mathjax></p>
<p>We can describe any vector <mathjax>$\vec{a} \in V$</mathjax> as a linear combination of the vectors in its basis<br />
<mathjax>$B$</mathjax> :</p>
<p><mathjax>$$ \vec{a} = c_1\vec{v_1} + c_2\vec{v_2} + \dots + c_k\vec{v_k} $$</mathjax></p>
<p>We can take these coefficients <mathjax>$c_1, c_2, \dots, c_k$</mathjax> as <strong>the coordinates of <mathjax>$\vec{a}$</mathjax> with respect to <mathjax>$B$</mathjax></strong>, notated as:</p>
<p><mathjax>$$ [\vec{a}]_B = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_k \end{bmatrix} $$</mathjax></p>
<p>Basically what has happened here is a new coordinate system based off of the basis <mathjax>$B$</mathjax> is being used.</p>
<h5>示例</h5>
<h5>Example</h5>
<p>Say we have <mathjax>$\vec{v_1} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \vec{v_2} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$</mathjax>, where <mathjax>$B = \{\vec{v_1}, \vec{v_2}\}$</mathjax> is the basis for <mathjax>$\mathbb R^2$</mathjax>.</p>
<p>The point <mathjax>$(8,7)$</mathjax> in <mathjax>$\mathbb R^2$</mathjax> is equal to <mathjax>$3\vec{v_1} + 2\vec{v_2}$</mathjax>. If we set:</p>
<p><mathjax>$$ \vec{a} = 3\vec{v_1} + 2\vec{v_2} $$</mathjax></p>
<p>Then we can describe <mathjax>$\vec{a}$</mathjax> with respect to <mathjax>$B$</mathjax> :</p>
<p><mathjax>$$ [\vec{a}]_B = \begin{bmatrix} 3 \\ 2 \end{bmatrix} $$</mathjax></p>
<p>which looks like:</p>
<figure><img alt="Coordinates wrt a basis" src="../assets/coordinates_wrt_basis.svg" /><figcaption>Coordinates wrt a basis</figcaption>
</figure>
<h4>基变换</h4>
<h4>Change of basis matrix</h4>
<p>Given the basis:</p>
<p><mathjax>$$ B = \{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_k} \} $$</mathjax></p>
<p>and:</p>
<p><mathjax>$$ [\vec{a}]_B = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_k \end{bmatrix} $$</mathjax></p>
<p>say there is some <mathjax>$n \times k$</mathjax> matrix where the column vectors are the basis vectors:</p>
<p><mathjax>$$ \mathbf C  = [\vec{v_1}, \vec{v_2}, \dots, \vec{v_k}] $$</mathjax></p>
<p>We can do:</p>
<p><mathjax>$$ \mathbf C [\vec{a}]_B = \vec{a} $$</mathjax></p>
<p>The matrix <mathjax>$\mathbf C$</mathjax> is known as the <em>change of basis matrix</em> and allows us to get <mathjax>$\vec{a}$</mathjax> in standard coordinates.</p>
<h4>可逆基变换</h4>
<h4>Invertible change of basis matrix</h4>
<p>Given the basis of some subspace:</p>
<p><mathjax>$$ B = \{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_k} \} $$</mathjax></p>
<p>where <mathjax>$\vec{v_1}, \vec{v_2}, \dots, \vec{v_k} \in \mathbb R^n$</mathjax>, and we have a change of basis matrix:</p>
<p><mathjax>$$ \mathbf C  = [\vec{v_1}, \vec{v_2}, \dots, \vec{v_k}] $$</mathjax></p>
<p>Assume:</p>
<ul>
<li><mathjax>$\mathbf C$</mathjax> is invertible</li>
<li><mathjax>$\mathbf C$</mathjax> is square (that is, <mathjax>$k=n$</mathjax>, which implies that we have <mathjax>$n$</mathjax> basis vectors, that is, <mathjax>$B$</mathjax> is a basis for <mathjax>$\mathbb R^n$</mathjax>)</li>
<li><mathjax>$\mathbf C$</mathjax>'s columns are linearly independent (which they are because it is formed out of basis vectors, which by definition are linearly independent)</li>
</ul>
<p>Under these assumptions:</p>
<ul>
<li>If <mathjax>$\mathbf C$</mathjax> is invertible, the span of <mathjax>$B$</mathjax> is equal to <mathjax>$\mathbb R^n$</mathjax>.</li>
<li>If the span of <mathjax>$B$</mathjax> is equal to <mathjax>$\mathbb R^n$</mathjax>, <mathjax>$\mathbf C$</mathjax> is invertible.</li>
</ul>
<p>Thus:</p>
<p><mathjax>$$ [\vec{a}]_B = C^{-1}\vec{a} $$</mathjax></p>
<h4>一组基下的转换矩阵</h4>
<h4>Transformation matrix with respect to a basis</h4>
<p>Say we have a linear transformation <mathjax>$T: \mathbb R^n \to \mathbb R^n$</mathjax>, which we can express as <mathjax>$T(\vec{x}) = \mathbf A \vec{x}$</mathjax>. This is with respect to the standard basis; we can say <mathjax>$\mathbf A$</mathjax> is the transformation for <mathjax>$T$</mathjax> with respect to the standard basis.</p>
<p>Say we have another basis <mathjax>$B = \{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_n} \}$</mathjax> for <mathjax>$\mathbb R^n$</mathjax>, that is, it is a basis for for <mathjax>$\mathbb R^n$</mathjax>.</p>
<p>We could write:</p>
<p><mathjax>$$ [T(\vec{x})]_B = \mathbf D [\vec{x}]_B $$</mathjax></p>
<p>and we call <mathjax>$\mathbf D$</mathjax> the transformation matrix for <mathjax>$T$</mathjax> with respect to the basis <mathjax>$B$</mathjax>.</p>
<p>Then we have (proof omitted):</p>
<p><mathjax>$$ \mathbf D = \mathbf C^{-1} \mathbf A \mathbf C $$</mathjax></p>
<p>where:</p>
<ul>
<li><mathjax>$\mathbf D$</mathjax> is the transformation matrix for <mathjax>$T$</mathjax> with respect to the basis <mathjax>$B$</mathjax></li>
<li><mathjax>$\mathbf A$</mathjax> is the transformation matrix for <mathjax>$T$</mathjax> with respect to the standard basis</li>
<li><mathjax>$\mathbf C$</mathjax> is the change of basis matrix for <mathjax>$B$</mathjax></li>
</ul>
<h3>正交基</h3>
<h3>Orthonormal bases</h3>
<p>If <mathjax>$B$</mathjax> is an orthonormal set, it is linearly independent, and thus it could be a basis. If <mathjax>$B$</mathjax> <em>is</em> a basis, then it is an <strong>orthonormal basis</strong>.</p>
<h4>正交基下的坐标</h4>
<h4>Coordinates with respect to orthonormal bases</h4>
<p>Orthonormal bases make good coordinate systems - it is much easier to find <mathjax>$[\vec{x}]_B$</mathjax> if <mathjax>$B$</mathjax> is an orthonormal basis. It is just:</p>
<p><mathjax>$$ [\vec{x}]_B = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_k \end{bmatrix} =
\begin{bmatrix} \vec{v_1} \cdot \vec{x} \\ \vec{v_2} \cdot \vec{x} \\ \vdots \\ \vec{v_k}
\cdot \vec{x} \end{bmatrix} $$</mathjax></p>
<p>Note that the standard basis for <mathjax>$\mathbb R^n$</mathjax> is an orthonormal basis.</p>
<h2>变换</h2>
<h2>Transformations</h2>
<p>A <strong>transformation</strong> is just a function which operates on vectors, which, instead of using <mathjax>$f$</mathjax>, is usually denoted <mathjax>$T$</mathjax>.</p>
<h3>线性变换</h3>
<h3>Linear transformations</h3>
<p>A <strong>linear transformation</strong> is a transformation:</p>
<p><mathjax>$$ T: \mathbb R^n \to \mathbb R^m $$</mathjax></p>
<p>where we can take two vectors <mathjax>$\vec{a}, \vec{b} \in \mathbb R^n$</mathjax> and the following conditions are satisfied:</p>
<p><mathjax>$$
\begin{aligned}
T(\vec{a} + \vec{b}) &amp;= T(\vec{a}) + T(\vec{b}) \\
T(c\vec{a}) &amp;= cT(\vec{a})
\end{aligned}
$$</mathjax></p>
<p>Put another way, a linear transformation is a transformation in which lines are preserved (they don't become curves) and the origin remains at the origin. This can be thought of as transforming space such that grid lines remain parallel and evenly-spaced.</p>
<p>A linear transformation of a space can be described in terms of transformations of the space's basis vectors, e.g. <mathjax>$\hat i, \hat j$</mathjax>. For example, if the basis vectors <mathjax>$\hat i, \hat j$</mathjax> end up at <mathjax>$[a,c], [b,d]$</mathjax> respectively, an arbitrary vector <mathjax>$[x,y]$</mathjax> would be transformed to:</p>
<p><mathjax>$$
x \begin{bmatrix}a \\ c \end{bmatrix} +
y \begin{bmatrix}b \\ d \end{bmatrix} =
\begin{bmatrix}
ax + by \\ cx + dy
\end{bmatrix}
$$</mathjax></p>
<p>which is equivalent to:</p>
<p><mathjax>$$
\begin{bmatrix}
a &amp; b \\
c &amp; d
\end{bmatrix}
\begin{bmatrix}
x \\ y
\end{bmatrix}
$$</mathjax></p>
<p>In this way, we can think of matrices as representing a transformation of space and the transformation itself as a product with that matrix.</p>
<p>Extending this further, you can think of matrix multiplication as a composition of transformations - each matrix represents one transformation; the resulting matrix product is a composition of those transformations.</p>
<p>The matrix does not have to be square; i.e. it does not have to share dimensionality (in terms of the matrix's rows) with the space it's being applied to. The resulting transformation will have different dimensions.</p>
<p>For example, a 3x2 matrix will transform a 2D space to a 3D space.</p>
<h4>示例</h4>
<h4>Linear transformation examples</h4>
<p>These examples are all in <mathjax>$\mathbb R^2$</mathjax> since it's easier to visualize. But you can scale them up to any <mathjax>$\mathbb R^n$</mathjax>.</p>
<h5>镜面</h5>
<h5>Reflection</h5>
<figure><img alt="An example of reflection." src="../assets/reflection.svg" /><figcaption>An example of reflection.</figcaption>
</figure>
<p>To get from the triangle on the left and reflect it over the <mathjax>$y$</mathjax>-axis to get the triangle on the right, all you're doing is changing the sign of all the <mathjax>$x$</mathjax> values.</p>
<p>So a transformation would look like:</p>
<p><mathjax>$$ T(\begin{bmatrix} x \\ y \end{bmatrix}) = \begin{bmatrix} -x \\ y \end{bmatrix} $$</mathjax></p>
<h5>缩放</h5>
<h5>Scaling</h5>
<p>Say you want to double the size of the triangle instead of flipping it. You'd just scale up all of its values:</p>
<p><mathjax>$$ T(\begin{bmatrix} x \\ y \end{bmatrix}) = \begin{bmatrix} 2x \\ 2y \end{bmatrix} $$</mathjax></p>
<h4>线性变换组合</h4>
<h4>Compositions of linear transformation</h4>
<p>The composition of linear transformations <mathjax>$S(\vec{x}) = A\vec{x}$</mathjax> and <mathjax>$T(\vec{x}) = B\vec{x}$</mathjax> is denoted:</p>
<p><mathjax>$$ T \circ S(\vec{x}) = T(S(\vec{x})) $$</mathjax></p>
<p>This is read: "the composition of <mathjax>$T$</mathjax> with <mathjax>$S$</mathjax>".</p>
<p>If <mathjax>$T: Y \to Z$</mathjax> and <mathjax>$S: X \to Y$</mathjax>, then <mathjax>$T \circ S: X \to Z$</mathjax>.</p>
<p>A composition of linear transformations is also a linear transformation (proof omitted here). Because of this, this composition can also be expressed:</p>
<p><mathjax>$$ T \circ S(\vec{X}) = C\vec{x} $$</mathjax></p>
<p>Where <mathjax>$C = BA$</mathjax> (proof omitted), so:</p>
<p><mathjax>$$ T \circ S(\vec{X}) = BA\vec{x} $$</mathjax></p>
<h3>核</h3>
<h3>Kernels</h3>
<p>The <strong>kernel</strong> of T, denoted <mathjax>$\Ker(T)$</mathjax>, is all of the vectors in the domain such that the transformation of those vectors is equal to the zero vector:</p>
<p><mathjax>$$ \Ker(T) = \{ \vec{x} \in \mathbb R^n \, | \, T(\vec{x}) = \{ \vec{0} \} \} $$</mathjax></p>
<p>You may notice that, because <mathjax>$T(\vec{x}) = \mathbf A \vec{x}$</mathjax>,</p>
<p><mathjax>$$ \Ker(T) = N(\mathbf A) $$</mathjax></p>
<p>That is, the kernel of the transformation is the same as the nullspace of the transformation matrix.</p>
<h2>象</h2>
<h2>Images</h2>
<h3>定义域子集的象</h3>
<h3>Image of a subset of a domain</h3>
<p>When you pass a set of vectors (i.e. a subset of a domain <mathjax>$\mathbb R^n$</mathjax>) through a transformation, the result is called the <strong>image</strong> of the set under the transformation. E.g. <mathjax>$T(S)$</mathjax> is the image of <mathjax>$S$</mathjax> under <mathjax>$T$</mathjax>.</p>
<p>For example, say we have some vectors which define the triangle on the left. When a transformation is applied to that set, the image is the result on the right.</p>
<figure><img alt="Example: Image of a triangle." src="../assets/image.svg" /><figcaption>Example: Image of a triangle.</figcaption>
</figure>
<p>Another example: if we have a transformation <mathjax>$T: X \to Y$</mathjax> and <mathjax>$A$</mathjax> which is a subset of <mathjax>$T$</mathjax>, then <mathjax>$T(A)$</mathjax> is the image of <mathjax>$A$</mathjax> under <mathjax>$T$</mathjax>, which is equivalent to the set of transformations for each vector in <mathjax>$A$</mathjax> :</p>
<p><mathjax>$$ T(A) = \{ T(\vec{x}) \in Y \, | \, \vec{x} \in A \} $$</mathjax></p>
<figure><img alt="$T: X \to Y$, where $A \subseteq X$." src="../assets/transformation.svg" /><figcaption><mathjax>$T: X \to Y$</mathjax>, where <mathjax>$A \subseteq X$</mathjax>.</figcaption>
</figure>
<p>Images describe surjective functions, that is, a surjective function <mathjax>$f : X \to Y$</mathjax> can also be written:</p>
<p><mathjax>$$ \Img(f) = Y $$</mathjax></p>
<p>since the image of the transformation encompasses the entire codomain <mathjax>$Y$</mathjax>.</p>
<h3>子空间的象</h3>
<h3>Image of a subspace</h3>
<p>The image of a subspace under a transformation is also a subspace. That is, if <mathjax>$V$</mathjax> is a subspace, <mathjax>$T(V)$</mathjax> is also a subspace.</p>
<h3>变换的象</h3>
<h3>Image of a transformation</h3>
<p>If, instead of a subset or subspace, you take the transformation of an entire space, i.e. <mathjax>$T(\mathbb R^n)$</mathjax>, the terminology is different: that is called the <em>image of <mathjax>$T$</mathjax></em>, notated <mathjax>$\Img(T)$</mathjax>.</p>
<p>Because we know matrix-vector products are linear transformations:</p>
<p><mathjax>$$ T(\vec{x}) = \mathbf A \vec{x} $$</mathjax></p>
<p>The image of a linear transformation matrix <mathjax>$\mathbf A$</mathjax> is equivalent to its column space, that is:</p>
<p><mathjax>$$ \Img(T) = C(\mathbf A) $$</mathjax></p>
<h3>集合的原象</h3>
<h3>Preimage of a set</h3>
<p>The <strong>preimage</strong> is the inverse image. For instance, consider a transformation mapping from the domain <mathjax>$X$</mathjax> to the codomain <mathjax>$Y$</mathjax> :</p>
<p><mathjax>$$ T: X \to Y $$</mathjax></p>
<p>And say you have a set <mathjax>$S$</mathjax> which is a subset of <mathjax>$Y$</mathjax>. You want to find the set of values in <mathjax>$X$</mathjax> which map to <mathjax>$S$</mathjax>, that is, the subset of <mathjax>$X$</mathjax> for which <mathjax>$S$</mathjax> is the image.</p>
<p>For a set <mathjax>$S$</mathjax>, this is notated:</p>
<p><mathjax>$$ T^{-1}(S) $$</mathjax></p>
<p>Note that not every point in <mathjax>$S$</mathjax> needs to map back to <mathjax>$X$</mathjax>. That is, <mathjax>$S$</mathjax> may contain some points for which there are no corresponding points in <mathjax>$X$</mathjax>. Because of this, the image of the preimage of <mathjax>$S$</mathjax> is not necessarily equivalent to <mathjax>$S$</mathjax>, but we can be sure that it is at least a subset:</p>
<p><mathjax>$$ T(T^{-1}(S)) \subseteq S $$</mathjax></p>
<h2>投影</h2>
<h2>Projections</h2>
<p>A <strong>projection</strong> can kind of be thought of as a "shadow" of a vector:</p>
<figure><img alt="Here, we have the projection of $\vec{x}$ - the red vector - onto the green line $L$. The projection is the dark red vector. This example is in $\mathbb R^2$ but this works in any $\mathbb R^n$." src="../assets/projection.svg" /><figcaption>Here, we have the projection of <mathjax>$\vec{x}$</mathjax> - the red vector - onto the green line <mathjax>$L$</mathjax>. The projection is the dark red vector. This example is in <mathjax>$\mathbb R^2$</mathjax> but this works in any <mathjax>$\mathbb R^n$</mathjax>.</figcaption>
</figure>
<p>Alternatively, it can be thought of answering "how far does one vector go in the direction of another vector?". In the accompanying figure, the projection of <mathjax>$b$</mathjax> onto <mathjax>$a$</mathjax> tells us how far <mathjax>$b$</mathjax> goes in the direction of <mathjax>$a$</mathjax>.</p>
<p>The projection of <mathjax>$\vec{x}$</mathjax> onto line <mathjax>$L$</mathjax> is notated:</p>
<p><mathjax>$$ \Proj_L(\vec{x}) $$</mathjax></p>
<p>More formally, a projection of a vector <mathjax>$\vec{x}$</mathjax> onto a line <mathjax>$L$</mathjax> is some vector in <mathjax>$L$</mathjax> where <mathjax>$\vec{x} - \Proj_L(\vec{x})$</mathjax> is orthogonal to <mathjax>$L$</mathjax>.</p>
<p>A line can be expressed as the set of all scalar multiples of a vector, i.e:</p>
<p><mathjax>$$ L = \{ c\vec{v} \, | \, c \in \mathbb R \} $$</mathjax></p>
<p>So we know that "some vector in <mathjax>$L$</mathjax>" can be represented as <mathjax>$c\vec{v}$</mathjax> :</p>
<p><mathjax>$$ \Proj_L(\vec{x}) = c\vec{v} $$</mathjax></p>
<p>By our definition of a projection, we also know that <mathjax>$\vec{x} - \Proj_L(\vec{x})$</mathjax> is orthogonal to <mathjax>$L$</mathjax>, which can now be rewritten as:</p>
<p><mathjax>$$ (\vec{x} - c\vec{v}) \cdot \vec{v} = \vec{0} $$</mathjax></p>
<p>(This is the definition of orthogonal vectors.)</p>
<p>Written in terms of <mathjax>$c$</mathjax>, this simplifies down to:</p>
<p><mathjax>$$ c = \frac{\vec{x} \cdot \vec{v}}{\vec{v} \cdot \vec{v}} $$</mathjax></p>
<p>So then we can rewrite:</p>
<p><mathjax>$$ \Proj_L(\vec{x}) = \frac{\vec{x} \cdot \vec{v}}{\vec{v} \cdot \vec{v}}\vec{v} $$</mathjax></p>
<p>or, better:</p>
<p><mathjax>$$ \Proj_L(\vec{x}) = \frac{\vec{x} \cdot \vec{v}}{||\vec{v}||^2}\vec{v} $$</mathjax></p>
<p>And you can pick whatever vector for <mathjax>$\vec{v}$</mathjax> so long as it is part of line <mathjax>$L$</mathjax>.</p>
<p>However, if <mathjax>$\vec{v}$</mathjax> is a unit vector, then the projection is simplified even further:</p>
<p><mathjax>$$ \Proj_L(\vec{x}) = (\vec{x} \cdot \hat{u}) \hat{u} $$</mathjax></p>
<p>Projections are linear transformations (they satisfy the requirements, proof omitted), so you can represent them as matrix-vector products:</p>
<p><mathjax>$$ \Proj_L(\vec{x}) = \mathbf A \vec{x} $$</mathjax></p>
<p>where the transformation matrix <mathjax>$\mathbf A$</mathjax> is:</p>
<p><mathjax>$$ \mathbf A = \begin{bmatrix} u_1^2 &amp; u_2u_1 \\ u_1u_2 &amp; u_2^2 \end{bmatrix} $$</mathjax></p>
<p>where <mathjax>$u_i$</mathjax> are components of the unit vector.</p>
<p>Also note that the length of a projection (i.e. the scalar component of the projection) is given by the dot product of the two vectors. For example, in the accompanying figure, the length of <mathjax>$\Proj_{\vec a}(\vec b)$</mathjax> is <mathjax>$a \cdot b$</mathjax>.</p>
<h3>投影到子空间</h3>
<h3>Projections onto subspaces</h3>
<p>Given that <mathjax>$V$</mathjax> is a subspace of <mathjax>$\mathbb R^n$</mathjax>, we know that <mathjax>$V^{\perp}$</mathjax> is also a subspace of <mathjax>$\mathbb R^n$</mathjax>, and we have a vector <mathjax>$\vec{x}$</mathjax> such that <mathjax>$\vec{x} \in \mathbb R^n$</mathjax>, we know that <mathjax>$\vec{x} = \vec{v} + \vec{w}$</mathjax> where <mathjax>$\vec{v} \in V$</mathjax> and <mathjax>$\vec{w} \in V^{\perp}$</mathjax>, then:</p>
<p><mathjax>$$
\begin{aligned}
\Proj_V\vec{x} &amp;= \vec{v} \\
\Proj_{V^{\perp}}\vec{x} &amp;= \vec{w}
\end{aligned}
$$</mathjax></p>
<p>Reminder: a projection onto a subspace is the same as a projection onto a line (a line is a subspace):</p>
<p><mathjax>$$ \Proj_V\vec{x} = \frac{\vec{x} \cdot \vec{v}}{\vec{v}\cdot\vec{v}}\vec{v} $$</mathjax></p>
<p>where:</p>
<p><mathjax>$$
\begin{aligned}
V &amp;= \Span(\vec{v}) \\
V &amp;= \{ c\vec{v} \, | \, c \in \mathbb R \}
\end{aligned}
$$</mathjax></p>
<p>So <mathjax>$\Proj_V \vec{x}$</mathjax> is the unique vector <mathjax>$\vec{v} \in V$</mathjax> such that <mathjax>$\vec{x} = \vec{v} + \vec{w}$</mathjax> where <mathjax>$\vec{w}$</mathjax> is a unique member of <mathjax>$V^{\perp}$</mathjax>.</p>
<h4>作为线性变换的投影</h4>
<h4>Projection onto a subspace as a linear transform</h4>
<p><mathjax>$$ \Proj_V(\vec{x}) = \mathbf A (\mathbf A^T \mathbf A)^{-1}\mathbf A^T \vec{x} $$</mathjax></p>
<p>where <mathjax>$V$</mathjax> is a subspace of <mathjax>$\mathbb R^n$</mathjax>.</p>
<p>Note that <mathjax>$\mathbf A (\mathbf A^T \mathbf A)^{-1}\mathbf A^T$</mathjax> is just some matrix, which we can call <mathjax>$\mathbf B$</mathjax>, so this is in the form of a linear transform, <mathjax>$\mathbf B \vec{x}$</mathjax>.</p>
<p>Also <mathjax>$\vec{v} = \Proj_V\vec{x}$</mathjax>, so:</p>
<p><mathjax>$$ \vec{x} = \Proj_V\vec{x} + \vec{w} $$</mathjax></p>
<p>where <mathjax>$\vec{w}$</mathjax> is a unique member of <mathjax>$V^{\perp}$</mathjax>.</p>
<h4>使用正交基投影到子空间</h4>
<h4>Projections onto subspaces with orthonormal bases</h4>
<p>Given that <mathjax>$V$</mathjax> is a subspace of <mathjax>$\mathbb R^n$</mathjax> and <mathjax>$B = \{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_k} \}$</mathjax>  is an orthonormal basis for <mathjax>$V$</mathjax>. We have a vector <mathjax>$\vec{x} \in \mathbb R^n$</mathjax>, so <mathjax>$\vec{x} = \vec{v} + \vec{w}$</mathjax> where <mathjax>$\vec{v} \in V$</mathjax> and <mathjax>$\vec{w} \in V^{\perp}$</mathjax>.</p>
<p>We know (see previously) that by definition:</p>
<p><mathjax>$$ \Proj_V(\vec{x}) = \mathbf A (\mathbf A^T \mathbf A)^{-1}\mathbf A^T \vec{x} $$</mathjax></p>
<p>which is quite complicated. It is much simpler for orthonormal bases:</p>
<p><mathjax>$$ \Proj_V(\vec{x}) = \mathbf A \mathbf A^T \vec{x} $$</mathjax></p>
<h2>识别变换的属性</h2>
<h2>Identifying transformation properties</h2>
<h3>是否满射</h3>
<h3>Determining if a transformation is surjective</h3>
<p>A transformation <mathjax>$T(\vec{x}) = \mathbf A \vec{x}$</mathjax> is surjective ("onto") if the column space of <mathjax>$\mathbf A$</mathjax> equals the codomain:</p>
<p><mathjax>$$ \Span(a_1, a_2, \dots, a_n) = C(\mathbf A) = \mathbb R^m $$</mathjax></p>
<p>which can also be stated as:</p>
<p><mathjax>$$ \Rank(\mathbf A) = m $$</mathjax></p>
<h3>是否单射</h3>
<h3>Determining if a transformation is injective</h3>
<p>A transformation <mathjax>$T(\vec{x}) = \mathbf A \vec{x}$</mathjax> is injective ("one-to-one") if the the nullspace of <mathjax>$\mathbf A$</mathjax> contains only the zero vector:</p>
<p><mathjax>$$ N(\mathbf A) = \{ \vec{0} \} $$</mathjax></p>
<p>which is true if the set of <mathjax>$\mathbf A$</mathjax>'s column vectors is linearly independent.</p>
<p>This can also be stated as:</p>
<p><mathjax>$$ \Rank(\mathbf A) = n $$</mathjax></p>
<h3>是否可逆</h3>
<h3>Determining if a transformation is invertible</h3>
<p>A transformation is invertible if it is both injective and surjective.</p>
<p>For a transformation to be surjective:</p>
<p><mathjax>$$ \Rank(\mathbf A) = m $$</mathjax></p>
<p>And for a transformation to be surjective:</p>
<p><mathjax>$$ \Rank(\mathbf A) = n $$</mathjax></p>
<p>Therefore for a transformation to be invertible:</p>
<p><mathjax>$$ \Rank(\mathbf A) = m = n $$</mathjax></p>
<p>So the transformation matrix <mathjax>$\mathbf A$</mathjax> must be a square matrix.</p>
<h3>线性变换的逆变换</h3>
<h3>Inverse transformations of linear transformations</h3>
<p>Inverse transformations are linear transformations if the original transformation is both linear and invertible. That is, if <mathjax>$T$</mathjax> is invertible and linear, <mathjax>$T^{-1}$</mathjax> is linear:</p>
<p><mathjax>$$
\begin{aligned}
T^{-1}(\vec{x}) &amp;= \mathbf A^{-1} \vec{x} \\
(T^{-1} \circ T)(\vec{x}) &amp;= \mathbf A^{-1} \mathbf A \vec{x} = I_n \vec{x} = \mathbf A \mathbf A^{-1} \vec{x} = (T \circ T^{-1})(\vec{x})
\end{aligned}
$$</mathjax></p>
<h2>特征值和特征向量</h2>
<h2>Eigenvalues and Eigenvectors</h2>
<p>Say we have a linear transformation <mathjax>$T: \mathbb R^n \to \mathbb R^n$</mathjax> :</p>
<p><mathjax>$$ T(\vec{v}) = \mathbf A \vec{v} = \lambda\vec{v} $$</mathjax></p>
<p>That is, <mathjax>$\vec{v}$</mathjax> is scaled by a transformation matrix <mathjax>$\lambda$</mathjax>.</p>
<p>We say that:</p>
<ul>
<li><mathjax>$\vec{v}$</mathjax> is the <strong>eigenvector</strong> for <mathjax>$T$</mathjax></li>
<li><mathjax>$\lambda$</mathjax> is the <strong>eigenvalue</strong> associated with that eigenvector</li>
</ul>
<p><strong>Eigenvectors</strong> are vectors for which matrix multiplication is equivalent to only a scalar multiplication, nothing more. <mathjax>$\lambda$</mathjax>, the <strong>eigenvalue</strong>, is the scalar that the transformation matrix <mathjax>$\mathbf A$</mathjax> is equivalent to.</p>
<p>Another way to put this: given a square matrix <mathjax>$A \in \mathbb R^{n \times n}$</mathjax>, we say <mathjax>$\lambda \in \mathbb C$</mathjax> is an <strong>eigenvalue</strong> of <mathjax>$A$</mathjax> and <mathjax>$x \in \mathbb C^n$</mathjax> is the corresponding <strong>eigenvector</strong> if:</p>
<p><mathjax>$$
Ax = \lambda x, x \neq 0
$$</mathjax></p>
<p>Note that <mathjax>$\mathbb C$</mathjax> refers to the set of complex numbers.</p>
<p>So this means that multiplying <mathjax>$A$</mathjax> by <mathjax>$x$</mathjax> just results in a new vector which points in the same direction has <mathjax>$x$</mathjax> but scaled by a factor <mathjax>$\lambda$</mathjax>.</p>
<p>For any eigenvector <mathjax>$x \in \mathbb C^n$</mathjax> and a scalar <mathjax>$t \in \mathbb C$</mathjax>, <mathjax>$A(cx) = cAx = c \lambda x = \lambda(cx)$</mathjax>, that is, <mathjax>$cx$</mathjax> is also an eigenvector - but when talking about "the" eigenvector associated with <mathjax>$\lambda$</mathjax>, it is assumed that the eigenvector is normalized to length 1 (though you still have the ambiguity that both <mathjax>$x$</mathjax> and <mathjax>$-x$</mathjax> are eigenvectors in this sense).</p>
<p>Eigenvalues and eigenvectors come up when maximizing some function of a matrix.</p>
<p>So what are our eigenvectors? What <mathjax>$\vec{v}$</mathjax> satisfies:</p>
<p><mathjax>$$ \mathbf A \vec{v} = \lambda\vec{v}, \vec{v} \neq 0$$</mathjax></p>
<p>We can do:</p>
<p><mathjax>$$
\begin{aligned}
\mathbf A \vec{v} &amp;= \lambda \vec{v} \\
\vec{0} &amp;= \lambda \vec{v} - \mathbf A \vec{v}
\end{aligned}
$$</mathjax></p>
<p>We know that <mathjax>$\vec{v} = \mathbf I_n \vec{v}$</mathjax>, so we can do:</p>
<p><mathjax>$$
\vec{0} = \lambda \mathbf I_n \vec{v} - \mathbf A \vec{v} = (\lambda \mathbf I_n - \mathbf A)\vec{v}
$$</mathjax></p>
<p>The first term, <mathjax>$\lambda \mathbf I_n - \mathbf A$</mathjax>, is just some matrix which we can call <mathjax>$\mathbf B$</mathjax>, so we have:</p>
<p><mathjax>$$ \vec{0} = \mathbf B \vec{v} $$</mathjax></p>
<p>which, by our definition of nullspace, indicates that <mathjax>$\vec{v}$</mathjax> is in the nullspace of <mathjax>$\mathbf B$</mathjax>. That is:</p>
<p><mathjax>$$ \vec{v} \in N(\lambda \mathbf I_n - \mathbf A) $$</mathjax></p>
<h3>特征值和特征向量的属性</h3>
<h3>Properties of eigenvalues and eigenvectors</h3>
<ul>
<li>The trace of <mathjax>$A$</mathjax> is equal to the sum of its eigenvalues: <mathjax>$\Tr(A) = \sum^n_{i=1} \lambda_i$</mathjax>.</li>
<li>The determinant of <mathjax>$A$</mathjax> is equal to the product of its eigenvalues: <mathjax>$|A| = \prod^n_{i=1} \lambda_i$</mathjax>.</li>
<li>The rank of <mathjax>$A$</mathjax> is equal to the number of non-zero eigenvalues of <mathjax>$A$</mathjax>.</li>
<li>If <mathjax>$A$</mathjax> is non-singular, then <mathjax>$\frac{1}{\lambda_i}$</mathjax> is an eigenvalue of <mathjax>$A^{-1}$</mathjax> with associated eigenvector <mathjax>$x_i$</mathjax>, i.e. <mathjax>$A^{-1}x_i = (\frac{1}{\lambda_i})x_i$</mathjax>.</li>
<li>The eigenvalues of a diagonal matrix <mathjax>$D = \Diag(d_1, \dots, d_n)$</mathjax> are just the diagonal entries <mathjax>$d_1, \dots, d_n$</mathjax>.</li>
</ul>
<h3>对角化的矩阵</h3>
<h3>Diagonalizable matrices</h3>
<p>All eigenvector equations can be written simultaneously as:</p>
<p><mathjax>$$
AX = X\Lambda
$$</mathjax></p>
<p>where the columns of <mathjax>$X \in \mathbb R^{n \times n}$</mathjax> are the eigenvectors of <mathjax>$A$</mathjax> and <mathjax>$\Lambda$</mathjax> is a diagonal matrix whose entries are the eigenvalues of <mathjax>$A$</mathjax>, i.e. <mathjax>$\Lambda = \Diag(\lambda_1, \dots, \lambda_n)$</mathjax>.</p>
<p>If the eigenvectors of <mathjax>$A$</mathjax> are linearly independent, then the matrix <mathjax>$X$</mathjax> will be invertible, so that <mathjax>$A = X \Lambda X^{-1}$</mathjax>. A matrix that can be written in this form is called <strong>diagonalizable</strong>.</p>
<h3>对称矩阵的特征值和特征向量</h3>
<h3>Eigenvalues &amp; eigenvectors of symmetric matrices</h3>
<p>For a symmetric matrix <mathjax>$A \in \mathbb S^n$</mathjax>, the eigenvalues of <mathjax>$A$</mathjax> are all real and the eigenvectors of <mathjax>$A$</mathjax> are all orthonormal.</p>
<p>If for all of <mathjax>$A$</mathjax>'s eigenvalues <mathjax>$\lambda_i$</mathjax>...</p>
<ul>
<li><mathjax>$\lambda_i &gt; 0$</mathjax>, then <mathjax>$A$</mathjax> is positive definite.</li>
<li><mathjax>$\lambda_i \geq 0$</mathjax>, then <mathjax>$A$</mathjax> is positive semidefinite.</li>
<li><mathjax>$\lambda_i &lt; 0$</mathjax>, then <mathjax>$A$</mathjax> is negative definite.</li>
<li><mathjax>$\lambda_i \leq 0$</mathjax>, then <mathjax>$A$</mathjax> is negative semidefinite.</li>
<li>have both positive and negative values, then <mathjax>$A$</mathjax> is indefinite.</li>
</ul>
<h4>示例</h4>
<h4>Example</h4>
<p>Say we have a linear transformation <mathjax>$T(\vec{x}) = \mathbf A \vec{x}$</mathjax>. Here are some example values of <mathjax>$\vec{x}$</mathjax> being input and the output vectors they yield (it's not important here what <mathjax>$\mathbf A$</mathjax> actually looks like, its just to help distinguish what is and isn't an eigenvector.)</p>
<ul>
<li><mathjax>$\mathbf A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$</mathjax><ul>
<li><mathjax>$\vec{x}$</mathjax> is not an eigenvector, it was not merely scaled by <mathjax>$\mathbf A$</mathjax>.</li>
</ul>
</li>
<li><mathjax>$\mathbf A \begin{bmatrix} 4 \\ 7 \end{bmatrix} = \begin{bmatrix} 8 \\ 14 \end{bmatrix}$</mathjax><ul>
<li><mathjax>$\vec{x}$</mathjax> is an eigenvector, it was only scaled by <mathjax>$\mathbf A$</mathjax>. This is a<br />
simple example where the vector was scaled up by 2, so the eigenvalue here is 2.</li>
</ul>
</li>
</ul>
<h3>特征空间</h3>
<h3>Eigenspace</h3>
<p>The eigenvectors that correspond to an eigenvalue <mathjax>$\lambda$</mathjax> form the <strong>eigenspace</strong> for that <mathjax>$\lambda$</mathjax>, notated <mathjax>$E_{\lambda}$</mathjax> :</p>
<p><mathjax>$$ E_{\lambda} = N(\lambda \mathbf I_n - \mathbf A) $$</mathjax></p>
<h3>特征基</h3>
<h3>Eigenbasis</h3>
<p>Say we have an <mathjax>$n \times n$</mathjax> matrix <mathjax>$\mathbf A$</mathjax>. An <strong>eigenbasis</strong> is a basis for <mathjax>$\mathbb R^n$</mathjax> consisting entirely of eigenvectors for <mathjax>$\mathbf A$</mathjax>.</p>
<h2>张量</h2>
<h2>Tensors</h2>
<p><strong>Tensors</strong> are generalizations of scalars, vectors, and matrices. A tensor is distinguished by its <strong>rank</strong>, which is the number of indices it has. A scalar is a <mathjax>$0^{th}$</mathjax>-rank tensor (it has no indices), a vector is a <mathjax>$1^{th}$</mathjax>-rank tensor, i.e. its components are accessed by one index, e.g. <mathjax>$x_i$</mathjax>, and a matrix is a <mathjax>$2^{th}$</mathjax>-rank tensor, i.e. its components are accessed by two indices, e.g. <mathjax>$X_{i,j}$</mathjax>, and so on.</p>
<p>Just as we have scalar and vector fields, we also have <strong>tensor fields</strong>.</p>
<h2>References</h2>
<ul>
<li><a href="https://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf">Math for Machine Learning</a>. Hal Daumé III. August 28, 2009.</li>
<li><a href="http://mathworld.wolfram.com/Tensor.html">Tensor</a>. Rowland, Todd and Weisstein, Eric W. Wolfram MathWorld.</li>
<li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a>. 3Blue1Brown.</li>
<li><a href="https://deeplearning4j.org/cn/eigenvector">本征向量、PCA和熵的基础教程</a></li>
</ul>
    
    <script src="http://cdn.bootcss.com/jquery/1.11.1/jquery.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script src="http://ai-code.tech/ai_notes_html/js/custom.js"></script>
    <script>
        $(document).ready(function() {
            $('pre').each(function(i, e) {hljs.highlightBlock(e)});
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$","$"]],
                    displayMath: [['$$','$$']],
                    processEscapes: true
                },
                "HTML-CSS": {
                    linebreaks: { automatic: true }
                }
            });
            MathJax.Hub.Startup.onload();
        });
    </script>


</body>
</html>
