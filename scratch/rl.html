
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <title>rl</title>

    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/color-brewer.min.css">
    <link rel="stylesheet" href="http://ai-code.tech/ai_notes_html/css/custom.css">
</head>

<body>

    <p>notes from David Silver's Reinforcement Learning course <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa">Advanced Topics: RL</a> (<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">see also</a>). 2015 (COMPM050/COMPGI13).</p>
<p>Reinforcement learning characteristics:</p>
<ul>
<li>no supervisor (nothing top-down saying what's right and what's wrong as in supervised learning), only a reward signal</li>
<li>feedback (reward) may be delayed (not instantaneous)<ul>
<li>e.g. giving up short-term gain may be better for long-term gain</li>
</ul>
</li>
<li>time is important (i.e. sequences of actions)</li>
<li>the agent's actions affects the subsequent data it receives</li>
</ul>
<p>A <em>reward</em> <mathjax>$R_t$</mathjax> is a scalar feedback signal that indicates how well the agent is doing at step <mathjax>$t$</mathjax>. The agent's objective is to maximize cumulative reward.</p>
<p>Note that the reward must be scalar though you could take a reward vector over different aspects (e.g. this increases my income, but decreases my happiness) and take its dot product with some weights (e.g. I value my happiness more than my income) to retrieve a scalar value.</p>
<p>The <em>reward hypothesis</em> states that goals can be described by the maximization of expected cumulative reward and is the central assumption of reinforcement learning.</p>
<p>The term "reward" may be a bit confusing, since they may be negative as well, i.e. "punishment".</p>
<p>Rewards can come at each time step (for example, if you want an agent to do something quickly, you can set a reward of -1 for each time step), or at the end of the task, or at the end of each "episode" if the task has no end (but you need to chunk up time in some way), or for certain states, etc.</p>
<p>The general framework is that the agent decides on and executes some action <mathjax>$A_t$</mathjax> which affects the environment, receives observation <mathjax>$O_t$</mathjax> from the environment, and gets a reward <mathjax>$R_t$</mathjax>.</p>
<p>The <em>history</em> is the sequence of observations, actions, and rewards:</p>
<p><mathjax>$$
H_t = A_1, O_1, R_1, \dots, A_t, O_t, R_t
$$</mathjax></p>
<p>We are essentially looking to build a mapping from this history to some action to take next.</p>
<p>Practically, we can't work with the full history, so we summarize it as <em>state</em> <mathjax>$S_t = f(H_t)$</mathjax> (i.e. it is some function of the history) and so the task is to learn a mapping from states to some action to take next.</p>
<p>We can represent a state for the environment as well (distinct from the agent's state <mathjax>$S_t^a$</mathjax>, and typically hidden from the agent) as <mathjax>$S_t^e$</mathjax>. In multi-agent simulations, from the perspective of a single agent, the other agents can be represented as part of the environment.</p>
<p>We can more specifically define an <em>information state</em> (also called a <em>Markov state</em>), which is a state in which:</p>
<p><mathjax>$$
P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \dots, S_t]
$$</mathjax></p>
<p>That is, we make a Markov assumption that the probability of the next state depends only on the current state. This assumption allows us to effectively ignore the history previous to the present state.</p>
<p>How we represent state greatly affects learning. Consider an experiment in which a rat either gets shocked or gets a piece of cheese. The rat observes the following two sequences:</p>
<ol>
<li>light, light, lever, bell -&gt; shock</li>
<li>bell, light, lever, lever -&gt; cheese</li>
</ol>
<p>Consider this sequence:</p>
<ul>
<li>lever, light, lever, bell</li>
</ul>
<p>Will the rat get shocked or get cheese?</p>
<p>You might think "shock" because the same 3-item sequence is at the end (light, lever, bell). However, if you represent the state as numbers of each event, you'd say cheese (1 bell, 1 light, 2 levers). Or you could represent it as the entire 4-item sequence in which case it's not clear what the result will be.</p>
<p>In a <em>fully observable</em> environment, the agent directly observes environment state, i.e. <mathjax>$O_t = S_t^a = S_t^e$</mathjax>. In this case we have a Markov decision process (MDP).</p>
<p>In a <em>partially observable</em> environment, the agent indirectly observes the environment (a lot of information is hidden). In this case the agent and environment states are not congruent. This is a partially observable Markov decision process (POMDP). In this case we must construct the agent's state representation separately - one possibility is as the beliefs of the environment state, or use a recurrent neural network to generate the next state, etc.</p>
<p>The main components that may be included in an RL agent:</p>
<ul>
<li>policy: the agent's behavior function<ul>
<li>a map from state to action</li>
<li>it may be deterministic, i.e. <mathjax>$a = \pi(s)$</mathjax></li>
<li>or it may be stochastic, i.e. <mathjax>$\pi(a|s) = P[A=a|S=s]$</mathjax></li>
</ul>
</li>
<li>value function: how good is each state and/or action<ul>
<li>a prediction of (i.e. expected) future reward</li>
<li>evaluates the goodness or badness of states</li>
<li>used to select between actions</li>
<li>e.g. <mathjax>$v_{\pi}(s) = E_{\pi}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots | S_t = s]$</mathjax></li>
<li>the value function <mathjax>$V$</mathjax> is the state value function, which values states, and <mathjax>$Q$</mathjax> is the action value function, which values actions (given states)</li>
</ul>
</li>
<li>model: agent's representation of the environment<ul>
<li>often has two components</li>
<li>the <em>transitions</em> <mathjax>$P$</mathjax> predicts the next state (the "dynamics"), e.g. <mathjax>$P_{ss'}^a = P[S'=s'|S=s,A=a]$</mathjax></li>
<li>the <em>rewards</em> <mathjax>$R$</mathjax> predicts the next immediate reward, e.g. <mathjax>$R_s^a = E[R|S=s,A=a]$</mathjax></li>
<li>but there are many model-free methods as well</li>
</ul>
</li>
</ul>
<p>RL agents can be categorized according to which of these components they have:</p>
<ul>
<li><em>value based</em> agents<ul>
<li>have a value function</li>
<li>if it has a value function, the policy is implied (choose the highest-valued action)</li>
</ul>
</li>
<li><em>policy based</em> agents<ul>
<li>have an explicit policy</li>
<li>without a value function</li>
</ul>
</li>
<li><em>actor critic</em> agents<ul>
<li>has both a policy and a value function</li>
</ul>
</li>
</ul>
<p>Further distinction is made between model-free agents (the agent doesn't model the environment, just has a policy and/or value function) or model-based agents (has a model and a policy and/or value function).</p>
<p>Prediction vs control:</p>
<ul>
<li>prediction: evaluate the future, given a policy</li>
<li>control: optimize the future, find the best policy</li>
</ul>
<hr />
<h1>Markov Decision Processes</h1>
<p>Markov decision processes formally describe a fully-observable environment for reinforcement learning. Almost all RL problems can be formalized as MDPs, including partially-observable RL problems: partially-observable problems can be converted into MDPs.</p>
<p>We make the Markov assumption (see above) for MDPs. That is, the current state captures all relevant information from the history, so the history can be thrown away.</p>
<p>We have a <em>state transition matrix</em> which encodes the state transition probabilities, i.e. the probability of going to some state <mathjax>$s'$</mathjax> from a state <mathjax>$s$</mathjax>.</p>
<p>A Markov process then is a memoryless (i.e. Markov) random process, i.e. a stochastic sequence of states, i.e. a Markov chain. We define a Markov process as a tuple <mathjax>$(S, P)$</mathjax>, where <mathjax>$S$</mathjax> is a finite set of state and <mathjax>$P$</mathjax> is the state transition probability matrix:</p>
<p><mathjax>$$
P_{ss'} = p[S_{t+1} = s' | S_t = s]
$$</mathjax></p>
<p>A <em>Markov reward process</em> is a Markov chain with values. It is described by a tuple <mathjax>$(S,P,R,\gamma)$</mathjax>, which includes the reward function <mathjax>$R$</mathjax> and the discount factor <mathjax>$\gamma \in [0,1]$</mathjax>:</p>
<p><mathjax>$$
R_s = E[R_{t+1} | S_t = s]
$$</mathjax></p>
<p>The <em>return</em> <mathjax>$G_t$</mathjax> ("G" for "goal") is the total discounted reward from time-step <mathjax>$t$</mathjax>, i.e.:</p>
<p><mathjax>$$
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$</mathjax></p>
<p>The value function <mathjax>$v(s)$</mathjax> gives the long-term value of state <mathjax>$s$</mathjax>. Formally, it is the expected return of a Markov reward process from state <mathjax>$s$</mathjax>, i.e.:</p>
<p><mathjax>$$
v(s) = E[G_t | S_t = s]
$$</mathjax></p>
<p>The Bellman Equation decomposes the value function into two parts:</p>
<ul>
<li>the immediate reward <mathjax>$R_{t+1}$</mathjax></li>
<li>the discounted value of successor state <mathjax>$\gamma v(S_{t+1})$</mathjax></li>
</ul>
<p>So it turns it into a recursive function:</p>
<p><mathjax>$$
v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]
$$</mathjax></p>
<p>i.e.:</p>
<p><mathjax>$$
v(s) = R_s + \gamma \sum_{s' \in S} P_{ss'} v(s')
$$</mathjax></p>
<p>Or written more concisely using matrices:</p>
<p><mathjax>$$
v = R + \gamma P v
$$</mathjax></p>
<p>As an aside, the Bellman equation is a linear equation, so it can be solved directly:</p>
<p><mathjax>$$
\begin{aligned}
v &amp;= R + \gamma Pv \\
(I - \gamma P) v &amp;= R \\
v &amp;= (I - \gamma P)^{-1}R
\end{aligned}
$$</mathjax></p>
<p>Where <mathjax>$I$</mathjax> is the identity matrix.</p>
<p>This however has a computational complexity  of <mathjax>$O(n^3)$</mathjax> for <mathjax>$n$</mathjax> states so generally it is not practical.</p>
<p>There are other iterative methods for solving MRPs which are more important/practical, e.g. dynamic programming, Monte-Carlo evaluation, and Temporal-Difference learning.</p>
<p>A Markov decision process (MDP) is a Markov reward process with decisions. It is described by a tuple <mathjax>$(S,A,P,R,\gamma)$</mathjax> which now includes a finite set of actions <mathjax>$A$</mathjax>, thus now <mathjax>$P$</mathjax> and <mathjax>$R$</mathjax> become:</p>
<p><mathjax>$$
\begin{aligned}
P_{ss'}^a &amp;= p[S_{t+1} = s' | S_t = s, A_t = a] \\
R_s^a &amp;= E[R_{t+1} | S_t = s, A_t=a]
\end{aligned}
$$</mathjax></p>
<p>A policy <mathjax>$\pi$</mathjax> is a distribution over actions given states:</p>
<p><mathjax>$$
\pi(a|s) = p[A_t = a | S_t = s]
$$</mathjax></p>
<p>A policy fully defines the behavior of an agent. Because it depends only on the current state (and not the history), it is said to be <em>stationary</em> (time-independent).</p>
<p>The <em>state-value function</em> <mathjax>$v_{\pi}(s)$</mathjax> of an MDP is the expected return starting from state <mathjax>$s$</mathjax> and then following policy <mathjax>$\pi$</mathjax>:</p>
<p><mathjax>$$
v_{\pi}(s) = E_{\pi}[G_t|S_t = s]
$$</mathjax></p>
<p>This is different than the value function, which does not involve a policy for selecting actions but rather only proceeds randomly.</p>
<p>The <em>action-value function</em> <mathjax>$q_{\pi}(s,a)$</mathjax> is the expected return starting from state <mathjax>$s$</mathjax>, taking action <mathjax>$a$</mathjax>, and then following policy <mathjax>$\pi$</mathjax>:</p>
<p><mathjax>$$
q_{\pi}(s,a) = E_{\pi}[G_t|S_t = s, A_t = a]
$$</mathjax></p>
<p>We can also define Bellman equations for these value functions (these are called Bellman expectation equations):</p>
<p><mathjax>$$
\begin{aligned}
v_{\pi}(s) &amp;= E_{\pi} [R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s] \\
&amp;= \sum_{a \in A} \pi(a|s) q_{\pi}(s,a) \\
q_{\pi}(s,a) &amp;= E_{\pi} [R_{t+1} + \gamma q_{\pi} (S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \\
&amp;= R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{\pi}(s')
\end{aligned}
$$</mathjax></p>
<p>We can combine these:</p>
<p><mathjax>$$
v_{\pi}(s) = \sum_{a \in A} \pi(a|s) (R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{\pi}(s'))
$$</mathjax></p>
<p>There is some optimal state-value function <mathjax>$v_*(s) = \max_{\pi} v_{\pi}(s)$</mathjax>, i.e. the maximum value function over all policies.</p>
<p>Similarly, there is an optimal action-value function <mathjax>$q_*(s,a) = \max_{\pi} q_{\pi}(s,a)$</mathjax>. This gives us the optimal way for the agent to behave (i.e. we can get the optimal policy from this by always choosing the best action from the optimal action-value function), so this is the most important thing to figure out! That is, the MDP is "solved" when we know <mathjax>$q_*(s,a)$</mathjax>.</p>
<p>From this we can get the Bellman optimality equation:</p>
<p><mathjax>$$
\begin{aligned}
v_*(s) &amp;= \max_a q_*(s,a) \\
q_*(s,a) &amp;= R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_*(s')
\end{aligned}
$$</mathjax></p>
<p>Which can then be combined:</p>
<p><mathjax>$$
v_*(s) = \max_a R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_*(s')
$$</mathjax></p>
<p>Which can equivalently be written:</p>
<p><mathjax>$$
q_*(s,a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \max_{a'} q_*(s', a')
$$</mathjax></p>
<p>The Bellman optimality equation is non-linear, unlike the Bellman equation we saw previously. In general there is no closed form solution, but we can use iterative methods, e.g. value iteration, policy iteration, Q-learning, and Sarsa.</p>
<hr />
<h2>Dynamic programming</h2>
<p>"Dynamic" refers to some sequential or temporal aspect to the problem and we want to optimize some program, i.e. a policy.</p>
<p>Dynamic programming is a method for solving complex problems by breaking them into subproblems, then solving the subproblems (divide-and-conquer).</p>
<p>For dynamic programming to work, the problem must have two properties:</p>
<ul>
<li>optimal substructure; i.e. the optimal solutions of the subproblems tell us about the optimal solution for the overall problem</li>
<li>the subproblems should "overlap"; i.e. they should recur throughout the overall problem. That way, by solving a subproblem, we are simultaneously solving many parts of the overall problem (the solutions can be cached and reused).</li>
</ul>
<p>MDPs satisfy both of these properties with the Bellman equation (it is recursive decomposition) and the value function stores and reuses solutions.</p>
<p>Dynamic programming assumes full knowledge of the MDP, so it is used for <em>planning</em> in an MDP (i.e. prediction and control).</p>
<p>For <em>prediction</em> it takes a fully-specified MDP <mathjax>$(S,A,P,R,\gamma)$</mathjax> (or MRP <mathjax>$(S,P^{\pi}, R^{\pi}, \gamma)$</mathjax>) and a policy <mathjax>$\pi$</mathjax> and gives us a value function <mathjax>$v_{\pi}$</mathjax>. So this is not reinforcement learning because the MDP is fully-specified (nothing needs to be learned!).</p>
<p>For <em>control</em> it takes a fully-specified MDP and gives us the optimal value function <mathjax>$v_*$</mathjax> which gives us an optimal policy <mathjax>$\pi_*$</mathjax> as well.</p>
<h3>Policy evaluation</h3>
<p>Policy evaluation involves taking a policy and an MDP and computing the expected reward for following that policy.</p>
<p>To solve this, we apply the Bellman expectation equation as an iterative update.</p>
<p>We start off with some arbitrary value function <mathjax>$v_1$</mathjax> (e.g. value of every state is 0), then, use <em>synchronous</em> backups (i.e. consider each state in turn):</p>
<ul>
<li>at each iteration <mathjax>$k+1$</mathjax></li>
<li>for all states <mathjax>$s \in S$</mathjax></li>
<li>update <mathjax>$v_{k+1}(s)$</mathjax> from <mathjax>$v_k(s')$</mathjax>, where <mathjax>$s'$</mathjax> is a successor state of <mathjax>$s$</mathjax></li>
</ul>
<p><mathjax>$$
v_{k+1}(s) = \sum_{a \in A} \pi(a|s) (R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_k(s'))
$$</mathjax></p>
<p>This eventually converges to <mathjax>$v_{\pi}$</mathjax>.</p>
<h3>Policy iteration</h3>
<p>Now that we can evaluate a policy, we can figure out how to improve it.</p>
<p>Given a policy <mathjax>$\pi$</mathjax>:</p>
<ul>
<li>evaluate the policy <mathjax>$\pi$</mathjax>: <mathjax>$v_{\pi}(s) = E[R_{t+1} + \gamma R_{t+2}  + \dots | S_t = s]$</mathjax></li>
<li>improve the policy by acting greedily wrt to <mathjax>$v_{\pi}$</mathjax>: <mathjax>$\pi' = \text{greedy}(v_{\pi})$</mathjax> (greedy just means we move to the state with the highest value)</li>
</ul>
<p>And we can iteratively apply this approach (called <em>greedy policy improvement</em>), which will converge to the optimal policy <mathjax>$\pi^*$</mathjax> (no matter how you start).</p>
<figure><img alt="Policy iteration intuition from &lt;https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node46.html&gt;" src="../assets/policy_iteration.png" /><figcaption>Policy iteration intuition from <a href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node46.html">https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node46.html</a></figcaption>
</figure>
<p>The policy and the value function influence each other, since the policy dictates which states are explored and the value function influences how the policy chooses states, so they push off each other to convergence.</p>
<p>With the greedy policy, we always choose the best state (remember that the value of a state takes into account future reward from that state as well!) so we update the value function for that state so that it is equal to or greater than it was before. This is how the greedy policy improves the value function.</p>
<p>Because the value function drives the greed policy, that in turn improves the policy.</p>
<p>Eventually the value function will only be equal to (rather than greater than or equal to) what it was before; at this point convergence is achieved.</p>
<h3>Value iteration</h3>
<p>The solution <mathjax>$v_*(s)$</mathjax> can be found with one-step lookahead:</p>
<p><mathjax>$$
v_*(s) \leftarrow \max_{a \in A} (R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_*(s'))
$$</mathjax></p>
<p>Then apply these updates iteratively. This is <em>value iteration</em>.</p>
<p>Basically we iterate over states and apply this update until convergence, giving us the optimal value function.</p>
<p>The pseudocode is:</p>
<ul>
<li>using synchronous backups<ul>
<li>at each iteration <mathjax>$k + 1$</mathjax></li>
<li>for all states <mathjax>$s \ in S$</mathjax></li>
<li>update <mathjax>$v_{k+1}(s)$</mathjax> from <mathjax>$v_k(s')$</mathjax></li>
</ul>
</li>
<li>until convergence</li>
</ul>
<p>Unlike policy iteration, there is no explicit policy here, and the intermediate value functions may not correspond to any policy. But once we have <mathjax>$v_*(s)$</mathjax> we get the optimal policy.</p>
<h3>Asynchronous dynamic programming</h3>
<p>The methods so far have used synchronous backups, i.e. each iteration we look at and update every single state simultaneously before updating any state again.</p>
<p>We can instead use <em>asynchronous backups</em> in which states are backed up individually in any order, without waiting for other states to update. This can significantly reduce computation. As long as we continue to select all states (again, order doesn't matter), we will still have convergence.</p>
<p>Three asynchronous dynamic programming methods:</p>
<h4>In-place dynamic programming</h4>
<p>Synchronous value iteration stores two copies of the value function, i.e. for all <mathjax>$s \in S$</mathjax>, we have:</p>
<p><mathjax>$$
v_{\text{new}}(s) \leftarrow \max_{a \in A}(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{\text{old}}(s'))
$$</mathjax></p>
<p><em>In-place</em> value iteration, on the other hand, only stores one copy of the value function, i.e. for all <mathjax>$s \in S$</mathjax>, we instead have:</p>
<p><mathjax>$$
v(s) \leftarrow \max_{a \in A}(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v(s'))
$$</mathjax></p>
<p>That is, we immediately update the value function and always use the newest value function.</p>
<h4>Prioritised sweeping</h4>
<p>Sometimes the order can affect how quickly you reach the optimal value function. <em>Prioritised sweeping</em> uses the magnitude of the Bellman error to guide state selection, e.g.:</p>
<p><mathjax>$$
|\max_{a \in A}(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v(s')) - v(s)|
$$</mathjax></p>
<p>We backup the state with the largest remaining Bellman error, then update the Bellman error of affected states after each backup.</p>
<p>The magnitude of the Bellman error tells us how much our value estimate for that state changed; the intuition is that this change was really big, we want to attend to that state first, since will likely have a large influence on other values.</p>
<h4>Real-time dynamic programming</h4>
<p>With <em>real-time dynamic programming</em> we only select and update the states that the agent actually visits.</p>
<h3>Complexity</h3>
<p>These algorithms (i.e. algorithms based on the state-value function <mathjax>$v_{\pi}(s)$</mathjax> or <mathjax>$v_*(s)$</mathjax>) have complexity <mathjax>$O(mn^2)$</mathjax> per iteration for <mathjax>$m$</mathjax> actions and <mathjax>$n$</mathjax> states.</p>
<p>Dynamic programming uses <em>full-width</em> backups which means we consider <em>all</em> actions and <em>all</em> successor states (i.e. we consider the full branching factor), which is really expensive. Furthermore, to actually do these branch lookaheads we need to know the MDP transitions and reward function (i.e. have a model of the dynamics of the environment).</p>
<p>So we run into the problem of dimensionality, where the number of states <mathjax>$n = |S|$</mathjax> grows exponentially with the number of state variables. So this is not a great approach for larger problems.</p>
<p>One way around this is by sampling - instead of considering the entire branching factor, we sample a single complete trajectory.</p>
<p>But sometimes the problem is so big that even one backup is too expensive - in these cases, we can use <em>sample backups</em>, i.e. we start in a state, sample one action according to our policy, sample one transition according to our dynamics, etc, then backup this branch.</p>
<p>With sample backups, because we are sampling from the dynamics of the environment, this also frees us from needing a model of the dynamics of the environment.</p>
<h2>Model-free prediction</h2>
<p>We have an environment that can be represented as an MDP but we are not given the dynamics or reward function (i.e. we don't know what the MDP is). With <em>model-free</em> prediction methods, we can still learn a value function even without this knowledge (i.e. without needing to model the environment).</p>
<h3>Monte-Carlo learning</h3>
<p>Not necessarily efficient, but effective.</p>
<p>MC methods learn directly from <em>complete</em> episodes of experience. Note that MC methods must learn from episodes (i.e. they are only applicable to <em>episodic MDPs</em>, where episodes eventually terminate). So by "complete" episode, this means the episode is expanded to a terminal state. We do this a lot (each episode is a sample) and estimate the value of our start state as the mean return from these episodes.</p>
<p>Note that the <em>return</em> is the total discounted reward.</p>
<p>The value function is usually the expected return (where <mathjax>$G_t$</mathjax> is the return):</p>
<p><mathjax>$$
v_{\pi}(s) = E_{\pi}[G_t | S_t = s]
$$</mathjax></p>
<p>But Monte-Carlo policy evaluation uses the <em>empirical mean</em> return instead of the expected return (i.e. as mentioned before, we collect sample episodes and compute the mean of their returns).</p>
<p>So how do we get these empirical mean returns for <em>all</em> states in the environment?</p>
<p>There are two methods:</p>
<ul>
<li><em>first-visit</em> Monte-Carlo policy evaluation: to evaluate a state <mathjax>$s$</mathjax>, the first time-step <mathjax>$t$</mathjax> that state <mathjax>$s$</mathjax> is visited in an episode (i.e. if the state is returned to later one, ignore), increment counter <mathjax>$N(s) \leftarrow N(s) + 1$</mathjax>, which tracks number of visits to the state, increment the total return <mathjax>$S(s) \leftarrow S(s) + G_t$</mathjax>, then the value is estimated as the mean return <mathjax>$V(s) = \frac{S(s)}{N(s)}$</mathjax>. By the law of large numbers, <mathjax>$V(s) \to v_{\pi}(s)$</mathjax> as <mathjax>$N(s) \to \infty$</mathjax>; that is, with enough samples, this will converge on the true value.</li>
<li><em>every-visit</em> Monte-Carlo policy evaluation: same as the first-visit variant, except now we do these updates for <em>every</em> visit to the state (instead of just the first)</li>
</ul>
<p>The mean can be computed incrementally, i.e. we can do it in an online fashion. The mean <mathjax>$\mu_1, \mu_2, \dots$</mathjax> of a sequence <mathjax>$x_1, x_2, \dots$</mathjax> can be computed incrementally:</p>
<p><mathjax>$$
\begin{aligned}
\mu_k &amp;= \frac{1}{k} \sum_{j=1}^k x_j \\
&amp;= \frac{1}{k}(x_k + \sum_{j=1}^{k-1} x_j) \\
&amp;= \frac{1}{k} (x_k + (k-1) \mu_{k-1}) \\
&amp;= \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})
\end{aligned}
$$</mathjax></p>
<p>Using this, we'd change our <mathjax>$V(s)$</mathjax> update to:</p>
<p><mathjax>$$
V(s) \leftarrow V(s) + \frac{1}{N(s)} (G_t - V(s))
$$</mathjax></p>
<p>For non-stationary problems (which is the typical case) we might want to have a running mean (i.e. forget old episodes):</p>
<p><mathjax>$$
V(s) \leftarrow V(s) + \alpha (G_t - V(s))
$$</mathjax></p>
<h3>Temporal Difference learning</h3>
<p>TD methods, like Monte Carlo learning, learns from episodes, but it can learn from <em>incomplete</em> episodes by <em>bootstrapping</em> (similar to dynamic programming). We substitute the rest of the trajectory (the rest of the episode, before it is finished) with an estimate of what will happen from that state onwards. You take another step, generating an estimate for that step and updating the previous estimate with what what you've learned from then new step.</p>
<p>So whereas with Monte-Carlo learning we update <mathjax>$V(S_t)$</mathjax> towards the <em>actual</em> return <mathjax>$G_t$</mathjax>, with temporal-difference learning (<mathjax>$TD(0)$</mathjax>) we update <mathjax>$V(S_t)$</mathjax> towards the <em>estimated</em> return <mathjax>$R_{t+1} + \gamma V(S_{t+1})$</mathjax> (like the Bellman equations):</p>
<p><mathjax>$$
V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
$$</mathjax></p>
<p><mathjax>$R_{t+1} + \gamma V(S_{t+1})$</mathjax> is called the <em>TD target</em>.</p>
<p><mathjax>$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$</mathjax> is called the <em>TD error</em> (the difference between our estimate before and after taking the step).</p>
<p>The fact that TD methods do not require complete methods means that it can be applied to non-terminating environments.</p>
<p>TDs are often more efficient for Markov environments because they exploit the Markov property (assuming the current state summarizes all previous states). MC methods, on the other hand, do not exploit this property.</p>
<p>Both MC and TD use samples, whereas dynamic programming does not (dynamic programming is exhaustive).</p>
<h3><mathjax>$TD(\lambda)$</mathjax></h3>
<p>A compromise between TD (looks 1 step into the future) and MC (looks all the way to the end) is to look <mathjax>$n$</mathjax> steps into the future (i.e. we observe <mathjax>$n$</mathjax> steps and update the estimate <mathjax>$n$</mathjax> steps in the past).</p>
<p>So we define the <mathjax>$n$</mathjax>-step return as:</p>
<p><mathjax>$$
G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})
$$</mathjax></p>
<p>So <mathjax>$n$</mathjax>-step temporal-difference learning's update is:</p>
<p><mathjax>$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t^{(n)} - V(S_t))
$$</mathjax></p>
<p>We don't have to commit to one value for <mathjax>$n$</mathjax>, we can actually average over <mathjax>$n$</mathjax>-step returns over different <mathjax>$n$</mathjax>. In fact, we can consider <em>all</em> <mathjax>$n$</mathjax> values with <mathjax>$TD(\lambda)$</mathjax>. Here, we compute the <mathjax>$\lambda$</mathjax>-return <mathjax>$G_t^{\lambda}$</mathjax> which combines all <mathjax>$n$</mathjax>-step returns <mathjax>$G_t^{(n)}$</mathjax>:</p>
<p><mathjax>$$
G_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}
$$</mathjax></p>
<p><mathjax>$\lambda$</mathjax> takes a value from 0 to 1.</p>
<p>So the update is (this is called <em>forward-view</em> <mathjax>$TD(\lambda)$</mathjax>):</p>
<p><mathjax>$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t^{\lambda} - V(S_t))
$$</mathjax></p>
<p>Note that like MC this can only be computed from complete episodes.</p>
<p>There is <em>backward-view</em> <mathjax>$TD(\lambda)$</mathjax> which allows online updates at every step (i.e. from incomplete sequences).</p>
<p>We compute <em>eligibility traces</em> for the credit assignment problem which helps us figure out the eligibility of the states we saw for credit assignment (i.e. responsibility for the reward at the end). These combine the frequency heuristic (assign credit to the most frequent states) and the recency heuristic (assign credit to the most recent states):</p>
<p><mathjax>$$
\begin{aligned}
E_0(s) &amp;= 0 \\
E_t(s) &amp;= \gamma \lambda E_{t-1}(s) + \mathbb{1}(S_t = s)
\end{aligned}
$$</mathjax></p>
<p>So in backward-view <mathjax>$TD(\lambda)$</mathjax> we keep an eligibility trace for every state <mathjax>$s$</mathjax>. We update value <mathjax>$V(s)$</mathjax> for every state <mathjax>$s$</mathjax> in proportion to TD-error <mathjax>$\delta t$</mathjax> and eligibility trace <mathjax>$E_t(s)$</mathjax>:</p>
<p><mathjax>$$
\begin{aligned}
\delta_t &amp;= R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \\
V(s) \leftarrow V(s) + \alpha \delta_t E_t(s)
\end{aligned}
$$</mathjax></p>
<p>When <mathjax>$\lambda=0$</mathjax> we have the "vanilla" TD algorithm as presented earlier.</p>
<h2>Model-Free Control</h2>
<p>So how does an agent in a new environment learn how to maximize its reward? Another way of putting this: how do we optimize the value function of an unknown MDP?</p>
<p>There are two main paradigms for control:</p>
<ul>
<li><em>on-policy</em>: learning about policy <mathjax>$\pi$</mathjax> while sampling experiences from following <mathjax>$\pi$</mathjax></li>
<li><em>off-policy</em>: learning about policy <mathjax>$\pi$</mathjax> while sampling experiences from following some other policy <mathjax>$\mu$</mathjax></li>
</ul>
<p>Generally, the MDP might be unknown, but we can sample experiences, or the MDP might be known, but too big to use except by sampling it. So in either case, model-free control is appropriate.</p>
<p>We can take approach as we saw with dynamic programming: generalized policy iteration. That is, we evaluate our policy (estimate <mathjax>$v_{\pi}$</mathjax>), e.g. iterative policy evaluation, and then improve our policy (generate <mathjax>$\pi' \geq \pi$</mathjax>), e.g. greedy policy improvement.</p>
<p>So generalized policy iteration has two components:</p>
<ul>
<li>policy evaluation</li>
<li>policy improvement (e.g. greedy policy improvement)</li>
</ul>
<h3>Monte Carlo control</h3>
<p>Unfortunately we can't just drop in a model-free policy evaluation method (e.g. Monte-Carlo evaluation) as our policy evaluation method to use with greedy policy improvement.</p>
<p>One reason is because of exploration - with model-free policy evaluation methods, we only sample experiences so there may be some we miss out on. If we use greedy policy improvement, we'll never explore those other experiences.</p>
<p>Another is because greedy policy improvement over <mathjax>$V(s)$</mathjax> requires a model of the MDP:</p>
<p><mathjax>$$
\pi'(s) = \argmax_{a \in A} R_s^a + P_{ss'}^a V(s')
$$</mathjax></p>
<p>That is, we need <mathjax>$P_{ss'}^a$</mathjax> and <mathjax>$R_s^a$</mathjax>, which we don't know (because we are model-free).</p>
<p>However, greedy policy improvement over <mathjax>$Q(s,a)$</mathjax> is model-free:</p>
<p><mathjax>$$
\pi'(s) = \argmax_{a \in A} Q(s,a)
$$</mathjax></p>
<p>So instead our Monte-Carlo policy evaluation estimates <mathjax>$q_{\pi}$</mathjax> instead of <mathjax>$v_{\pi}$</mathjax>, and now we can use it with greedy policy improvement.</p>
<p>However, we still have the problem of exploration.</p>
<p>One way around this is <em><mathjax>$\epsilon$</mathjax>-greedy exploration</em>, where with probability <mathjax>$1 - \epsilon$</mathjax> we choose the greedy action and with probability <mathjax>$\epsilon$</mathjax> we choose an action at random.</p>
<p>For any <mathjax>$\epsilon$</mathjax>-greedy policy <mathjax>$\pi$</mathjax>, the <mathjax>$\epsilon$</mathjax>-greedy policy <mathjax>$\pi'$</mathjax> with respect to <mathjax>$q_{\pi}$</mathjax> is guaranteed to be an improvement, i.e. <mathjax>$v_{\pi}'(s) \geq v_{\pi}(s)$</mathjax>:</p>
<p><mathjax>$$
\begin{aligned}
q_{\pi} (s, \pi'(s)) &amp;= \sum_{a \in A} \pi'(a|s) q_{\pi}(s,a) \\
&amp;= \frac{\epsilon}{m} \sum_{a \in A} q_{\pi}(s,a) + (1 - \epsilon) \max_{a \in A} q_{\pi}(s,a) \\
&amp;\geq \frac{\epsilon}{m} \sum_{a \in A} q_{\pi}(s,a) + (1 - \epsilon) \sum_{a \in A} \frac{\pi(a|s) - \frac{\epsilon}{m}}{1 - \epsilon} q_{\pi}(s,a) \\
&amp;= \sum_{a \in A} \pi(a|s) q_{\pi}(s,a) = v_{\pi}(s)
\end{aligned}
$$</mathjax></p>
<p>So now our policy iteration strategy has become:</p>
<ul>
<li>policy evaluation: Monte-Carlo policy evaluation</li>
<li>policy improvement: <mathjax>$\epsilon$</mathjax>-greedy policy improvement</li>
<li>every episode</li>
</ul>
<p>We can do this update after each episode.</p>
<p>Given infinite time, this will eventually explore all states, but it could happen very slowly (there may be some hard-to-access states that take awhile to randomly reach). In practice, we don't want to explore ad infinitum, we want to explore until we find an optimal policy, then stop exploring.</p>
<p><em>Greedy in the limit with infinite exploration</em> (GLIE) has two properties:</p>
<ul>
<li>all state-action pairs are explored infinitely many times: <mathjax>$\lim_{k \to \infty} N_l(s,a) = \infty$</mathjax></li>
<li>the policy converges on a greedy policy: <mathjax>$\lim_{k \to \infty} \pi_k (a|s) = \mathbb{1} (a = \argmax_{a' \in A} Q_k(s, a'))$</mathjax></li>
</ul>
<p>For example, <mathjax>$\epsilon$</mathjax>-greedy is GLIE if <mathjax>$\epsilon$</mathjax> reduces to zero at <mathjax>$\epsilon_k = \frac{1}{k}$</mathjax> (i.e. decay <mathjax>$\epsilon$</mathjax> slowly towards 0).</p>
<p>We can modify our Monte-Carlo control to be GLIE:</p>
<ul>
<li>sample <mathjax>$k$</mathjax>th episode using <mathjax>$\pi$</mathjax>: <mathjax>$\{S_1, A_1, R_2, \dots, S_T\} \sim \pi$</mathjax></li>
<li>for each state <mathjax>$S_t$</mathjax> and action <mathjax>$A_t$</mathjax> in the episode:</li>
</ul>
<p><mathjax>$$
\begin{aligned}
N(S_t, A_t) &amp;\leftarrow N(S_t, A_t) + 1 \\
Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))
\end{aligned}
$$</mathjax></p>
<p>That is, we keep track of how many times we've encountered this state-action pair and update its values according to how many times we've encountered it.</p>
<p>Then we improve the policy:</p>
<p><mathjax>$$
\begin{aligned}
\epsilon &amp;\leftarrow \frac{1}{k} \\
\pi &amp;\leftarrow \epsilon\text{-greedy}(Q)
\end{aligned}
$$</mathjax></p>
<p>This converges to the optimal action-value function <mathjax>$q^*(s,a)$</mathjax>.</p>
<h3>Temporal-difference control</h3>
<p>TD learning has several advantages over MC:</p>
<ul>
<li>lower variance</li>
<li>online</li>
<li>can use incomplete sequences</li>
</ul>
<p>So we just use TD instead of MC for policy evaluation, where we apply TD to <mathjax>$Q(S,A)$</mathjax>, use <mathjax>$\epsilon$</mathjax>-greedy policy improvement, and update every time-step (as opposed to between episodes). This method is called <em>Sarsa</em>:</p>
<p><mathjax>$$
Q(S,A) \leftarrow Q(S,A) + \alpha (R + \gamma Q(S', A') - Q(S, A))
$$</mathjax></p>
<p>We start with some state-action pair <mathjax>$S,A$</mathjax>, we see a reward <mathjax>$R$</mathjax>, end up in state <mathjax>$S'$</mathjax>, and sample the policy again to generate <mathjax>$A'$</mathjax> (hence "SARSA").</p>
<p>So our (on-policy) policy iteration strategy has become:</p>
<ul>
<li>policy evaluation: Sarsa</li>
<li>policy improvement: <mathjax>$\epsilon$</mathjax>-greedy policy improvement</li>
<li>every time-step</li>
</ul>
<p>In greater detail:</p>
<ul>
<li>arbitrarily initialize <mathjax>$Q(s,a)$</mathjax> for each <mathjax>$s \in \mathcal S, a \in \mathcal A(s)$</mathjax> and <mathjax>$Q(\text{terminal state}, \cdot) = 0$</mathjax></li>
<li>repeat, for each episode:<ul>
<li>initialize <mathjax>$S$</mathjax></li>
<li>choose <mathjax>$A$</mathjax> from <mathjax>$S$</mathjax> using policy derived from <mathjax>$Q$</mathjax> (e.g. <mathjax>$\epsilon$</mathjax>-greedy)</li>
<li>repeat, for each step of episode:<ul>
<li>take action <mathjax>$A$</mathjax>, observe <mathjax>$R, S'$</mathjax></li>
<li>choose <mathjax>$A'$</mathjax> from <mathjax>$S'$</mathjax> using policy derived from Q (e.g. <mathjax>$\epsilon$</mathjax>-greedy)</li>
<li><mathjax>$Q(S,A) \leftarrow Q(S,A) + \alpha (R + \gamma Q(S', A') - Q(S, A))$</mathjax></li>
<li><mathjax>$S \leftarrow S'$</mathjax>, <mathjax>$A \leftarrow A'$</mathjax></li>
</ul>
</li>
<li>until <mathjax>$S$</mathjax> is terminal</li>
</ul>
</li>
</ul>
<p>Here <mathjax>$Q$</mathjax> is a lookup-table, but it doesn't have to be (later other forms will be covered).</p>
<p>This is on-policy because we are evaluating and improving the policy we are currently using.</p>
<p>Sarsa converges to the optimal action-value function <mathjax>$Q(s,a) \to q^*(s,a)$</mathjax> under the following conditions:</p>
<ul>
<li>GLIE sequence of policies <mathjax>$pi_t(a|s)$</mathjax> (e.g. can use the same modifications we did with MC above)</li>
<li>Robbins-Monro sequence of step-sizes <mathjax>$\alpha_t$</mathjax>:</li>
</ul>
<p><mathjax>$$
\begin{aligned}
\sum_{t=1}^{\infty} \alpha_t &amp;= \infty \\
\sum_{t=1}^{\infty} \alpha_t^2 &amp;&lt; \infty
\end{aligned}
$$</mathjax></p>
<p>These basically say:</p>
<ul>
<li>your step size is sufficiently large that your <mathjax>$Q$</mathjax> value can change as much as it needs to (e.g. if your initial estimate is way off)</li>
<li>eventually the changes to your <mathjax>$Q$</mathjax> values need to trend towards 0</li>
</ul>
<p>In practice we often don't worry about the Robbins-Monro sequence of step-sizes, and sometimes even GLIE, and Sarsa still works out.</p>
<h3><mathjax>$n$</mathjax>-step Sarsa</h3>
<p>What is the middle ground between TD and MC control?</p>
<p>We can again consider <mathjax>$n$</mathjax>-step returns, like we did previously comparing TD and MC evaluation methods.</p>
<p>The <mathjax>$n$</mathjax>-step Q-return would be:</p>
<p><mathjax>$$
q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n})
$$</mathjax></p>
<p>Then <mathjax>$n$</mathjax>-step Sarsa updates <mathjax>$Q(s,a)$</mathjax> towards the <mathjax>$n$</mathjax>-step Q-return:</p>
<p><mathjax>$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (q_t^{(n)} - Q(S_t, A_t))
$$</mathjax></p>
<p>From this we get Sarsa<mathjax>$(\lambda)$</mathjax> and there are forward view and backward view variants like there are with <mathjax>$TD(\lambda)$</mathjax>.</p>
<p>With forward view Sarsa<mathjax>$(\lambda)$</mathjax> we also use weight <mathjax>$(1 - \lambda)\lambda^{n-1}$</mathjax>, i.e.:</p>
<p><mathjax>$$
q_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_t^{(n)}
$$</mathjax></p>
<p>And use this with the general <mathjax>$n$</mathjax>-step Sarsa update above:</p>
<p><mathjax>$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (q_t^{(n)} - Q(S_t, A_t))
$$</mathjax></p>
<p>And in this way we are averaging over these <mathjax>$n$</mathjax>-step Q-returns.</p>
<p>With backward view Sarsa<mathjax>$(\lambda)$</mathjax> we can again use eligibility traces in an online algorithm, the difference from <mathjax>$TD(\lambda)$</mathjax> is that Sarsa<mathjax>$(\lambda)$</mathjax> has one eligibility trace for each state-action pair:</p>
<p><mathjax>$$
\begin{aligned}
E_0(s,a) &amp;= 0 \\
E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \mathbb 1 (S_t = s, A_t = a)
\end{aligned}
$$</mathjax></p>
<p>Basically this says, every time we visit a state-action pair <mathjax>$(s,a)$</mathjax> we increment its eligibility trace by 1 (using the indicator variable), and we also decay the eligibility trace of all state-action pairs (even the ones we don't visit) by a little.</p>
<p><mathjax>$Q(s,a)$</mathjax> is updated for every state <mathjax>$s$</mathjax> and action <mathjax>$a$</mathjax> in proportion to TD-error <mathjax>$\delta_t$</mathjax> (i.e. the difference between actual reward and what we expected) and eligibility trace <mathjax>$E_t(s,a)$</mathjax>:</p>
<p><mathjax>$$
\begin{aligned}
\delta_t &amp;= R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\
Q(s,a) &amp;\leftarrow Q(s,a) + \alpha \delta_t E_t(s,a)
\end{aligned}
$$</mathjax></p>
<p>Sarsa<mathjax>$(\lambda)$</mathjax> in more detail:</p>
<ul>
<li>arbitrarily initialize <mathjax>$Q(s,a)$</mathjax> for all <mathjax>$s \in \mathcal S, a \in \mathcal A(s)$</mathjax></li>
<li>repeat (for each episode):<ul>
<li><mathjax>$E(s,a) = 0$</mathjax> for all <mathjax>$s \in \mathcal S, a \in \mathcal A(s)$</mathjax></li>
<li>initialize <mathjax>$S, A$</mathjax></li>
<li>repeat (for each step of episode):<ul>
<li>take action <mathjax>$A$</mathjax>, observe <mathjax>$R, S'$</mathjax></li>
<li>choose <mathjax>$A'$</mathjax> from <mathjax>$S'$</mathjax> using policy derived from <mathjax>$Q$</mathjax> (e.g. <mathjax>$\epsilon$</mathjax>-greedy)</li>
<li><mathjax>$\delta \leftarrow R + \gamma Q(S', A') - Q(S,A)$</mathjax></li>
<li><mathjax>$E(S,A) \leftarrow E(S,A) + 1$</mathjax></li>
<li>for all <mathjax>$s \in mathcal S, a \in \mathcal A(s)$</mathjax>:<ul>
<li><mathjax>$Q(s,a) \leftarrow Q(s,a) + \alpha \delta E(s,a)$</mathjax></li>
<li><mathjax>$E(s,a) \leftarrow \gamma \lambda E(s,a)$</mathjax></li>
</ul>
</li>
<li><mathjax>$S \leftarrow S'$</mathjax>, <mathjax>$A \leftarrow A'$</mathjax></li>
</ul>
</li>
<li>until <mathjax>$S$</mathjax> is terminal</li>
</ul>
</li>
</ul>
<p>Essentially what the <mathjax>$\lambda$</mathjax> value dictates is how far information propagates backwards through the trajectory the agent took.</p>
<p>Consider a situation where a long path is taken, with zero reward at each step, until the agent reaches a terminal state with a positive reward (one episode).</p>
<p>With <mathjax>$\lambda=0$</mathjax>, i.e. one-step Sarsa, only the state immediately before the terminal state is updated; i.e. the reward propagates backwards only one step. The agent would have to travel along that same path many times for the reward to propagate further along that trajectory.</p>
<p>With <mathjax>$\lambda &gt; 0$</mathjax>, the reward propagates backwards further in one episode (each previous step has its eligibility trace increased, i.e. it is assigned more responsibility for leading to that reward)</p>
<h3>Off-policy learning</h3>
<p>Evaluate target policy <mathjax>$\pi(a|s)$</mathjax> to compute <mathjax>$v_{\pi}(s)$</mathjax> or <mathjax>$q_{\pi}(s,a)$</mathjax> while following a different behavior policy <mathjax>$\mu(a|s)$</mathjax> (i.e. we use this policy to choose actions).</p>
<p>This can be thought of as "learning by observation", i.e. the agent watches another agent perform some task, and from that observation it can actually learn a better policy than the one it observed.</p>
<p>This can also be thought of as "reusing" experience learned from old policies.</p>
<p>Off-policy learning has a couple advantages in what it enables us to do:</p>
<ul>
<li>learn about an optimal policy while following an <em>exploratory</em> policy</li>
<li>learn about <em>multiple</em> policies while following one policy</li>
</ul>
<p>So how can we do this?</p>
<h4>Importance sampling</h4>
<p>Importance sampling gives us a means to estimate the expectation of a different distribution using another distribution:</p>
<p><mathjax>$$
\begin{aligned}
E_{X \sim P}[f(X)] &amp;= \sum P(X) f(X) \\
&amp;= \sum Q(X) \frac{P(X)}{Q(X)}f(X) \\
&amp;= E_{X \sim Q}[\frac{P(X)}{Q(X)}f(X)]
\end{aligned}
$$</mathjax></p>
<p>For off-policy Monte Carlo, we can apply important sampling and use returns generated from <mathjax>$\mu$</mathjax> to evaluate <mathjax>$\pi$</mathjax>. We weight the return <mathjax>$G_t$</mathjax> according to the similarity between policies and multiply importance sampling corrections along the whole episode:</p>
<p><mathjax>$$
G_t^{\pi/\mu} = \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} \frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})} \dots \frac{\pi(A_T|S_T)}{\mu(A_T|S_T)} G_t
$$</mathjax></p>
<p>Then update value towards the corrected return:</p>
<p><mathjax>$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t^{\pi/\mu} - V(S_t))
$$</mathjax></p>
<p>(we can't use this if <mathjax>$\mu$</mathjax> is zero when <mathjax>$\pi$</mathjax> is non-zero)</p>
<p>However, importance sampling can increase variance to be too high to be practical (since Monte Carlo learning is already high variance). Because of the high variance, the target and behavior policies never get close enough to be useful.</p>
<p>So off-policy TD is preferred; it has much lower variance. We use the TD targets generated from <mathjax>$\mu$</mathjax> to evaluate <mathjax>$\pi$</mathjax>, and we weight the TD target <mathjax>$R + \gamma V(S')$</mathjax> by importance sampling. We only need to importance sample over one step because we're bootstrapping off of one step:</p>
<p><mathjax>$$
V(S_t) \leftarrow V(S_t) + \alpha (\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} (R_{t+1} + \gamma V(S_{t+1})) - V(S_t))
$$</mathjax></p>
<h3>Q-learning</h3>
<p>The best offline-policy is Q-learning, which does not require importance sampling.</p>
<p>We choose the next action using the behavior policy, <mathjax>$A_{t+1} \sim \mu(\cdot | S_t)$</mathjax>.</p>
<p>We also consider an alternative successor action from our target policy, <mathjax>$A' \sim \pi(\cdot | S_t)$</mathjax>.</p>
<p>Then we update <mathjax>$Q(S_t, A_t)$</mathjax> (of the action we actually took) towards the value of the alternative action:</p>
<p><mathjax>$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A') - Q(S_t, A_t))
$$</mathjax></p>
<p>There's a special case of Q-learning which is usually what is meant when the Q-learning algorithm is referred to, where the target policy <mathjax>$\pi$</mathjax> is greedy with respect to <mathjax>$Q(s,a)$</mathjax>:</p>
<p><mathjax>$$
\pi(S_{t+1}) = \argmax_{a'} Q(S_{t+1}, a')
$$</mathjax></p>
<p>The behavior policy <mathjax>$\mu$</mathjax> is, for example, <mathjax>$\epsilon$</mathjax>-greedy with respect to <mathjax>$Q(s,a)$</mathjax>.</p>
<p>The Q-learning target then simplifies:</p>
<p><mathjax>$$
\begin{aligned}
&amp;R_{t+1} + \gamma Q (S_{t+1}, A') \\
&amp;R_{t+1} + \gamma Q (S_{t+1}, argmax_{a'} Q(S_{t+1}, a')) \\
&amp;R_{t+1} + \max_{a'} \gamma Q (S_{t+1}, a')
\end{aligned}
$$</mathjax></p>
<p>What happens here is both the behavior and target policies improve.</p>
<p>So the updates then are:</p>
<p><mathjax>$$
Q(S,A) \leftarrow Q(S,A) + \alpha (R+ \gamma \max_{a'} Q(S', a') - Q(S,A))
$$</mathjax></p>
<p>and it converges to the optimal action-value function.</p>
<h2>Value Function Approximation</h2>
<p>If state spaces get very large (e.g. with Go, with <mathjax>$10^{170}$</mathjax> states, or any problem with a continuous state space), the methods discussed so far (i.e. these tabular methods where we essentially maintain a lookup table of values, e.g. a table of values where each state <mathjax>$s$</mathjax> has an entry <mathjax>$V(s)$</mathjax> or where every state-action pair <mathjax>$s,a$</mathjax> has an entry <mathjax>$Q(s,a)$</mathjax>) will not scale. We won't have enough memory or they will be very slow to learn (too many spaces to explore).</p>
<p>Instead, we can consider <em>value function approximation</em> in which we don't need to distinctly map each state to some value, rather we use some function to approximate the value of states, such that nearby states take similar values (i.e. generalize to states we have not encountered before).</p>
<p>These allow us to scale up the model-free methods we've seen so far for both prediction and control.</p>
<p>So there is some true value function for a policy <mathjax>$v_{\pi}(s)$</mathjax> (or action-value function <mathjax>$q_\pi(s,a)$</mathjax>). With value function approximation we estimate it by fitting a function to it (e.g. with a neural network):</p>
<p><mathjax>$$
\begin{aligned}
\hat v(s, w) &amp;\approx v_{\pi}(s) \\
\hat q(s, a, w) &amp;\approx q_{\pi}(s, a)
\end{aligned}
$$</mathjax></p>
<p>where <mathjax>$w$</mathjax> is some vector of parameters/weights. We can update it using MC or TD learning.</p>
<p>The function we learn could:</p>
<ul>
<li>take an input state <mathjax>$s$</mathjax> and return <mathjax>$\hat v(s,w)$</mathjax></li>
<li>take an input state <mathjax>$s$</mathjax> and an input action <mathjax>$a$</mathjax> and return <mathjax>$\hat q(s,a,w)$</mathjax></li>
<li>take an input state <mathjax>$s$</mathjax> and return values for all actions <mathjax>$\hat q(s,a,w) \forall a \in \mathcal A$</mathjax></li>
</ul>
<p>We want our training method to work with non-stationary (our policy will be changing, causing its value function to change), non-iid (e.g. where I am next correlates highly with where I just was) data. This is a main difference between supervised and reinforcement learning.</p>
<p>Broadly, we can break value function approximation into two categories:</p>
<ul>
<li><em>incremental methods</em>, in which the approximator (e.g. a neural network) makes an update after each time step (somewhat similar to online methods, but not exactly)</li>
<li><em>batch methods</em>, where the approximator looks at whole sets of data</li>
</ul>
<p>As an aside, note that table lookup is actually a special case of linear value function approximation, where each state is a one-hot vector and its associated weight is its lookup value.</p>
<h3>Incremental methods</h3>
<p>Basically, we use stochastic gradient descent to fit the approximator.</p>
<p>We can represent states with a feature vector.</p>
<p>Instead of computing mean squared error with respect to the true value function <mathjax>$v_{\pi}(s)$</mathjax> (which we don't know), we use a target, i.e.:</p>
<ul>
<li>for MC, the target is the return <mathjax>$G_t$</mathjax>, so the parameter update is:</li>
</ul>
<p><mathjax>$$
\Delta w = \alpha(G_t - \hat v(S_t, w)) \nabla_w \hat v (S_t, w)
$$</mathjax></p>
<ul>
<li>for <mathjax>$TD(0)$</mathjax>, the target is the TD target <mathjax>$R_{t+1} + \gamma \hat v(S_{t+1}, w)$</mathjax>, so the parameter update is:</li>
</ul>
<p><mathjax>$$
\Delta w = \alpha(R_{t+1} + \gamma \hat v(S_{t+1}, w) - \hat v(S_t, w)) \nabla_w \hat v (S_t, w)
$$</mathjax></p>
<ul>
<li>for <mathjax>$TD(\lambda)$</mathjax>, the target is the <mathjax>$\lambda$</mathjax>-return <mathjax>$G_t^{\lambda}$</mathjax></li>
</ul>
<p><mathjax>$$
\Delta w = \alpha(G_t^{\lambda} - \hat v(S_t, w)) \nabla_w \hat v (S_t, w)
$$</mathjax></p>
<p>Monte-Carlo with value function approximation involves running episodes, and these episodes essentially are "training data", e.g.:</p>
<p><mathjax>$$
(S_1, G_1), (S_2, G_2), \dots, (S_T, G_T)
$$</mathjax></p>
<p>The result is basically a supervised learning problem. This will converge to a local optimum in both linear and non-linear cases.</p>
<p>TD learning (<mathjax>$TD(0)$</mathjax>) with value function approximation is similar in that we also assemble "training data":</p>
<p><mathjax>$$
(S_1, R_2 + \gamma \hat v(S_2, w)), (S_2, R_3 + \gamma \hat v(S_3, w)), \dots, (S_{T-1}, R_T)
$$</mathjax></p>
<p>But note that unlike Monte-Carlo, this sample is biased because we are using our neural network to estimate further reward at each step.</p>
<p>Linear <mathjax>$TD(0)$</mathjax> converges (close) to a global optimum.</p>
<p><mathjax>$TD(\lambda)$</mathjax> also produces a (biased) sample of "training data":</p>
<p><mathjax>$$
(S_1, G_1^{\lambda}), (S_2, G_2^{\lambda}), \dots, (S_{T-1}, G_{T-1}^{\lambda})
$$</mathjax></p>
<h3>Control with value approximation</h3>
<ul>
<li>Policy evaluation: approximate policy evaluation <mathjax>$\hat q(\cdot, cdot, w) \approx q_{\pi}$</mathjax></li>
<li>Policy improvement: <mathjax>$\epsilon$</mathjax>-greedy policy improvement</li>
</ul>
<p>So we want to approximate the action-value function (so we can remain model-free):</p>
<p><mathjax>$$
\hat q(S,A,w) \approx q_{\pi}(S,A)
$$</mathjax></p>
<p>Again we want to minimize the mean-squared error between the approximate action-value function <mathjax>$\hat q(S,A,w)$</mathjax> and the true action-value function <mathjax>$q_{\pi}(S,A)$</mathjax> and we again can use stochastic gradient descent.</p>
<p>We can represent both the states and actions with feature vectors.</p>
<p>Like with approximating the value function, we don't know the true action-value function, so we can't optimize against it. Rather, we again substitute a target for <mathjax>$q_{\pi}(S,A)$</mathjax>:</p>
<ul>
<li>for MC, the target is the return <mathjax>$G_t$</mathjax>, so the parameter update is:</li>
</ul>
<p><mathjax>$$
\Delta w = \alpha(G_t - \hat q(S_t, A_t, w)) \nabla_w \hat q (S_t, A_t, w)
$$</mathjax></p>
<ul>
<li>for <mathjax>$TD(0)$</mathjax>, the target is the TD target <mathjax>$R_{t+1} + \gamma \hat q(S_{t+1}, A_{t+1}, w)$</mathjax>, so the parameter update is:</li>
</ul>
<p><mathjax>$$
\Delta w = \alpha(R_{t+1} + \gamma \hat q(S_{t+1}, A_{t+1}, w) - \hat q(S_t, A_t, w)) \nabla_w \hat v (S_t, w)
$$</mathjax></p>
<ul>
<li>for <mathjax>$TD(\lambda)$</mathjax>, the target is the <mathjax>$\lambda$</mathjax>-return <mathjax>$q_t^{\lambda}$</mathjax>. For forward-view <mathjax>$TD(\lambda)$</mathjax>:</li>
</ul>
<p><mathjax>$$
\Delta w = \alpha(q_t^{\lambda} - \hat q(S_t, A_t, w)) \nabla_w \hat q (S_t, A_t, w)
$$</mathjax></p>
<p>For backward-view <mathjax>$TD(\lambda)$</mathjax>:</p>
<p><mathjax>$$
\begin{aligned}
\delta_t &amp;= R_{t+1} + \gamma \hat q(S_{t+1}, A_{t+1}, w) - \hat q(S_t, A_t, w) \\
E_t &amp;= \gamma \lambda E_{t-1} + \nabla_w \hat q(S_t, A_t, w) \\
\Delta w &amp;= \alpha \delta_1 E_t
\end{aligned}
$$</mathjax></p>
<p>Note that TD (weights) can sometimes blow up. A rough summary of when TD is guaranteed to converge ("ok") and when it <em>might</em> diverge ("maybe"):</p>
<table>
<thead>
<tr>
<th>On/Off-Policy</th>
<th>Algorithm</th>
<th>Table Lookup</th>
<th>Linear</th>
<th>Non-Linear</th>
</tr>
</thead>
<tbody>
<tr>
<td>On</td>
<td>MC</td>
<td>ok</td>
<td>ok</td>
<td>ok</td>
</tr>
<tr>
<td></td>
<td>TD(0)</td>
<td>ok</td>
<td>ok</td>
<td>maybe</td>
</tr>
<tr>
<td></td>
<td>TD(<mathjax>$\lambda$</mathjax>)</td>
<td>ok</td>
<td>ok</td>
<td>maybe</td>
</tr>
<tr>
<td>Off</td>
<td>MC</td>
<td>ok</td>
<td>ok</td>
<td>ok</td>
</tr>
<tr>
<td></td>
<td>TD(0)</td>
<td>ok</td>
<td>maybe</td>
<td>maybe</td>
</tr>
<tr>
<td></td>
<td>TD(<mathjax>$\lambda$</mathjax>)</td>
<td>ok</td>
<td>maybe</td>
<td>maybe</td>
</tr>
</tbody>
</table>
<p>Although MC works in all cases, it can be very, very slow (variance is too high), so we generally want to bootstrap (i.e  have <mathjax>$\lambda \neq 1$</mathjax>).</p>
<p>The reason TD may diverge in off-policy or non-linear approximations is because it does not follow the gradient of <em>any</em> objective function.</p>
<p><em>Gradient TD</em> on the other hand follows the true gradient of projected Bellman error, so it will guaranteed converge for all cases.</p>
<p>For control algorithms:</p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Table Lookup</th>
<th>Linear</th>
<th>Non-linear</th>
</tr>
</thead>
<tbody>
<tr>
<td>Monte-Carlo Control</td>
<td>ok</td>
<td>~ok</td>
<td>maybe</td>
</tr>
<tr>
<td>Sarsa</td>
<td>ok</td>
<td>~ok</td>
<td>maybe</td>
</tr>
<tr>
<td>Q-learning</td>
<td>ok</td>
<td>maybe</td>
<td>maybe</td>
</tr>
<tr>
<td>Gradient Q-learning</td>
<td>ok</td>
<td>ok</td>
<td>maybe</td>
</tr>
</tbody>
</table>
<p>Where "~ok" means it wiggles around the near-optimal value function (it might get a little worse, then get better, etc; we aren't guaranteed that every update will be an improvement).</p>
<h3>Batch methods</h3>
<p>The incremental methods waste experience - we have an experience, use it to update, then we toss that experience away. They aren't "sample efficient".</p>
<p>Batch methods try to find the value function that best fits all the data we've seen (all the experience).</p>
<p>With <em>experience replay</em> we just store all the experiences. Then, at every time step, we sample from these stored experiences and use it for a stochastic gradient descent update.</p>
<p><em>Deep Q-networks</em> use experience replay and fixed Q-targets (essentially Q-learning):</p>
<ul>
<li>take action <mathjax>$a_t$</mathjax> according to <mathjax>$\epsilon$</mathjax>-greedy policy</li>
<li>store transition <mathjax>$(s_t, a_t, r_{t+1}, s_{t+1})$</mathjax> in replace memory <mathjax>$\mathcal D$</mathjax></li>
<li>sample random mini-batch of transitions <mathjax>$(s,a,r,s')$</mathjax> from <mathjax>$\mathcal D$</mathjax></li>
<li>compute Q-learning targets wrt to old, fixed parameters <mathjax>$w^-$</mathjax></li>
<li>optimize MSE between Q-network and Q-learning targets using variant of stochastic gradient descent:</li>
</ul>
<p><mathjax>$$
L_i(w_i) = \mathbb E_{s,a,r,s' \sim \mathcal D_i} [(r + \gamma \max_{a'} Q(s', a'; w_i^-) - Q(s,a,;w_i))^2]
$$</mathjax></p>
<p>What prevents this from blowing up is experience replay and the fixed Q-targets.</p>
<p>Experience replay stabilizes updates by decorrelating the data (you're not seeing them in a specific sequence; they're randomized by sampling past experiences)</p>
<p>Here there are actually two neural networks (one with parameters <mathjax>$w^-$</mathjax> and one with parameters <mathjax>$w$</mathjax>). One network is basically the "frozen" old network, i.e. with the old parameters <mathjax>$w^-$</mathjax>, and we use that for bootstrapping (instead of always using the latest updated parameters) for some iterations, then we freeze the newer network as the old network and repeat. This also stabilizes the learning by decorrelating.</p>
<h2>Policy Gradient methods</h2>
<p>Policy gradient methods work directly with the policy, i.e. without using a value or action-value function. The general class of methods that directly learn a policy is called <em>policy-based</em> reinforcement learning (in contrast to <em>value-based</em> reinforcement learning which just learns a value function).</p>
<p>To review: we have thus far tried to learn a value (or action-value) function (e.g. approximating it with parameters <mathjax>$\theta$</mathjax>):</p>
<p><mathjax>$$
\begin{aligned}
V_{\theta}(s) &amp;\approx V^{\pi}(s) \\
Q_{\theta}(s, a) &amp;\approx Q^{\pi}(s, a)
\end{aligned}
$$</mathjax></p>
<p>and generated an <em>implicit</em> policy <em>from</em> this value function by using some policy improvement method, e.g. <mathjax>$\epsilon$</mathjax>-greedy. We don't learn a policy directly, we just use the value or action-value function to pick actions.</p>
<p>Policy gradient methods directly parametrize the policy itself:</p>
<p><mathjax>$$
\pi_{\theta}(s,a) = \mathbb P[a | s, \theta]
$$</mathjax></p>
<p>So the policy itself can be approximated by some function approximator, e.g. a neural network.</p>
<p>Here we again focus on model-free methods.</p>
<p>Advantages of policy-based RL:</p>
<ul>
<li>better convergence properties: remember that value-based methods could wiggle around points or even diverge under certain conditions; policy-based methods tend to be more stable since you're just following the policy gradient</li>
<li>effective in high-dimensional or continuous action spaces: we don't have to deal with any maximums, i.e. we don't have to figure out the maximum-valued action; this is good because that maximization itself can be very expensive, e.g. in continuous action spaces where there are many, many actions to maximize over. Policy-based methods give us the action to take in a more direct way.</li>
<li>can learn stochastic policies: deterministic policies may be easily exploited (e.g. in rock-paper-scissors, where the optimal policy is actually a uniform random policy)</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>typically converge to a local rather than global optimum</li>
<li>evaluating a policy is typically inefficient and high variance: value-based methods, by using the max, aggressively push you towards what you think is the best policy, but policy gradient methods take small steps in the right direction.</li>
</ul>
<p>Learning stochastic policies are useful in partially-observed environments (i.e. we are dealing with an POMDP rather than an MDP), i.e. when we have limited information about states.</p>
<p>A simple example is the aliased gridworld, where we don't know our exact position; rather, we only have features like "is there a wall to my north", "is there a wall to my south", etc. <em>State aliasing</em> refers to environments where states which may actually be different can't be distinguished given the features we know about. For example, for the following grid world:</p>
<pre><code>_____________________
| 0 | 1 | 2 | 3 | 4 |
|___|___|___|___|___|
| 5 |   | 6 |   | 7 |
|___|   |___|   |___|
</code></pre>
<p>Cells 1 and 3 are aliased, as are 5, 6 and 7. From the perspective of the agent, 1 and 3 are identical (both have walls just to the north and south), and 5, 6, and 7 are identical (both have walls to the west, east, and south).</p>
<p>A deterministic policy must always take the same action from aliased states. For example, if it learns to go left in cell 3, it will also go left in cell 1 (the two states are identical as far as the agent is concerned).</p>
<p>A stochastic policy, on the other hand, may learn to go left or right with 50/50 probability in cell 3 and will do the same for cell 1.</p>
<p>Remember that there is a deterministic optimal policy for any MDP, but this is not the case for POMDPs, where optimal policies may be stochastic.</p>
<h3>Policy objective functions</h3>
<p>So how do we optimize a policy? Remember, we want to find the best parameters <mathjax>$\theta$</mathjax> for a given policy <mathjax>$\pi_{\theta}(s,a)$</mathjax>. So we need some way to measure the quality of policy <mathjax>$\pi_{\theta}$</mathjax>, i.e. an objective function.</p>
<p>We have a few options:</p>
<ul>
<li>in episodic environments (i.e. where there is a clear notion of a "starting state") we can use the <em>start value</em>:</li>
</ul>
<p><mathjax>$$
J_1(\theta) = V^{\pi_{\theta}}(s_1) = \mathbb E_{\pi_{\theta}}[v_1]
$$</mathjax></p>
<ul>
<li>in continuing environments we can use the <em>average value</em>:</li>
</ul>
<p><mathjax>$$
J_{\text{av}V}(\theta) = \sum_s d^{\pi_{\theta}}(s) V^{\pi_{\theta}}(s)
$$</mathjax></p>
<ul>
<li>or the <em>average reward per time-step</em>:</li>
</ul>
<p><mathjax>$$
J_{\text{av}R}(\theta) = \sum_s d^{\pi_{\theta}}(s) \sum_a \pi_{\theta} (s,a) R_s^a
$$</mathjax></p>
<p>where <mathjax>$d^{\pi_{\theta}}(s)$</mathjax> is the stationary distribution of the Markov chain for <mathjax>$\pi_{\theta}}$</mathjax>.</p>
<p>Fortunately, the policy gradient is essentially the same for all of these objective functions.</p>
<p>So we want to optimize the policy, i.e. find <mathjax>$\theta$</mathjax> that maximizes <mathjax>$J(\theta)$</mathjax>.</p>
<p>Let <mathjax>$J(\theta)$</mathjax> be any policy objective function.</p>
<p>Policy gradient algorithms search for a local maximum in <mathjax>$J(\theta)$</mathjax> by ascending the gradient of the policy wrt parameters <mathjax>$\theta$</mathjax> (i.e. gradient ascent):</p>
<p><mathjax>$$
\Delta \theta = \alpha \nabla_{\theta} J(\theta)
$$</mathjax></p>
<p>where <mathjax>$\nabla_{\theta} J(\theta)$</mathjax> is the policy gradient, i.e.:</p>
<p><mathjax>$$
\nabla_{\theta} J(\theta) = \begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_1} \\
\vdots
\frac{\partial J(\theta)}{\partial \theta_n}
\end{bmatrix}
$$</mathjax></p>
<p>and <mathjax>$\alpha$</mathjax> is a step-size parameter.</p>
<h3>Finite difference policy gradient</h3>
<p>Computing gradients by finite differences just involves perturbing the parameters in each dimension and seeing if the objective function result is better:</p>
<ul>
<li>for each dimension <mathjax>$k \in [1,n]$</mathjax><ul>
<li>estimate <mathjax>$k$</mathjax>th partial derivative of objective function wrt <mathjax>$\theta$</mathjax> by perturbing <mathjax>$\theta$</mathjax> by a small amount <mathjax>$\epsilon$</mathjax> in the <mathjax>$k$</mathjax>th dimension:</li>
</ul>
</li>
</ul>
<p><mathjax>$$
\frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta + \epsilon u_k) - J(\theta)}{\epsilon}
$$</mathjax></p>
<p>where <mathjax>$u_k$</mathjax> is the unity vector with 1 in the <mathjax>$k$</mathjax>th component and 0 elsewhere.</p>
<p>This requires <mathjax>$n$</mathjax> evaluations to estimate the policy gradient in <mathjax>$n$</mathjax> dimensions. It is simple, noisy, inefficient, but sometimes effective, and it works for arbitrary policies, even if it is not differentiable.</p>
<h3>Monte Carlo policy gradient</h3>
<p>There are analytic ways of computing the gradient so we don't have to numerically compute it in each dimension.</p>
<p>We assume the policy <mathjax>$\pi_{\theta}$</mathjax> is differentiable whenever it is non-zero (i.e. wherever you are taking actions) and that we know the gradient <mathjax>$\nabla_{\theta} \pi_{\theta}(s,a)$</mathjax> (because we define the structure of the policy, e.g. a neural network).</p>
<p>We can use <em>likelihood ratios</em>. These exploit the following identity:</p>
<p><mathjax>$$
\begin{aligned}
\nabla_{\theta} \pi_{\theta} (s,a) &amp;= \pi_{\theta}(s,a) \frac{\nabla_{\theta} \pi_{\theta}(s,a)}{\pi_{\theta}(s,a)} \\
&amp;= \pi_{\theta}(s,a) \nabla_{\theta} \log \pi_{\theta}(s,a)
\end{aligned}
$$</mathjax></p>
<p><mathjax>$\nabla_{\theta} \log \pi_{\theta}(s,a)$</mathjax> is called the <em>score function</em>. You see this in maximum likelihood learning; here this essentially tells us how to adjust the policy to get more of a particular action.</p>
<p>For example, consider a (linear) softmax policy. We weight actions using a linear combination of features <mathjax>$\phi(s,a)^T \theta$</mathjax>. The probability of an action is proportional to exponentiated weight:</p>
<p><mathjax>$$
\pi_{\theta}(s,a) \propto e^{\phi(s,a)^T \theta}
$$</mathjax></p>
<p>The score function is:</p>
<p><mathjax>$$
\nabla_{\theta} \log \pi_{\theta}(s,a) = \phi(s,a) - \mathbb E_{\pi_{\theta}} [\phi(s, \cdot)]
$$</mathjax></p>
<p>i.e. the feature for the action we took (<mathjax>$\phi(s,a)$</mathjax>) minus the average feature for all actions we might have taken.</p>
<p>For another example, we could use a (linear) Gaussian policy, which is good for continuous action spaces. The mean is a linear combination of state features <mathjax>$\mu(s) = \phi(s)^T \theta$</mathjax>. The variance may be fixed <mathjax>$\sigma^2$</mathjax> or also parametrized.</p>
<p>The policy is Gaussian, so <mathjax>$a \sim \mathcal N(\mu(s), \sigma^2)$</mathjax>.</p>
<p>The score function is:</p>
<p><mathjax>$$
\nabla_{\theta} \log \pi_{\theta}(s,a) = \frac{(a - \mu(s))\phi(s)}{\sigma^2}
$$</mathjax></p>
<p>So how do we apply this?</p>
<p>Consider the simple case of a one-step MDP, i.e. our episodes consist of only one step. So we start in some state <mathjax>$s \sim d(s)$</mathjax> and terminate after one time step with reward <mathjax>$r = \mathcal R_{s,a}$</mathjax>.</p>
<p>We use likelihood ratios to compute the policy gradient:</p>
<p><mathjax>$$
\begin{aligned}
J(\theta) &amp;= \mathbb E_{\pi_{\theta}}[r] \\
&amp;= \sum_{s \in \mathcal S} d(s) \sum_{a \in \mathcal A} \pi_{\theta}(s,a) \mathcal R_{s,a} \\
\nabla_{\theta} J(\theta) &amp;= \sum_{s \in \mathcal S} d(s) \sum_{a \in A} \pi_{\theta}(s,a) \nabla_{\theta} \log \pi_{\theta}(s,a) \mathcal R_{s,a} \\
&amp;= \mathbb E_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) r]
\end{aligned}
$$</mathjax></p>
<p>You can see that this is still model-free since we replace <mathjax>$R_{s,a}$</mathjax> with an explicit value <mathjax>$r$</mathjax>, that is, we don't need to model the reward function; we only need to look at the actual reward <mathjax>$r$</mathjax> we received.</p>
<p>We can expand this to multi-step MDPs by just replacing the instantaneous reward <mathjax>$r$</mathjax> with the long-term value <mathjax>$Q^{\pi}(s,a)$</mathjax>.</p>
<p>The <em>policy gradient theorem</em> tells us that this ends up being the true gradient of the policy (this holds for any of the policy objectives we mentioned, i.e. start state objective, average reward and average value objectives):</p>
<p>The policy gradient theorem: For any differentiable policy <mathjax>$\pi_{\theta}(s,a)$</mathjax>, for any of the policy objective functions  <mathjax>$J = J_1, J_{\text{av}R}, \frac{1}{1 - \gamma}J_{\text{av}V}$</mathjax>, the policy gradient is:</p>
<p><mathjax>$$
\nabla_{\theta} J(\theta) = \mathbb E_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) Q^{\pi_{\theta}}(s,a)]
$$</mathjax></p>
<p>We can use this for <em>Monte-Carlo Policy Gradient</em>, which is essentially the <em>REINFORCE</em> algorithm:</p>
<ul>
<li>update parameters by stochastic gradient ascent (get rid of the expectation, just sample it instead, see below)</li>
<li>using policy gradient theorem</li>
<li>using return <mathjax>$v_t$</mathjax> as an unbiased sample of <mathjax>$Q^{\pi_{\theta}}(s_t, a_t)$</mathjax>:</li>
</ul>
<p><mathjax>$$
\Delta \theta_t = \alpha \nabla_{\theta} \log \pi_{\theta}(s_t,a_t) v_t
$$</mathjax></p>
<p>REINFORCE:</p>
<ul>
<li>arbitrarily initialize <mathjax>$\theta$</mathjax></li>
<li>for each episode <mathjax>${s_1, a_1, r_2, \dots, s_{T-1}, a_{T-1}, r_T} \sim \pi_{\theta}$</mathjax> do (at the end of the episode):<ul>
<li>for <mathjax>$t = 1$</mathjax> to <mathjax>$T-1$</mathjax> do:<ul>
<li><mathjax>$\theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(s_t,a_t) v_t$</mathjax></li>
</ul>
</li>
</ul>
</li>
<li>return <mathjax>$\theta$</mathjax></li>
</ul>
<p>where, like before, <mathjax>$v_t$</mathjax> is the accumulated reward from that time step <mathjax>$t$</mathjax> to the end of the episode.</p>
<p>Monte-Carlo gradient methods, however, tend to be slow, again, because of high variance in the sampled rewards (for example, if playing an Atari game, in one episode I may get 10,000 points and in the next I may get 0; over the course of any episode there are so many things that could occur that can lead to very different outcomes; it is very noisy).</p>
<h3>Actor-Critic policy gradient methods</h3>
<p><em>Actor-critic policy gradient methods</em> try to get the best of both worlds - they learn a policy explicitly <em>and</em> learn a value function.</p>
<p>We use a <em>critic</em> to estimate the action-value function using a function approximator:</p>
<p><mathjax>$$
Q_w(s,a) \approx Q^{\pi_{\theta}}(s,a)
$$</mathjax></p>
<p>So instead of using the return to estimate the action-value function (which, as just stated, is high in variance), we estimate it directly with the critic. This reduces the variance so training is quicker.</p>
<p>So we have two sets of parameters:</p>
<ul>
<li>the critic: updates action-value function parameters <mathjax>$w$</mathjax></li>
<li>the actor: updates policy parameters <mathjax>$\theta$</mathjax>, in direction suggested by critic</li>
</ul>
<p>So the actor is the one that actually has the policy and is deciding and taking actions, the critic is just evaluating the actor.</p>
<p>Actor-critic algorithms follow an <em>approximate</em> policy gradient:</p>
<p><mathjax>$$
\begin{aligned}
\nabla_{\theta} J(\theta) &amp;\approx \mathbb E_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) Q_w(s,a)] \\
\Delta \theta &amp;= \alpha \nabla_{\theta} \log \pi_{\theta}(s,a) Q_w(s,a)
\end{aligned}
$$</mathjax></p>
<p>So we've replaced the true action value function <mathjax>$Q^{\pi_{\theta}}(s,a)$</mathjax> with our estimate <mathjax>$Q_w(s,a)$</mathjax>.</p>
<p>So the critic is basically just incorporating policy evaluation, i.e. how good is policy <mathjax>$\pi_{\theta}$</mathjax> for current parameters <mathjax>$\theta$</mathjax>? We've already looked at this with Monte-Carlo policy evaluation, Temporal-Difference learning, and <mathjax>$TD(\lambda)$</mathjax>.</p>
<p>For example, we could use a linear value function approximator <mathjax>$Q_w(s,a) = \phi(s,a)^T w$</mathjax> and update the critic's parameters <mathjax>$w$</mathjax> with linear <mathjax>$TD(0)$</mathjax> and update the actor's parameters <mathjax>$\theta$</mathjax> by policy gradient:</p>
<ul>
<li>initialize <mathjax>$s, \theta$</mathjax></li>
<li>sample <mathjax>$a \sim \pi_{\theta}$</mathjax></li>
<li>for each step do:<ul>
<li>sample reward <mathjax>$r = \mathcal R_s^a$</mathjax>; sample transition <mathjax>$s' \sim P_s^a$</mathjax></li>
<li>sample action <mathjax>$a' \sim \pi_{\theta}(s', a')$</mathjax></li>
<li><mathjax>$\delta = r + \gamma Q_w(s',a') - Q_w(s,a)$</mathjax></li>
<li><mathjax>$\theta = \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(s,a) Q_w(s,a)$</mathjax></li>
<li><mathjax>$w \leftarrow w + \beta \delta \phi(s,a)$</mathjax></li>
<li><mathjax>$a \leftarrow a', s \leftarrow s'$</mathjax></li>
</ul>
</li>
</ul>
<p>(note that because we are using TD we don't need to wait until the end of the episode, we can update at each step)</p>
<p>This is similar to policy iteration: we evaluate the policy (like value-based RL), then improve it, except now we're improving it explicitly using its gradient (like policy-based RL).</p>
<p>How can we improve this?</p>
<p>We can reduce variance using a <em>baseline</em>. We subtract a baseline function <mathjax>$B(s)$</mathjax> from the policy gradient. This can reduce variance without changing expectation:</p>
<p><mathjax>$$
\begin{aligned}
\mathbb E_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) B(s)] &amp;= \sum_{s \in \mathcal S} d^{\pi_{\theta}}(s) \sum_a \nabla_{\theta}\pi_{\theta}(s,a) B(s) \\
&amp;= \sum_{s \in \mathcal S} d^{\pi_{\theta}} B(s) \nabla_{\theta} \sum_{a \in \mathcal A} \pi_{\theta}(s,a) \\
&amp;= 0
\end{aligned}
$$</mathjax></p>
<p>So the expectation of this term is 0, so it does not affect the expectation wrt to our action-value function.</p>
<p>We choose a baseline function explicitly to reduce variance (e.g. mean 0, roughly the right scale, etc).</p>
<p>A good baseline is the state value function <mathjax>$B(s) = V^{\pi_{\theta}}(s)$</mathjax>.</p>
<p>So we subtract the state value function from the action-value function. This gives us what is called the <em>advantage function</em> <mathjax>$A^{\pi_{\theta}}(s,a)$</mathjax>, which tells us how much better than usual is it to take action <mathjax>$a$</mathjax>.</p>
<p>So we can rewrite the policy gradient using the advantage function:</p>
<p><mathjax>$$
\begin{aligned}
A^{\pi_{\theta}}(s,a) &amp;= Q^{\pi_{\theta}}(s,a) - V^{\pi_{\theta}}(s) \\
\nabla_{\theta} J(\theta) &amp;= \mathbb E_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) A^{\pi_{\theta}}(s,a)]
\end{aligned}
$$</mathjax></p>
<p>So basically if we take an action that has a better-than-usual result, we update the policy to encourage (towards) the action; if worse-than-usual, we update the policy to discourage (away from) that action.</p>
<p>This can significantly reduce the variance of the policy gradient.</p>
<p>So what we really want is for the critic to estimate the advantage function, e.g. by estimating both <mathjax>$V^{\pi_{\theta}}(s)$</mathjax> and <mathjax>$Q^{\pi_{\theta}}(s)$</mathjax>. So we can use two function approximators with two parameter vectors:</p>
<p><mathjax>$$
\begin{aligned}
V_v(s) &amp;\approx V^{\pi_{\theta}}(s) \\
Q_w(s,a) &amp;\approx Q^{\pi_{\theta}}(s,a) \\
A(s,a) &amp;= Q_w(s,a) - V_v(s)
\end{aligned}
$$</mathjax></p>
<p>And then update both value functions by e.g. TD learning.</p>
<p>However, we can make this a bit simpler.</p>
<p>For the true value function <mathjax>$V^{\pi_{\theta}}(s)$</mathjax>, the TD error <mathjax>$\delta^{\pi_{\theta}}$</mathjax> is:</p>
<p><mathjax>$$
\delta^{\pi_{\theta}} = r + \gamma V^{\pi_{\theta}}(s') - V^{\pi_{\theta}}(s)
$$</mathjax></p>
<p>and it is actually an unbiased estimate of the advantage function:</p>
<p><mathjax>$$
\begin{aligned}
\mathbb E_{\pi_{\theta}} [\delta^{\pi_{\theta}}|s,a] &amp;= \mathbb E_{\pi_{\theta}} [r + \gamma V^{\pi_{\theta}} (s') | s,a] - V^{\pi_{\theta}}(s) \\
&amp;= Q^{\pi_{\theta}}(s,a) - V^{\pi_{\theta}}(s) \\
&amp;= A^{\pi_{\theta}}(s,a)
\end{aligned}
$$</mathjax></p>
<p>Remember that <mathjax>$E_{\pi_{\theta}} [r + \gamma V^{\pi_{\theta}} (s') | s,a]$</mathjax> is the definition of <mathjax>$Q^{\pi_{\theta}}(s,a)$</mathjax>.</p>
<p>So we can just use the TD error the compute the policy gradient:</p>
<p><mathjax>$$
\nabla_{\theta} J(\theta) = \mathbb E_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) \delta^{\pi_{\theta}}]
$$</mathjax></p>
<p>In practice we can use an approximate TD error:</p>
<p><mathjax>$$
\delta_v = r + \gamma V_v(s') - V_v(s)
$$</mathjax></p>
<p>So the critic only needs to learn one set of parameters, <mathjax>$v$</mathjax>.</p>
<p>We already know how to have the critic work on different time-scales (e.g. Monte Carlo requires the full episode, <mathjax>$TD(0)$</mathjax> only looks at one step, and <mathjax>$TD(\lambda)$</mathjax> looks somewhere in between).</p>
<p>We can do the same for the actor; i.e. have it consider the critic's response from multiple time-steps, i.e. the policy gradient can also be estimated at many time-scales.</p>
<p>The Monte-Carlo policy gradient uses error from the complete return, <mathjax>$v_t$</mathjax>:</p>
<p><mathjax>$$
\Delta \theta = \alpha (v_t - V_v(s_t))\nabla_{\theta} \log \pi_{\theta}(s_t,a_t)
$$</mathjax></p>
<p>The actor-critic policy gradient uses the one-step TD error, <mathjax>$r + \gamma V_v(s_{t+1})$</mathjax>:</p>
<p><mathjax>$$
\Delta \theta = \alpha (r + \gamma V_v(s_{t+1}) - V_v(s_t))\nabla_{\theta} \log \pi_{\theta}(s_t,a_t)
$$</mathjax></p>
<p>(remember that the terms in parentheses, e.g. <mathjax>$v_t - V_v(s_t)$</mathjax> and <mathjax>$r + \gamma V_v(s_{t+1}) - V_v(s_t)$</mathjax>, are estimates of the advantage function)</p>
<p>We can combine the policy gradient with eligibility traces to get what we want - to consider the critic's response from multiple time-steps.</p>
<p>Like forward-view <mathjax>$TD(\lambda)$</mathjax> we can mix over time-scales:</p>
<p><mathjax>$$
\Delta \theta = \alpha (v_t^{\lambda} - V_v(s_t))\nabla_{\theta} \log \pi_{\theta}(s_t,a_t)
$$</mathjax></p>
<p>where <mathjax>$v_t^{\lambda} - V_v(s_t)$</mathjax> is a biased estimate of the advantage function.</p>
<p>Like backward-view <mathjax>$TD(\lambda)$</mathjax>, we can also use eligibility traces:</p>
<ul>
<li>by equivalence with <mathjax>$TD(\lambda)$</mathjax>, subsisting <mathjax>$\phi(s) = \nabla_{\theta} \log \pi_{\theta}(s,a)$</mathjax>:</li>
</ul>
<p><mathjax>$$
\begin{aligned}
\delta &amp;= r_{t+1} + \gamma V_v(s_{t+1}) - V_v(s_t) \\
e_{t+1} &amp;= \lambda e_t + \nabla_{\theta} \log \pi_{\theta}(s,a) \\
\Delta \theta &amp;= \alpha \delta e_t
\end{aligned}
$$</mathjax></p>
<p>You can see it is similar to eligibility traces with value-based RL.</p>
<p>This update can be applied online, to incomplete sequences.</p>
<h2>Integrating Learning and Planning</h2>
<h3>Model-based RL</h3>
<p>Previously we learned value functions directly from experience, then we looked at how to learn policies directly from experience. Those methods were all model-free. Here we'll see how we can learn a model directly from experience; i.e. the agent learns the dynamics of the environment (i.e. the state transition function and the reward function).</p>
<p>Then we can use that model to <em>plan</em>; i.e. construct a value function or policy by looking ahead/imagining/simulating what will happen in the future.</p>
<p>The general idea:</p>
<ul>
<li>the agent gains experience by interacting with the real world to learn a model for the world (i.e. an MDP)</li>
<li>the agent uses this model to plan (e.g. solve the learned MDP), generating a value function or a policy</li>
<li>the agent uses this value function or policy to decide how to act</li>
<li>repeat</li>
</ul>
<p>Advantages of model-based RL:</p>
<ul>
<li>can be more compact than learning a value function or policy directly</li>
<li>can efficiently learn a model by supervised learning methods</li>
<li>can reason about model uncertainty</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>first learn a model, the construct a value function: thus, two sources of approximation error</li>
</ul>
<p>Formally, a model <mathjax>$\mathcal M$</mathjax> is a representation of an MDP <mathjax>$(\mathcal S, \mathcal A, \mathcal P, \mathcal R)$</mathjax> parametrized by <mathjax>$\eta$</mathjax>.</p>
<p>We assume the state space <mathjax>$\mathcal S$</mathjax> and action space <mathjax>$\mathcal A$</mathjax> are known (for now - there are forms of model-based RL that also learn the state and action spaces).</p>
<p>So a model <mathjax>$\mathcal M = (\mathcal P_{\eta}, \mathcal R_{\eta})$</mathjax> represents state transitions <mathjax>$\mathcal P_{\eta} \approx \mathcal P$</mathjax> and rewards <mathjax>$\mathcal R_{\eta} \approx \mathcal R$</mathjax>.</p>
<p><mathjax>$$
\begin{aligned}
S_{t+1} &amp;\sim \mathcal P_{\eta}(S_{t+1} | S_t, A_t) \\
R_{t+1} &amp;= \mathcal R_{\eta} (R_{t+1} | S_t, A_t)
\end{aligned}
$$</mathjax></p>
<p>We typically assume conditional independence between state transitions and rewards:</p>
<p><mathjax>$$
\mathbb P [S_{t+1}, R_{t+1} | S_t, A_t] = \mathbb P [S_{t+1} | S_t, A_t] \mathbb P[R_{t+1} | S_t, A_t]
$$</mathjax></p>
<p>So our goal is to estimate a model <mathjax>$M_{\eta}$</mathjax> from experience <mathjax>$(S_1, A_1, R_2, \dots, S_T)$</mathjax>.</p>
<p>This can be framed as a supervised learning problem:</p>
<p><mathjax>$$
\begin{aligned}
S_1, A_1 &amp;\to R_2, S_2 \\
S_2, A_2 &amp;\to R_3, S_3 \\
&amp;\vdots \\
S_{T-1}, A_{T-1} &amp;\to R_T, S_T
\end{aligned}
$$</mathjax></p>
<p>Learning <mathjax>$s, a \to r$</mathjax> is a regression problem, and learning <mathjax>$s, a \to s'$</mathjax> is a density estimation problem (e.g. we learn a distribution over states).</p>
<p>So we pick a loss function, e.g. mean-squared error, KL divergence, and the find the parameters <mathjax>$\eta$</mathjax> that minimize empirical loss.</p>
<p>Some example models:</p>
<ul>
<li>table lookup model</li>
<li>linear expectation model</li>
<li>linear gaussian model</li>
<li>gaussian process model</li>
<li>deep belief network model</li>
<li>etc</li>
</ul>
<p>As a really simple example, consider a table lookup model.</p>
<p>We can just model <mathjax>$\hat \mathcal P$</mathjax> with counts, i.e. each time we end up in state <mathjax>$s'$</mathjax> from state-action pair <mathjax>$s,a$</mathjax>, just increment the count of <mathjax>$s,a \to s'$</mathjax> by 1. Then to use our model we just convert these counts into a probability distribution.</p>
<p>Similarly, to model rewards we can just take the average reward received for state-action pair <mathjax>$s,a$</mathjax>.</p>
<p>Alternatively, we can just record each experience tuple <mathjax>$(S_t, A_t, R_{t+1}, S_{t+1})$</mathjax> at time <mathjax>$t$</mathjax> and then, to sample the model, we just randomly pick a tuple matching <mathjax>$(s,a,\cdot,\cdot)$</mathjax>.</p>
<h3>Planning with a model</h3>
<p>Given a model <mathjax>$\mathcal M_{\eta} = (\mathcal P_{\eta}, \mathcal R_{\eta})$</mathjax>, to plan we just solve the MDP <mathjax>$(\mathcal S, \mathcal A, \mathcal P_{\eta}, \mathcal R_{\eta})$</mathjax> using a planning algorithm such as value iteration, policy iteration, tree search, etc.</p>
<p>One such algorithm is <em>sample-based planning</em>, which is a simple but powerful approach. We use the model <em>only</em> to generate samples, i.e. we sample experience from the model:</p>
<p><mathjax>$$
\begin{aligned}
S_{t+1} &amp;\sim \mathcal P_{\eta}(S_{t+1} | S_t, A_t) \\
R_{t+1} &amp;= \mathcal R_{\eta}(R_{t+1} | S_t, A_t)
\end{aligned}
$$</mathjax></p>
<p>Then we apply model-free RL to the samples, e.g. Monte-Carlo control, Sarsa, Q-learning, etc.</p>
<p>Sample-based planning methods are often more efficient because, by sampling, we consider experiences that are more likely to happen.</p>
<p>This is basically the agent learning from the "imagined" or "simulated" world its learned from experience.</p>
<p>It then acts based on its model-free learning and generates more experiences which further refine the model, which it then samples to learn a better value function or policy, which it then acts from, and so on.</p>
<p>This can be extended to incorporate uncertainties with Bayesian model-based reinforcement learning (not covered here).</p>
<p>The performance of model-based RL is limited to the optimal policy for the approximate MDP; i.e. it's only as good as the estimated model. So we have to consider the case where the model is imperfect/inaccurate, i.e. <mathjax>$(\mathcal P_{\eta}, \mathcal R_{\eta}) \neq (\mathcal P, \mathcal R)$</mathjax>, where a suboptimal policy will result.</p>
<p>There are two solutions:</p>
<ul>
<li>when the model is wrong, use model-free RL</li>
<li>or reason explicitly about the model's uncertainty (e.g. Bayesian methods)</li>
</ul>
<h3>Integrated architectures</h3>
<p>So we can generate experience in two ways:</p>
<ul>
<li>real experience, i.e. sampled from the environment (the true MDP)</li>
<li>simulated experience, i.e. sampled from the model (the approximate MDP)</li>
</ul>
<p>Model-free RL has no model and <em>learns</em> a value function (and/or policy) from <em>real</em> experience.</p>
<p>Model-based RL (here, using sample-based planning), learns a model from real experience and <em>plans</em> a value function (and/or) policy from <em>simulated</em> experience.</p>
<p>We can combine model-free RL and model-based RL with the <em>Dyna</em> architecture to make the most of both real and simulated experience:</p>
<ul>
<li>learn a model from real experience</li>
<li><em>learn and plan</em> value function (and/or policy) from <em>real and simulated</em> experience</li>
</ul>
<p>The canonical Dyna algorithm is the <em>Dyna-Q</em> algorithm:</p>
<ul>
<li>initialize <mathjax>$Q(s,a)$</mathjax> and <mathjax>$\text{Model}(s,a)$</mathjax> for all <mathjax>$s \in \mathcal S$</mathjax> and <mathjax>$a \in \mathcal A(s)$</mathjax></li>
<li>do forever:<ul>
<li><mathjax>$S \leftarrow$</mathjax> current (nonterminal) state</li>
<li><mathjax>$A \leftarrow \epsilon\text{-greedy}(S,Q)$</mathjax></li>
<li>execute action <mathjax>$A$</mathjax>, observe resulting reward <mathjax>$R$</mathjax> and state <mathjax>$S'$</mathjax></li>
<li><mathjax>$Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \max_a Q(S', a) - Q(S,A)]$</mathjax></li>
<li><mathjax>$\text{Model}(S,A) \leftarrow R, S'$</mathjax> (assuming deterministic environment)</li>
<li>repeat <mathjax>$n$</mathjax> times:<ul>
<li><mathjax>$S \leftarrow$</mathjax> random previously observed state</li>
<li><mathjax>$A \leftarrow$</mathjax> random action previously taken in <mathjax>$S$</mathjax></li>
<li><mathjax>$R,S' \leftarrow \text{Model}(S,A)$</mathjax></li>
<li><mathjax>$Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \max_a Q(S', a) - Q(S,A)]$</mathjax></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>If you train a Dyna-Q agent in one environment and then change the environment on it, its model suddenly becomes inaccurate, but, if exploration is encouraged (there will be more on this later; but one simple variation is Dyna-Q+ in which a little bonus is assigned to unexplored states) then it can reorient itself fairly quickly.</p>
<h3>Simulation-based search</h3>
<p>This part focuses on the planning part; i.e. expanding on sample-based planning.</p>
<p>The key ideas are sampling and forward search.</p>
<p>Forward search algorithms select the best action by lookahead. They build a search tree with the current state <mathjax>$s_t$</mathjax> at the root and then use the model of the MDP to look ahead. So we don't need to solve the whole MDP, just the sub-MDP starting from the current state.</p>
<p><em>Simulation-based search</em> is a forward search paradigm that uses sample-based planning. We simulate episodes of experience from the current state with the model, i.e.:</p>
<p><mathjax>$$
\{s_t^k, A_t^k, R_{t+1}^k, \dots, S_T^k\}_{k=1}^K \sim \mathcal M_v
$$</mathjax></p>
<p>then apply model-free RL to the simulated episodes, e.g.:</p>
<ul>
<li>if we use Monte-Carlo control, we have <em>Monte-Carlo search</em>.</li>
<li>if we use Sarsa, we have <em>TD search</em>.</li>
</ul>
<h4>Simple Monte-Carlo Search</h4>
<ul>
<li>given a model <mathjax>$\mathcal M_v$</mathjax> and a <em>simulation policy</em> <mathjax>$\pi$</mathjax></li>
<li>for each action <mathjax>$a \in \mathcal A$</mathjax><ul>
<li>simulate <mathjax>$K$</mathjax> episodes from current (real) state <mathjax>$s_t$</mathjax>:</li>
</ul>
</li>
</ul>
<p><mathjax>$$
\{s_t, A_t, R_{t+1}^k, S_{t+1}^k, A_{t+1}^k ,\dots, S_T^k\}_{k=1}^K \sim \mathcal M_v, \pi
$$</mathjax></p>
<ul>
<li>evaluate actions by mean return (<em>Monte-Carlo evaluation</em>):</li>
</ul>
<p><mathjax>$$
Q(s_t, a) = \frac{1}{K} \sum_{k=1}^K G_t \to^P q_{\pi} (s_t, a)
$$</mathjax></p>
<ul>
<li>select current (real) action with maximum value:</li>
</ul>
<p><mathjax>$$
a_t = \argmax_{a \in \mathcal A} Q(s_t, a)
$$</mathjax></p>
<p>(this approach is "simple" because we aren't improving the simulation policy <mathjax>$\pi$</mathjax> as we go)</p>
<h4>Monte-Carlo Tree Search</h4>
<p>At time of writing, this is a state-of-the-art search method.</p>
<ul>
<li>given a model <mathjax>$\mathcal M_v$</mathjax></li>
<li>simulate <mathjax>$K$</mathjax> episodes from current state <mathjax>$s_t$</mathjax> using current simulation policy <mathjax>$\pi$</mathjax></li>
</ul>
<p><mathjax>$$
\{s_t, A_t, R_{t+1}^k, S_{t+1}^k, A_{t+1}^k ,\dots, S_T^k\}_{k=1}^K \sim \mathcal M_v, \pi
$$</mathjax></p>
<ul>
<li>build a search tree containing visited states and actions</li>
<li>evaluate states <mathjax>$Q(s,a)$</mathjax> by mean return of episodes from <mathjax>$s,a$</mathjax> (Monte Carlo evaluation):</li>
</ul>
<p><mathjax>$$
Q(s,a) = \frac{1}{N(s,a)} \sum_{k=1}^K \sum_{u=t}^T \mathbb 1 (S_u, A_u = s,a) G_u \to^P q_{\pi}(s,a)
$$</mathjax></p>
<p>(where <mathjax>$N(s,a)$</mathjax> is the number of times the pair <mathjax>$s,a$</mathjax> was encountered)</p>
<ul>
<li>after search is finished, select current (real) action with maximum value in the search tree:</li>
</ul>
<p><mathjax>$$
a_t = \argmax_{a \in \mathcal A} Q(s_t,a)
$$</mathjax></p>
<p>So now instead of just evaluating the root actions, as we did with simple Monte-Carlo search, we're evaluating <em>every</em> state-action pair we visit.</p>
<p>With MCTS, the simulation policy <mathjax>$\pi$</mathjax> improves (again, we did not see this with simple Monte-Carlo search).</p>
<p>Each simulation consists of two phases: <em>in-tree</em> and <em>out-of-tree</em>:</p>
<ul>
<li><em>tree policy</em> (improves): pick actions to maximize <mathjax>$Q(S,A)$</mathjax>. We can do this while we're "within" the tree</li>
<li><em>default policy</em> (fixed): pick actions randomly. We use this once we've gone "beyond" the tree (where we don't have any information)</li>
</ul>
<p>Repeat for each simulation:</p>
<ul>
<li>evaluate states <mathjax>$Q(S,A)$</mathjax> by Monte Carlo evaluation</li>
<li>improve tree policy, e.g. by <mathjax>$\epsilon\text{-greedy}(Q)$</mathjax></li>
</ul>
<p>This is basically Monte-Carlo control applied to simulated episodes of experience, starting from the current (i.e. root) state.</p>
<p>This converges on the optimal search tree, <mathjax>$Q(S,A) \to q_*(S,A)$</mathjax></p>
<p>For example: in a game of Go, white vs black, the agent is playing as black.</p>
<p>Starting at the current state, it rolls out (sample) one episode to a terminal state and see that it wins (<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf">images from here</a>):</p>
<figure><img alt="" src="../assets/mcts_01.png" /><figcaption></figcaption>
</figure>
<p>So according to our tree policy, we select the next state in that branch (since it's the highest valued at the moment), and roll out another episode starting from there. We get to a terminal state and see that it loses:</p>
<figure><img alt="" src="../assets/mcts_02.png" /><figcaption></figcaption>
</figure>
<p>So from our tree policy we pick another state and roll an episode out from there; at the terminal state it wins:</p>
<figure><img alt="" src="../assets/mcts_03.png" /><figcaption></figcaption>
</figure>
<p>So then our tree policy directs us to the next state in that branch and we roll out an episode from there; at the terminal state it loses:</p>
<figure><img alt="" src="../assets/mcts_04.png" /><figcaption></figcaption>
</figure>
<p>So then from our tree policy we pick another state and roll out an episode from there; at the terminal state it wins:</p>
<figure><img alt="" src="../assets/mcts_05.png" /><figcaption></figcaption>
</figure>
<p>and so on.</p>
<p>(note that when I say our "from our tree policy we pick another state and roll an episode out from there", technically we are always restarting from the root, i.e. rolling the episode out from the root, but our tree policy directs us to choose actions in such a way that we end up on what is the most promising node)</p>
<p>Advantages of MCTS:</p>
<ul>
<li>highly selective best-first search: we search through the space is by what is most promising, so we are quite efficient.</li>
<li>evaluates states dynamically (unlike e.g. DP): we don't have to worry about the entire state space, only the states that are relevant to us now</li>
<li>uses sampling to break curse of dimensionality: we don't have to consider everything that could happen in the environment</li>
<li>works for "black-box" models: we only need samples</li>
<li>computationally efficient, anytime, parallelizable</li>
</ul>
<h3>Temporal-Difference search</h3>
<p>MCTS is just one example of a family of very effective search algorithms. The key ideas are to use forward search and sampling.</p>
<p>For example, we can use TD instead of MC, i.e. we can use bootstrapping. So whereas MC tree search applies MC control to the sub-MDP from the current state, TD search applies Sarsa to the sub-MDP from the current state.</p>
<p>For simulation-based search, as we saw in model-free reinforcement learning, bootstrapping is helpful:</p>
<ul>
<li>TD search reduces variance but increases bias</li>
<li>TD search is usually more efficient than MC search</li>
<li><mathjax>$TD(\lambda)$</mathjax> search can be much more efficient than MC search</li>
</ul>
<p>TD search:</p>
<ul>
<li>simulate episodes from current (real) state <mathjax>$s_t$</mathjax></li>
<li>estimate action-value function <mathjax>$Q(s,a)$</mathjax></li>
<li>for each step of simulation, update action-values by Sarsa:</li>
</ul>
<p><mathjax>$$
\Delta Q(S,A) = \alpha (R + \gamma Q(S', A') - Q(S,A))
$$</mathjax></p>
<ul>
<li>select actions based on action-values <mathjax>$Q(s,a)$</mathjax>, e.g. <mathjax>$\epsilon$</mathjax>-greedy</li>
<li>you can also use function approximation for <mathjax>$Q$</mathjax></li>
</ul>
<p>This is an especially nice approach for domains where there are many ways to arrive at a given state. With bootstrapping, you'll already have some knowledge about a state if you re-visit it through another trajectory.</p>
<h3>Dyna-2</h3>
<p><em>Dyna-2</em> incorporates Dyna (using both real and simulated experience to learn and plan our value function and/or policy) with forward search. The agent stores two set of feature weights (i.e. two value functions):</p>
<ul>
<li>long-term memory</li>
<li>short-term (working) memory</li>
</ul>
<p>Long-term memory is updated from real experience using TD learning (this can be thought of as general domain knowledge that applies to any episode).</p>
<p>Short-term memory is updated from simulated experience using TD search (this can be thought of as specific local knowledge about the current situation).</p>
<p>The value function is the sum of long and short-term memories.</p>
<h2>Exploration and Exploitation</h2>
<p>How does a RL agent balance exploitation and exploration? That is, how do we balance making the best decision given what we know (exploitation) and gathering more information (exploration)?</p>
<p>The best long-term strategy may involve sacrificing some short-term gains by exploring.</p>
<p>Thus far we have just used the <mathjax>$\epsilon$</mathjax>-greedy method, but there are better ways.</p>
<p>Broadly, there are three approaches:</p>
<ul>
<li>random exploration<ul>
<li>explore random actions (e.g. <mathjax>$\epsilon$</mathjax>-greedy, softmax)</li>
</ul>
</li>
<li>optimism in the face of uncertainty<ul>
<li>estimate uncertainty on value</li>
<li>prefer to explore states/actions with highest uncertainty</li>
</ul>
</li>
<li>information state space<ul>
<li>consider agent's information as part of its state</li>
<li>lookahead to see how information helps reward</li>
<li>this is the most "correct" way but also the most computationally difficult</li>
</ul>
</li>
</ul>
<p>We can explore two different spaces:</p>
<ul>
<li>state-action exploration<ul>
<li>systematically explore state-space/action-space</li>
<li>e.g. pick different action <mathjax>$A$</mathjax> each time <mathjax>$S$</mathjax> is visited</li>
</ul>
</li>
<li>parameter exploration<ul>
<li>parametrize policy <mathjax>$\pi(A|S,u)$</mathjax></li>
<li>e.g. pick different parameters and try for awhile</li>
<li>advantage: consistent exploration</li>
<li>disadvantage: doesn't know about state/action space</li>
</ul>
</li>
</ul>
<p>We'll focus on state-action exploration.</p>
<h3>Multi-armed bandits</h3>
<p>A very simplified version of the reinforcement learning problem is the <em>multi-armed bandit</em> problem, which is useful for introducing concepts around exploration vs exploitation.</p>
<p>A multi-armed bandit is a tuple <mathjax>$(\mathcal A, \mathcal R)$</mathjax> (we don't have any states anymore).</p>
<p><mathjax>$\mathcal A$</mathjax> is a known set of actions (also called "arms").</p>
<p><mathjax>$\mathcal R^a(r) = \mathbb P[R = r|A=a]$</mathjax> is an unknown probability distribution over rewards.</p>
<p>At each step <mathjax>$t$</mathjax> the agent selects an action <mathjax>$A_t \in \mathcal A$</mathjax>. The environment then generates a reward <mathjax>$R_t \sim R^{A_t}$</mathjax>. The agent's goal is to maximize the cumulative reward <mathjax>$\sum_{\tau=1}^t R_{\tau}$</mathjax>.</p>
<p>The <em>action-value</em> is the mean reward for an action <mathjax>$a$</mathjax>:</p>
<p><mathjax>$$
q(a) = \mathbb E[R|A=a]
$$</mathjax></p>
<p>The <em>optimal value</em> <mathjax>$v_*$</mathjax> is:</p>
<p><mathjax>$$
v_* = q(a^*) = \max_{a \in \mathcal A} q(a)
$$</mathjax></p>
<p>The <em>regret</em> is the opportunity loss for one step (i.e. the difference between the best we could have gotten and what we actually did get):</p>
<p><mathjax>$$
l_t \ mathbb E[v_* - q(A_t)]
$$</mathjax></p>
<p>The <em>total regret</em> is the total opportunity loss:</p>
<p><mathjax>$$
L_t = \mathbb E[ \sum_{\tau=1}^t v_* - q(A_{\tau})]
$$</mathjax></p>
<p>So maximizing cumulative reward is equivalent to minimizing total regret.</p>
<p>The <em>count</em> <mathjax>$N_t(a)$</mathjax> is the expected number of selections for action <mathjax>$a$</mathjax>.</p>
<p>The <em>gap</em> <mathjax>$\Delta_a$</mathjax> is the difference in value between action <mathjax>$a$</mathjax> and optimal action <mathjax>$a^*$</mathjax>, i.e. <mathjax>$\Delta_a = v_* - q(a)$</mathjax>.</p>
<p>Regret is a function of gaps and the counts:</p>
<p><mathjax>$$
\begin{aligned}
L_t &amp;= \mathbb E [\sum_{\tau=1}^t v_* - q(A_{\tau})] \\
&amp;= \sum_{a \in \mathcal A} \mathbb E [N_t(a)] (v_* - q(a)) \\
&amp;= \sum_{a \in \mathcal A} \mathbb E [N_t(a)] \Delta_a
\end{aligned}
$$</mathjax></p>
<p>So basically, we want to have small counts of large gaps (i.e. we want to take few actions that are much worse than the optimal action).</p>
<p>However, we don't know what these gaps are.</p>
<figure><img alt="" src="../assets/total_regret.png" /><figcaption></figcaption>
</figure>
<p>With strategies we've seen so far, e.g. greedy and <mathjax>$\epsilon$</mathjax>-greedy, total regret increases linearly - with the greedy strategy, where we <em>never</em> explore, we may just get stuck with a suboptimal action, so total regret keeps accumulating; similarly with <mathjax>$\epsilon$</mathjax>-greedy, where we explore <em>forever</em>, we will always sometimes randomly choose an action which is most likely to be suboptimal (not much of a chance we randomly pick the optimal action). Ideally, we want to have a sublinear total regret (one that eventually stops increasing).</p>
<h4>Random exploration</h4>
<p>First let's go more into the greedy algorithm:</p>
<ul>
<li>we consider algorithms that estimate <mathjax>$Q_t(a) \approx q(a)$</mathjax>.</li>
<li>we estimate the value of each action by Monte-Carlo evaluation, i.e.:</li>
</ul>
<p><mathjax>$$
Q_t(a) = \frac{1}{N_t(a)} \sum_{t=1}^T \mathbb 1 (A_t = a) R_t
$$</mathjax></p>
<ul>
<li>the <em>greedy</em> algorithm selects the action with the highest value, i.e.:</li>
</ul>
<p><mathjax>$$
A_t = \argmax_{a \in \mathcal A} Q_t(a)
$$</mathjax></p>
<p>As mentioned before, this can lock onto a suboptimal action forever, so it has linear total regret.</p>
<p>An enhancement on the greedy algorithm is the greedy algorithm with <em>optimistic initialization</em>, in which we initialize all values to the maximum possible, i.e. <mathjax>$Q(a) = r_{\text{max}}$</mathjax>; that is we assume all actions are really good (over time these assumptions get attenuated with the actual observations using Monte-Carlo evaluation), then we act greedily, i.e. <mathjax>$A_t = \argmax_{a \in \mathcal A} Q_t(a)$</mathjax>.</p>
<p>Note that you can also initialize the counts <mathjax>$N_t(a)$</mathjax> in addition to the values - this is how we can assert our confidence in these optimistic initializations (e.g. if we set <mathjax>$N_t(a)=1$</mathjax> then we aren't very confident, but if we set it to <mathjax>$N_t(a)=1000$</mathjax> then we are much more confident).</p>
<p>This optimistic initialization encourages exploration of unknown values, which is great. But a few unlucky samples can also cause the algorithm to lock onto a suboptimal action (though in practice, this method can work very well). So this has linear total regret as well.</p>
<p>Note that by assigning more confidence to the optimistic prior, you can withstand more unlucky samples and the algorithm will perform better.</p>
<p>Now let's consider the <mathjax>$\epsilon$</mathjax>-greedy algorithm. As mentioned before, it has linear total regret. But we can easily make this sublinear by decaying <mathjax>$\epsilon$</mathjax>. For example, consider the following schedule:</p>
<p><mathjax>$$
\begin{aligned}
c &amp;&gt; 0 \\
d &amp;= \min_{a| \Delta_a &gt; 0} \Delta_a \\
\epsilon_t &amp;= \min \{1, \frac{c|\mathcal A|}{d^2t}\}
\end{aligned}
$$</mathjax></p>
<p>This has logarithmic asymptotic total regret.</p>
<p>In practice, that schedule is impossible because it requires knowledge of the gaps (i.e. of <mathjax>$v_*$</mathjax>), which we don't have.</p>
<p>So ideally we want to find an algorithm with sublinear regret for any multi-armed bandit which does not require knowledge of <mathjax>$\mathcal R$</mathjax>.</p>
<p>There is a lower bound to total regret (i.e. no algorithm can possibly do better than this lower bound), so we want to get as close to that lower bound as possible.</p>
<p>The difficulty of a multi-armed bandit problem is in large part determined by the similarity between the optimal arm/action and the other arms/actions. That is, if suboptimal arms have a reward distribution very similar to the optimal one, it will take a long time to discern the optimal one (contrast to an easier situation where one arm is "obviously" optimal in that it consistently gives higher reward than all the other arms).</p>
<p>We can describe this formally with the gap <mathjax>$\Delta_a$</mathjax> and the similarity in distributions <mathjax>$KL(\mathcal R^a || \mathcal R^{a^*})$</mathjax>.</p>
<p>Theorem (Lai and Robbins):</p>
<p>Asymptotic total regret is at least logarithmic in number of steps:</p>
<p><mathjax>$$
\lim_{t \to \infty} L_t \geq \log t \sum_{a| \Delta_a &gt; 0} \frac{\Delta_a}{KL(\mathcal R^a || \mathcal R^{a^*})}.
$$</mathjax></p>
<p>Intuitively, the larger the gap <mathjax>$\Delta_a$</mathjax>, the larger regret will occur, and is inversely proportional to the difference (KL divergence) in distributions (the less different they are, the more regret).</p>
<h4>Optimism in the face of uncertainty</h4>
<p>The principle of <em>optimism in the face of uncertainty</em> states that we should not the action that is certain to be best, but the action that has the potential to be best.</p>
<p>Consider the following example with three arms/actions:</p>
<figure><img alt="" src="../assets/optimism.png" /><figcaption></figcaption>
</figure>
<p>We are most certain about <mathjax>$a_3$</mathjax> and least certain about <mathjax>$a_1$</mathjax>. The principle of optimism in the face of uncertainty dictates that we should try <mathjax>$a_1$</mathjax> even though our current mean for it is less than <mathjax>$a_3$</mathjax> since it has the potential to yield higher rewards (i.e. it's possible that its mean is actually higher than <mathjax>$a_3$</mathjax>'s, we just don't know enough to be certain yet; this will be formalized in a bit). If the mean of <mathjax>$a_1$</mathjax> is actually lower then <mathjax>$a_3$</mathjax>, eventually we'll realize this as we start to be more and more certain about its value through more experience. Then we'd switch back to <mathjax>$a_3$</mathjax>.</p>
<p>More formally (for "potential to yield higher rewards"), we estimate an upper confidence <mathjax>$U_t(a)$</mathjax> for each action value such that <mathjax>$q(a) \leq Q_t(a) + U_t(a)$</mathjax> with high probability. This depends on the number of times <mathjax>$N(a)$</mathjax> has been selected:</p>
<ul>
<li>small <mathjax>$N_t(a) \to$</mathjax> large <mathjax>$U_t(a)$</mathjax> (estimated value is uncertain)</li>
<li>large <mathjax>$N_t(a) \to$</mathjax> small <mathjax>$U_t(a)$</mathjax> (estimated value is accurate)</li>
</ul>
<p>Then we select the action maximizing the <em>upper confidence bound</em> (UCB):</p>
<p><mathjax>$$
A_t = \argmax_{a \ in \mathcal A} Q_t(a) + U_t(a)
$$</mathjax></p>
<p>In statistics, there is a theorem known as <em>Hoeffding's Inequality</em>:</p>
<p>Let <mathjax>$X_1, \dots, X_t$</mathjax> be iid random variables in <mathjax>$[0,1]$</mathjax>, and let <mathjax>$\bar X_t = \frac{1}{\tau} \sum_{\tau=1}^t X_{\tau}$</mathjax> be the sample (empirical) mean. Then:</p>
<p><mathjax>$$
\mathbb P[\mathbb E[X] &gt; \bar X_t + u] \leq e^{-2tu^2}
$$</mathjax></p>
<p>i.e. the probability that we're wrong in our estimate of the empirical mean by the amount <mathjax>$u$</mathjax> is less than or equal to <mathjax>$e^{-2tu^2}$</mathjax>; this is true for <em>any</em> distribution.</p>
<p>We can apply Hoeffding's Inequality to the rewards of the bandit conditioned on selecting action <mathjax>$a$</mathjax>, i.e.:</p>
<p><mathjax>$$
\mathbb P [q(a) &gt; Q_t(a) + U_t(a)] \leq e^{-2N_t(a)U_t(a)^2}
$$</mathjax></p>
<p>So we can pick a probability <mathjax>$p$</mathjax> that the true value exceeds the UCB and then solve for <mathjax>$U_t(a)$</mathjax>:</p>
<p><mathjax>$$
\begin{aligned}
e^{-2N_t(a)U_t(a)^2} &amp;= p \\
U_t(a) &amp;= \sqrt{\frac{-\log p}{2 N_t(a)}}
\end{aligned}
$$</mathjax></p>
<p>For example, we can set <mathjax>$p=0.05$</mathjax> to get the 95% UCB.</p>
<p>We can reduce <mathjax>$p$</mathjax> as we observe reward, e.g. <mathjax>$p = t^{-4}$</mathjax>:</p>
<p><mathjax>$$
U_t(a) = \sqrt{{2 \log t}{N_t(a)}}
$$</mathjax></p>
<p>This ensures we select the optimal action as <mathjax>$t \to \infty$</mathjax>.</p>
<p>This leads to the <em>UCB1 algorithm</em>:</p>
<p><mathjax>$$
A_t = \argmax_{a \ in \mathcal A} Q_t(a) + \sqrt{{2 \log t}{N_t(a)}}
$$</mathjax></p>
<p>This achieves a logarithmic asymptotic total regret.</p>
<p>This was the frequentist approach to multi-armed bandits; we made no assumptions about the distributions of the bandits.</p>
<h5>Bayesian bandits</h5>
<p>Alternatively, we can use a Bayesian approach (<em>Bayesian bandits</em>) where we exploit prior knowledge about rewards, <mathjax>$p[\mathcal R^a]$</mathjax>.</p>
<p>Consider a distribution <mathjax>$p[Q | w]$</mathjax> over an action-value function with parameter <mathjax>$w$</mathjax>, e.g. we could assume the <mathjax>$Q$</mathjax> functions are independent Gaussians: <mathjax>$w = [\mu_1, \sigma_1^2, \dots, \mu_k, \sigma_k^2]$</mathjax> for <mathjax>$a \in [1,k]$</mathjax>.</p>
<p>Bayesian methods compute a posterior distribution over <mathjax>$w$</mathjax>, <mathjax>$p[w|R_1, \dots, R_t]$</mathjax>. Then we use this posterior to guide exploration, e.g. through upper confidence bounds or probability matching. If the prior knowledge is accurate, we get better performance.</p>
<p>So basically we describe <mathjax>$Q$</mathjax> as a distribution which we parameterize with <mathjax>$w$</mathjax>.</p>
<p>In more detail, the upper confidence bound method:</p>
<table>
<thead>
<tr>
<th>- we use Bayes law to compute posterior $p[w</th>
<th>R_1, \dots, R_{t-1}]$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>- then we estimate an upper confidence from the posterior, e.g. <mathjax>$U_t(a) = c \sigma(a)$</mathjax>, where <mathjax>$\sigma(a)$</mathjax> is a standard deviation of $p(Q(a)</td>
<td>w)<mathjax>$, i.e. the upper confidence bound could be $</mathjax>c$ standard deviations from the mean</td>
<td></td>
<td></td>
</tr>
<tr>
<td>- then we pick the action that maximizes <mathjax>$Q_t(a) + c \sigma(a)$</mathjax></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>So it's quite similar to UCB1, we just incorporate priors.</p>
<p>Alternative to the UCB method, we can use <em>probability matching</em>, where we select an action <mathjax>$a$</mathjax> according to the probability that <mathjax>$a$</mathjax> is the optimal action:</p>
<p><mathjax>$$
\pi(a) = \mathbb p[Q(a) = \max_{a'} Q(a') | R_1, \dots, R_{t-1}]
$$</mathjax></p>
<p>Probability matching is optimistic in the face of uncertainty (uncertain action shave a higher probability of being max). But it can be difficult to compute <mathjax>$\pi(a)$</mathjax> analytically from the posterior.</p>
<p>To do this, we can use <em>Thompson sampling</em>, which is sample-based probability matching:</p>
<p><mathjax>$$
\pi(a) = \mathbb E[\mathbb 1(Q(a) = \max_{a'} Q(a')) | R_1, \dots, R_{t-1}]
$$</mathjax></p>
<p>We use Bayes law to compute the posterior distribution <mathjax>$p_q(Q|R_1, \dots, R_{t-1})$</mathjax>. Then we sample an action-value, i.e. a single value for <mathjax>$Q(a)$</mathjax>, from the posterior. Then we select the action associated with the maximum sample, <mathjax>$A_t = \argmax_{a \in \mathcal A} Q(a)$</mathjax>.</p>
<p>For Bernoulli bandits, this achieves the Lai and Robbins lower bound on regret (i.e. it is optimal).</p>
<p>Note that "optimism in the face of uncertainty" won't work where the action-space is infinite (it will forever explore and never end up exploiting anything) or when it is expensive to explore (this isn't very "safe" exploration, e.g. if you have an expensive robot, you don't want it trying <em>everything</em>).</p>
<h4>Information state space</h4>
<p>With exploration we gain information. If we could quantify the value of that information, we could compute the exploration-exploitation trade off perfectly (i.e. the long-term reward after getting information minus the immediate reward of exploitation).</p>
<p>Information gain is higher in uncertain situations - for instance, if you already know everything about a situation, there's no information to be gained. So we want to explore uncertain situations more in the optimal way.</p>
<p>We can introduce an <em>information state</em> <mathjax>$\tilde {\mathcal S}$</mathjax> summarizing all information accumulated so far.</p>
<p>Each action <mathjax>$A$</mathjax> causes a transition to a new information state <mathjax>$\tilde {\mathcal S}'$</mathjax> (by adding information) with probability <mathjax>$\tilde {\mathcal P}_{\tilde {\mathcal S}, \tilde {\mathcal S}'}^A$</mathjax>.</p>
<p>So now we have an MDP <mathjax>$\tilde {\mathcal M}$</mathjax> in the augmented information state space:</p>
<p><mathjax>$$
\tilde {\mathcal M} = (\tilde {\mathcal S}, \mathcal A, \tilde {\mathcal P}, \mathcal R, \gamma)
$$</mathjax></p>
<p>For example, in a Bernoulli bandit (i.e. where there are only two outcomes, 0 or 1), we may have an information state <mathjax>$\tilde {\mathcal S} = (\alpha, \beta)$</mathjax> where <mathjax>$\alpha_a$</mathjax> counts the pulls of arm <mathjax>$a$</mathjax> where the reward was 0 and <mathjax>$\beta_a$</mathjax> counts the pulls of arm <mathjax>$a$</mathjax> where the reward as 1.</p>
<p>Now we have an infinite MDP, since there are infinitely many information states. We can solve this MDP with reinforcement learning, e.g. model-free reinforcement learning or Bayesian model-based reinforcement learning. The latter approach is known as <em>Bayes-adaptive</em> RL, where we characterize our information state as a posterior distribution.</p>
<p>For example, with Bernoulli bandits:</p>
<ul>
<li>start with a <mathjax>$\text{Beta}(\alpha_a, \beta_a)$</mathjax> prior over reward function <mathjax>$\mathcal R^a$</mathjax></li>
<li>each time action <mathjax>$a$</mathjax> is selected, update posterior for <mathjax>$\mathcal R^a$</mathjax>:<ul>
<li><mathjax>$\text{Beta}(\alpha_a + 1, \beta_a)$</mathjax> if reward is 0</li>
<li><mathjax>$\text{Beta}(\alpha_a, \beta_a + 1)$</mathjax> if reward is 1</li>
</ul>
</li>
<li>this defines transition function <mathjax>$\tilde {\mathcal P}$</mathjax> for the Bayes-adaptive MDP</li>
<li>each information state <mathjax>$(\alpha, \beta)$</mathjax> corresponds to a model <mathjax>$\text{Beta}(\alpha, \beta)$</mathjax></li>
<li>each state transition corresponds to a Bayesian model update</li>
<li>solving the Bayes-adaptive MDP takes account of the value of information because the effect of new information is factored into the model update</li>
</ul>
<p>The Bernoulli case of the Bayes-adaptive MDP can be solved by dynamic programming; the solution is known as the <em>Gittins index</em>.</p>
<p>Exactly solving a Bayes-adaptive MDP is typically intractable, but methods like large-scale (large state space) planning methods (e.g. Monte-Carlo tree search) can be applied.</p>
<h3>Contextual bandits</h3>
<p>So far we've introduced state as only information state but we can bring in our previous notions of state as well.</p>
<p>A <em>contextual bandit</em> is a tuple <mathjax>$(\mathcal A, \mathcal S, \mathcal R)$</mathjax>.</p>
<p>This is just like multi-arm bandits except that we've reintroduced states <mathjax>$\mathcal S$</mathjax>. <mathjax>$\mathcal S = \mathbb P[S]$</mathjax> is an unknown distribution over states (also called "<em>contexts</em>").</p>
<p><mathjax>$\mathcal R_s^a(r) = \mathbb P[R=r |S=s, A=a]$</mathjax> is an unknown probability distribution over rewards.</p>
<p>At each step <mathjax>$t$</mathjax>:</p>
<ul>
<li>environment generates state <mathjax>$S_t \sim \mathcal S$</mathjax></li>
<li>agent selects action <mathjax>$A_t \in \mathcal A$</mathjax></li>
<li>environment generates reward <mathjax>$R_t \sim \mathcal R_{S_t}^{A_t}$</mathjax></li>
</ul>
<p>The goal is to maximize cumulative reward <mathjax>$\sum_{\tau=1}^t R_{\tau}$</mathjax>.</p>
<p>(not covered in lecture)</p>
<h3>Extending to MDPs</h3>
<p>The UCB approach can be generalized to full MDPs to give a model-free RL algorithm, i.e. we can maximize the upper confidence bound on the action-value function:</p>
<p><mathjax>$$
A_t = \argmax_{a \in \mathcal A} A(S_t, a) + U(S_t, a)
$$</mathjax></p>
<p>One thing worth noting is that this ignores that, in MDPs, the policy will likely improve (if we're doing control with the MDP) and the <mathjax>$Q$</mathjax> values will get better. So the uncertainty should take into account that the <mathjax>$Q$</mathjax> value might be wrong not only because of the uncertainty in the estimate (e.g. because the policy evaluation isn't good yet) but also because the policy may still have a lot of improving to do.</p>
<p>You can also extend the general "optimism in the face of uncertainty" approach by replacing the reward for unknown or poorly estimated states with <mathjax>$r_max$</mathjax>, then just solve the MDP with some planning algorithm (e.g. policy iteration, value iteration, tree search, etc). An example of this is the <em>Rmax algorithm</em>.</p>
<p>The information state space approach can also be extended to MDPs. Here, the augmented state is now <mathjax>$\tilde S = (S,I)$</mathjax> where <mathjax>$S$</mathjax> is the original state within the MDP and <mathjax>$I$</mathjax> is the accumulated information. The Bayes-adaptive approach maintains a posterior model corresponding to each augmented state, <mathjax>$\mathbb P[\mathcal P, \mathcal R|I]$</mathjax>. Solving the Bayes-adaptive MDP finds the optimal exploration/exploitation trade-off with respect to prior, so this is the "correct" approach. Like before, the augmented MDP is typically enormous, but Monte-Carlo tree search has proven effective here.</p>
    
    <script src="http://cdn.bootcss.com/jquery/1.11.1/jquery.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script>
        $(document).ready(function() {
            $('pre').each(function(i, e) {hljs.highlightBlock(e)});
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$","$"]],
                    displayMath: [['$$','$$']],
                    processEscapes: true
                },
                "HTML-CSS": {
                    linebreaks: { automatic: true }
                }
            });
            MathJax.Hub.Startup.onload();
        });
    </script>
    <script src="http://ai-code.tech/ai_notes_html/js/custom.js"></script>


</body>
</html>
