
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <title>intro_to_rnn</title>

    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/color-brewer.min.css">
    <link rel="stylesheet" href="http://ai-code.tech/ai_notes_html/css/custom.css">
</head>

<body>

    <p>from: <a href="http://danijar.github.io/introduction-to-recurrent-networks-in-tensorflow">http://danijar.github.io/introduction-to-recurrent-networks-in-tensorflow</a><br />
by Danijar Hafner</p>
<h1>Introduction to Recurrent Networks in TensorFlow</h1>
<p>May 5, 2016</p>
<p>Recurrent networks like LSTM and GRU are powerful sequence models. I will explain how to create recurrent networks in TensorFlow and use them for sequence classification and sequence labelling tasks. If you are not familiar with recurrent networks, I suggest you take a look at Christopher Olah's <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">great post</a> first. On the TensorFlow part, I also expect some basic knowledge. The <a href="https://www.tensorflow.org/versions/r0.8/tutorials/index.html">official tutorials</a> are a good place to start.</p>
<h2>Defining the Network</h2>
<p>To use recurrent networks in TensorFlow we first need to define the network architecture consiting of one or more layers, the cell type and possibly dropout between the layers.</p>
<pre><code>from tensorflow.models.rnn import rnn_cell

num_hidden = 200
num_layers = 3
dropout = tf.placeholder(tf.float32)

network = rnn_cell.GRUCell(num_hidden)  # Or LSTMCell(num_hidden)
network = rnn_cell.DropoutWrapper(network, output_keep_prob=dropout)
network = rnn_cell.MultiRNNCell([network] * num_layers)
</code></pre>
<h2>Unrolling in Time</h2>
<p>We can now unroll this network in time using the <code>rnn</code> operation. This takes placeholders for the input at each timestep and returns the hidden <em>states</em> and <em>output</em> activations for each timestep.</p>
<pre><code>from tensorflow.models.rnn import rnn
max_length = 100

# Batch size times time steps times data width.
data = tf.placeholder(tf.float32, [None, max_length, 28])
outputs, states = rnn.rnn(network, unpack_sequence(data))
output = pack_sequence(outputs)
state = pack_sequence(states)
</code></pre>
<p>TensorFlow uses Python lists of one tensor for each timestep for the interface. Thus we make use of <a href="https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#pack"><code>tf.pack()</code> and <code>tf.unpack()</code></a> to split our data tensors into lists of frames and merge the results back to a single tensor.</p>
<pre><code>def unpack_sequence(tensor):
    """Split the single tensor of a sequence into a list of frames."""
    return tf.unpack(tf.transpose(tensor, perm=[1, 0, 2]))

def pack_sequence(sequence):
    """Combine a list of the frames into a single tensor of the sequence."""
    return tf.transpose(tf.pack(sequence), perm=[1, 0, 2])
</code></pre>
<h2>Sequence Classification</h2>
<p>For classification, you might only care about the output activation at the last timestep, which is just <code>outputs[-1]</code>. For now we assume sequences to be equal in length but I will cover variable length sequences in another post.</p>
<pre><code>in_size = num_hidden
out_size = int(target.get_shape()[2])
weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))
bias = tf.Variable(tf.constant(0.1, shape=[out_size]))
prediction = tf.nn.softmax(tf.matmul(outputs[-1], weight) + bias)
</code></pre>
<p>The code just adds a softmax layer ontop of the recurrent network that tries to predict the target from the last RNN activation.</p>
<pre><code>cross_entropy = -tf.reduce_sum(target * tf.log(prediction))
</code></pre>
<h2>Sequence Labelling</h2>
<p>For sequence labelling, we want a prediction for each timestamp. However, we share the weights for the softmax layer across all timesteps. This way, we have one softmax layer ontop of an unrolled recurrent network as desired.</p>
<pre><code>in_size = num_hidden
out_size = int(target.get_shape()[2])
weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))
bias = tf.Variable(tf.constant(0.1, shape=[out_size]))
predictions = [tf.nn.softmax(tf.matmul(x, weight) + bias) for x in outputs]
prediction = pack_sequence(predictions)
</code></pre>
<p>Since this also is a classification task, we keep using cross entropy. We first compute the cross entropy for every timestep and then average.</p>
<pre><code>cross_entropy = -tf.reduce_sum(
    target * tf.log(prediction), reduction_indices=[1])
cross_entropy = tf.reduce_mean(cross_entropy)
</code></pre>
<p>We learned how to construct recurrent networks in TensorFlow and use them for sequence learning tasks. Please ask any questions below if you couldn't follow.</p>
    
    <script src="http://cdn.bootcss.com/jquery/1.11.1/jquery.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script src="http://ai-code.tech/ai_notes_html/js/custom.js"></script>
    <script>
        $(document).ready(function() {
            $('pre').each(function(i, e) {hljs.highlightBlock(e)});
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$","$"]],
                    displayMath: [['$$','$$']],
                    processEscapes: true
                },
                "HTML-CSS": {
                    linebreaks: { automatic: true }
                }
            });
            MathJax.Hub.Startup.onload();
        });
    </script>


</body>
</html>
