
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <title>_stash</title>

    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/color-brewer.min.css">
    <link rel="stylesheet" href="http://ai-code.tech/ai_notes_html/css/custom.css">
</head>

<body>

    <h1>Things that have been edited out but may still be useful elsewhere</h1>
<hr />
<h2>Decision Theory</h2>
<p><strong>Decision theory</strong> is a framework for which the general goal is to <em>minimize expected loss</em>. In machine learning, minimizing loss is the typical optimization problem.</p>
<p>Given labeled data points <mathjax>$(x_1, y_1), \dots, (x_n, y_n)$</mathjax> and some <strong>loss function</strong> <mathjax>$L(y, \hat y)$</mathjax>, in which <mathjax>$y$</mathjax> is the true label and <mathjax>$\hat y$</mathjax> is the predicted label, we want to choose some function <mathjax>$f(x) = \hat y$</mathjax> (i.e. yields a predicted label or value for a given <mathjax>$x$</mathjax>) that minimizes <mathjax>$L(y, f(x))$</mathjax>.</p>
<p>But because we don't know <mathjax>$x$</mathjax> or <mathjax>$y$</mathjax> we want to minimize the average loss, that is, the <strong>expected loss</strong>:</p>
<p><mathjax>$$
E[L(Y, \hat y) | X = x] = \sum_{y \in Y} L(y, \hat y) P(y|x)
$$</mathjax></p>
<p>Thus our goal can be stated as:</p>
<p><mathjax>$$
\hat y = \argmin_y E[L(Y, \hat y) | X = x] = \argmax_y P(y|x)
$$</mathjax></p>
<p>That is, choose <mathjax>$\hat y$</mathjax> to minimize the expected loss, which is the same as the <mathjax>$y$</mathjax> which is maximally probably given <mathjax>$x$</mathjax>.</p>
<p>An example loss function is the <em>square loss</em>, which is used in regression:</p>
<p><mathjax>$$
L(y, \hat y) = (y - \hat y)^2
$$</mathjax></p>
<hr />
<h5>The Cauchy-Schwarz Inequality</h5>
<p>For two non-zero vectors, <mathjax>$\vec{x}, \vec{y} \in \mathbb R^n$</mathjax>, the following are true:</p>
<ul>
<li>The absolute value of their dot product is less than or equal to the product of their<br />
lengths: <mathjax>$|\vec{x} \cdot \vec{y}| \le ||\vec{x}|| \times ||\vec{y}||$</mathjax></li>
<li>The absolute value of their dot product will be equal to the product of their lengths<br />
only if they are collinear (i.e. if <mathjax>$\vec{x} = c\vec{y}$</mathjax>): <mathjax>$|\vec{x} \cdot \vec{y}| = ||\vec{x}|| \times ||\vec{y}||$</mathjax></li>
</ul>
<hr />
<h5>Intuition of dot products</h5>
<p>The dot product between two vectors tells you how much in the same direction they're<br />
going. So it is minimized when they are orthogonal, and maximized when they are<br />
collinear.</p>
<hr />
<h4>Intuition of cross products</h4>
<p>Cross products are kind of the opposite of dot products - the cross product is minimized<br />
when the vectors are collinear and maximized when they are orthogonal.</p>
<h4>The <mathjax>$sin$</mathjax> of the vectors' angle</h4>
<p>A property of the cross product is:</p>
<p><mathjax>$$ || \vec{a} \times \vec{b} || = || \vec{a} || || \vec{b} || sin\theta $$</mathjax></p>
<p>where <mathjax>$\theta$</mathjax> is the angle between <mathjax>$\vec{a}, \vec{b}$</mathjax>.</p>
<hr />
<h2>How likely is something to happen?</h2>
<p>If something has a 1-in-<mathjax>$m$</mathjax> chance of happening, does its likelihood increase the more trials there are? That is, how likely is it happen to at least once over <mathjax>$n$</mathjax> trials?</p>
<p>You certainly aren't guaranteed that it <em>will</em> happen in <mathjax>$m$</mathjax> trials, although the phrase "1-in-<mathjax>$m$</mathjax>" makes it sound that way. Think about flipping a coin - there is a 1-in-2 chance it will be heads, but you have seen that it is possible to get two heads in a row or two tails in a row.</p>
<p>A better question to ask is "How likely is this to <em>never</em> happen"? Then we can calculate <mathjax>$P(\text{never})$</mathjax> and calculate <mathjax>$P(\text{atLeastOnce}) = 1 - P(\text{never})$</mathjax>.</p>
<p>With the coin example, let's say we want to know how likely a heads flip will be. <mathjax>$P(\text{heads}) = 1/2$</mathjax> so <mathjax>$P(\text{heads}') = 1 - P(\text{heads})$</mathjax> ^[Note that <mathjax>$P(X')$</mathjax> is the probability of <mathjax>$X$</mathjax> <em>not</em> happening, also denoted <mathjax>$P(\bar X)$</mathjax>.].</p>
<p>So say we have two trials (<mathjax>$n = 2$</mathjax>). What is the probability that we <em>never</em> get a heads? It's just:</p>
<p><mathjax>$$ (1 - 1/2)(1 - 1/2) = (1 - 1/2)^2 = 0.25 $$</mathjax></p>
<p>We can generalize this to:</p>
<p><mathjax>$$ P(\text{never}) = (1 - \frac{1}{m})^n $$</mathjax></p>
<p>As <mathjax>$n$</mathjax> grows larger, <mathjax>$P(never)$</mathjax> grows smaller - but it never becomes 0. In fact, it approaches 0.37^[This happens to be <mathjax>$1/e$</mathjax> (one over Euler's number).] as depicted in the accompanying figure.</p>
<figure><img alt="$P(\text{never})$" src="../assets/pnever.svg" /><figcaption><mathjax>$P(\text{never})$</mathjax></figcaption>
</figure>
<p>So the probability of the event happening at least once in these 1-in-<mathjax>$m$</mathjax> scenarios is around 0.63.</p>
<ul>
<li><a href="http://www.countbayesie.com/blog/2015/2/18/one-in-a-million-and-e">http://www.countbayesie.com/blog/2015/2/18/one-in-a-million-and-e</a></li>
</ul>
<h2>Scale-free networks</h2>
<p>A network is be scale-free if its distribution of degrees is a scale-free distribution.</p>
<hr />
<p><mathjax>$$
\DeclareMathOperator{\SST}{SST}
\DeclareMathOperator{\SSW}{SSW}
\DeclareMathOperator{\SSB}{SSB}
$$</mathjax></p>
<hr />
<h2>Sum of Squares</h2>
<h3>The sum of squares within (SSW)</h3>
<p><mathjax>$$ \SSW = \sum_{i=1}^m (\sum{j=1}^n (x_{ij} - \bar{x_i})^2) $$</mathjax></p>
<ul>
<li>This shows how much of SST is due to variation <em>within</em> each group, i.e. variation from within that group's mean.</li>
<li>The degrees of freedom here is calculated <mathjax>$m(n-1)$</mathjax>.</li>
</ul>
<h3>The sum of squares between (SSB)</h3>
<p><mathjax>$$ \SSB = \sum_{i=1}^m [n_m [(\bar{x_m} - \bar{\bar{x}})^2]] $$</mathjax></p>
<ul>
<li>This shows how much of SST is due to variation between the group means</li>
<li>The degrees of freedom here is calculated <mathjax>$m-1$</mathjax>.</li>
</ul>
<h3>The total sum of squares (SST)</h3>
<p><mathjax>$$ \SST = \sum_{i=1}^m (\sum{j=1}^n (x_{ij} - \bar{\bar{x}})^2) $$</mathjax><br />
<mathjax>$$ \SST = \SSW + \SSB $$</mathjax></p>
<ul>
<li>Note: <mathjax>$\bar{\bar{x}}$</mathjax> is the mean of means, or the "grand mean".</li>
<li>This is the total variation for the groups</li>
<li>The degrees of freedom here is calculated <mathjax>$mn - 1$</mathjax>.</li>
</ul>
    
    <script src="http://cdn.bootcss.com/jquery/1.11.1/jquery.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script>
        $(document).ready(function() {
            $('pre').each(function(i, e) {hljs.highlightBlock(e)});
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$","$"]],
                    displayMath: [['$$','$$']],
                    processEscapes: true
                },
                "HTML-CSS": {
                    linebreaks: { automatic: true }
                }
            });
            MathJax.Hub.Startup.onload();
        });
    </script>
    <script src="http://ai-code.tech/ai_notes_html/js/custom.js"></script>


</body>
</html>
