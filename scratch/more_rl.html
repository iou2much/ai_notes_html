
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <title>more_rl</title>

    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/color-brewer.min.css">
    <link rel="stylesheet" href="http://ai-code.tech/ai_notes_html/css/custom.css">
</head>

<body>

    <p><a href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning: Pong from Pixels</a>. Andrej Karpathy. May 31, 2016.</p>
<p>Q-learning, and thus DQN, are is outperformed by <em>Policy Gradients</em>.</p>
<p>The policy we learn with policy gradients is a stochastic policy (i.e. it returns a probability distribution over actions, from which we draw to decide an action). This policy is learned via a <em>policy network</em>, i.e. a neural network. The policy network can be notated <mathjax>$p(a|s)$</mathjax>, i.e. it gives a distribution over actions given a state.</p>
<p>When working with data that has a temporal component (e.g. frames in a video game), you may want to feed frames in two at a time to capture motion, or use <em>difference frames</em> which are formed by subtracting the current frame from the last frame.</p>
<p>Basically the way policy gradients work is you get your action distribution, sample an action from there, and execute that action. When you (perhaps eventually) get a reward, you set that reward scalar as the gradient for the action you took. This sets the gradient ti discourage actions with negative reward and encourage actions with positive reward. Then we carry out backprop like we usually would.</p>
<p>Say you are training your agent to play a game. Each playthrough of the game is called a <em>policy rollout</em>. Say you do 100 rollouts. Your agent wins 12 games and loses 88. We take all decisions made (i.e. actions taken) in the winning games and so a <em>positive update</em> (fill in their gradients with the positive reward, run backprop, then update parameters). Then we take all the decisions in the losing games and do a <em>negative update</em> (fill in their gradients with the negative reward, run backprop, update parameters).</p>
<p>Then you do another set of rollouts, perform the updates again, and so on.</p>
<p>With games we typically only assign rewards to the terminal states (i.e. when the game is won or lost). More generally we may have reward <mathjax>$r_t$</mathjax> at every time step <mathjax>$t$</mathjax>. Here we might instead use a discounted reward, i.e. <mathjax>$R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}$</mathjax>. You may want to standardize these rewards (e.g. subtract mean, divide by standard deviation) before using them for backprop.</p>
<p>Under the hood, this basically learns a probability distribution <mathjax>$p(a|s)$</mathjax> so that samples from the distribution maximize the reward function <mathjax>$r(a,s)$</mathjax>.</p>
<hr />
<p>Good overview of shortcomings of policy gradients (in comparison to how humans learn):</p>
<blockquote>
<ul>
<li>In practical settings we usually communicate the task in some manner (e.g. English above), but in a standard RL problem you assume an arbitrary reward function that you have to discover through environment interactions. It can be argued that if a human went into game of Pong but without knowing anything about the reward function (indeed, especially if the reward function was some static but random function), the human would have a lot of difficulty learning what to do but Policy Gradients would be indifferent, and likely work much better. Similarly, if we took the frames and permuted the pixels randomly then humans would likely fail, but our Policy Gradient solution could not even tell the difference (if it’s using a fully connected network as done here).</li>
<li>A human brings in a huge amount of prior knowledge, such as intuitive physics (the ball bounces, it’s unlikely to teleport, it’s unlikely to suddenly stop, it maintains a constant velocity, etc.), and intuitive psychology (the AI opponent “wants” to win, is likely following an obvious strategy of moving towards the ball, etc.). You also understand the concept of being “in control” of a paddle, and that it responds to your UP/DOWN key commands. In contrast, our algorithms start from scratch which is simultaneously impressive (because it works) and depressing (because we lack concrete ideas for how not to).</li>
<li>Policy Gradients are a brute force solution, where the correct actions are eventually discovered and internalized into a policy. Humans build a rich, abstract model and plan within it. In Pong, I can reason that the opponent is quite slow so it might be a good strategy to bounce the ball with high vertical velocity, which would cause the opponent to not catch it in time. However, it also feels as though we also eventually “internalize” good solutions into what feels more like a reactive muscle memory policy. For example if you’re learning a new motor task (e.g. driving a car with stick shift?) you often feel yourself thinking a lot in the beginning but eventually the task becomes automatic and mindless.</li>
<li>Policy Gradients have to actually experience a positive reward, and experience it very often in order to eventually and slowly shift the policy parameters towards repeating moves that give high rewards. With our abstract model, humans can figure out what is likely to give rewards without ever actually experiencing the rewarding or unrewarding transition. I don’t have to actually experience crashing my car into a wall a few hundred times before I slowly start avoiding to do so.</li>
</ul>
</blockquote>
<p>Some other advice:</p>
<blockquote>
<p>In the case of Reinforcement Learning for example, one strong baseline that should always be tried first is the <a href="https://en.wikipedia.org/wiki/Cross-entropy_method">cross-entropy method (CEM)</a>, a simple stochastic hill-climbing “guess and check” approach inspired loosely by evolution. And if you insist on trying out Policy Gradients for your problem make sure you pay close attention to the tricks section in papers, start simple first, and use a variation of PG called <a href="https://arxiv.org/abs/1502.05477">TRPO</a>, which almost always works better and more consistently than vanilla PG <a href="http://arxiv.org/abs/1604.06778">in practice</a>. The core idea is to avoid parameter updates that change your policy too much, as enforced by a constraint on the KL divergence between the distributions predicted by the old and the new policy on a batch of data (instead of conjugate gradients the simplest instantiation of this idea could be implemented by doing a line search and checking the KL along the way).</p>
</blockquote>
<hr />
<p><a href="https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5">numpy implementation</a>:</p>
<div class="highlight"><pre><span class="sd">&quot;&quot;&quot; Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. &quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cPickle</span> <span class="kn">as</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="c"># hyperparameters</span>
<span class="n">H</span> <span class="o">=</span> <span class="mi">200</span> <span class="c"># number of hidden layer neurons</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="c"># every how many episodes to do a param update?</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span> <span class="c"># discount factor for reward</span>
<span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.99</span> <span class="c"># decay factor for RMSProp leaky sum of grad^2</span>
<span class="n">resume</span> <span class="o">=</span> <span class="bp">False</span> <span class="c"># resume from previous checkpoint?</span>
<span class="n">render</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c"># model initialization</span>
<span class="n">D</span> <span class="o">=</span> <span class="mi">80</span> <span class="o">*</span> <span class="mi">80</span> <span class="c"># input dimensionality: 80x80 grid</span>
<span class="k">if</span> <span class="n">resume</span><span class="p">:</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">&#39;save.p&#39;</span><span class="p">,</span> <span class="s">&#39;rb&#39;</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">model</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">model</span><span class="p">[</span><span class="s">&#39;W1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">)</span> <span class="c"># &quot;Xavier&quot; initialization</span>
  <span class="n">model</span><span class="p">[</span><span class="s">&#39;W2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>

<span class="n">grad_buffer</span> <span class="o">=</span> <span class="p">{</span> <span class="n">k</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iteritems</span><span class="p">()</span> <span class="p">}</span> <span class="c"># update buffers that add up gradients over a batch</span>
<span class="n">rmsprop_cache</span> <span class="o">=</span> <span class="p">{</span> <span class="n">k</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iteritems</span><span class="p">()</span> <span class="p">}</span> <span class="c"># rmsprop memory</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c"># sigmoid &quot;squashing&quot; function to interval [0,1]</span>

<span class="k">def</span> <span class="nf">prepro</span><span class="p">(</span><span class="n">I</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector &quot;&quot;&quot;</span>
  <span class="n">I</span> <span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="mi">35</span><span class="p">:</span><span class="mi">195</span><span class="p">]</span> <span class="c"># crop</span>
  <span class="n">I</span> <span class="o">=</span> <span class="n">I</span><span class="p">[::</span><span class="mi">2</span><span class="p">,::</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="c"># downsample by factor of 2</span>
  <span class="n">I</span><span class="p">[</span><span class="n">I</span> <span class="o">==</span> <span class="mi">144</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># erase background (background type 1)</span>
  <span class="n">I</span><span class="p">[</span><span class="n">I</span> <span class="o">==</span> <span class="mi">109</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># erase background (background type 2)</span>
  <span class="n">I</span><span class="p">[</span><span class="n">I</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># everything else (paddles, ball) just set to 1</span>
  <span class="k">return</span> <span class="n">I</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">discount_rewards</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; take 1D float array of rewards and compute discounted reward &quot;&quot;&quot;</span>
  <span class="n">discounted_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
  <span class="n">running_add</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">size</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span> <span class="n">running_add</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># reset the sum, since this was a game boundary (pong specific!)</span>
    <span class="n">running_add</span> <span class="o">=</span> <span class="n">running_add</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">discounted_r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_add</span>
  <span class="k">return</span> <span class="n">discounted_r</span>

<span class="k">def</span> <span class="nf">policy_forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s">&#39;W1&#39;</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">h</span><span class="p">[</span><span class="n">h</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># ReLU nonlinearity</span>
  <span class="n">logp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s">&#39;W2&#39;</span><span class="p">],</span> <span class="n">h</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">logp</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p</span><span class="p">,</span> <span class="n">h</span> <span class="c"># return probability of taking action 2, and hidden state</span>

<span class="k">def</span> <span class="nf">policy_backward</span><span class="p">(</span><span class="n">eph</span><span class="p">,</span> <span class="n">epdlogp</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; backward pass. (eph is array of intermediate hidden states) &quot;&quot;&quot;</span>
  <span class="n">dW2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eph</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">epdlogp</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
  <span class="n">dh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">epdlogp</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="s">&#39;W2&#39;</span><span class="p">])</span>
  <span class="n">dh</span><span class="p">[</span><span class="n">eph</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># backpro prelu</span>
  <span class="n">dW1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dh</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">epx</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">{</span><span class="s">&#39;W1&#39;</span><span class="p">:</span><span class="n">dW1</span><span class="p">,</span> <span class="s">&#39;W2&#39;</span><span class="p">:</span><span class="n">dW2</span><span class="p">}</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">&quot;Pong-v0&quot;</span><span class="p">)</span>
<span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">prev_x</span> <span class="o">=</span> <span class="bp">None</span> <span class="c"># used in computing the difference frame</span>
<span class="n">xs</span><span class="p">,</span><span class="n">hs</span><span class="p">,</span><span class="n">dlogps</span><span class="p">,</span><span class="n">drs</span> <span class="o">=</span> <span class="p">[],[],[],[]</span>
<span class="n">running_reward</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">reward_sum</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">episode_number</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">render</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

  <span class="c"># preprocess the observation, set input to network to be difference image</span>
  <span class="n">cur_x</span> <span class="o">=</span> <span class="n">prepro</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">cur_x</span> <span class="o">-</span> <span class="n">prev_x</span> <span class="k">if</span> <span class="n">prev_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
  <span class="n">prev_x</span> <span class="o">=</span> <span class="n">cur_x</span>

  <span class="c"># forward the policy network and sample an action from the returned probability</span>
  <span class="n">aprob</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">policy_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">action</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">aprob</span> <span class="k">else</span> <span class="mi">3</span> <span class="c"># roll the dice!</span>

  <span class="c"># record various intermediates (needed later for backprop)</span>
  <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># observation</span>
  <span class="n">hs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c"># hidden state</span>
  <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">0</span> <span class="c"># a &quot;fake label&quot;</span>
  <span class="n">dlogps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">aprob</span><span class="p">)</span> <span class="c"># grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)</span>

  <span class="c"># step the environment and get new measurements</span>
  <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
  <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>

  <span class="n">drs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span> <span class="c"># record reward (has to be done after we call step() to get reward for previous action)</span>

  <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="c"># an episode finished</span>
    <span class="n">episode_number</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c"># stack together all inputs, hidden states, action gradients, and rewards for this episode</span>
    <span class="n">epx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="n">eph</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">hs</span><span class="p">)</span>
    <span class="n">epdlogp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">dlogps</span><span class="p">)</span>
    <span class="n">epr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">drs</span><span class="p">)</span>
    <span class="n">xs</span><span class="p">,</span><span class="n">hs</span><span class="p">,</span><span class="n">dlogps</span><span class="p">,</span><span class="n">drs</span> <span class="o">=</span> <span class="p">[],[],[],[]</span> <span class="c"># reset array memory</span>

    <span class="c"># compute the discounted reward backwards through time</span>
    <span class="n">discounted_epr</span> <span class="o">=</span> <span class="n">discount_rewards</span><span class="p">(</span><span class="n">epr</span><span class="p">)</span>
    <span class="c"># standardize the rewards to be unit normal (helps control the gradient estimator variance)</span>
    <span class="n">discounted_epr</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">discounted_epr</span><span class="p">)</span>
    <span class="n">discounted_epr</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">discounted_epr</span><span class="p">)</span>

    <span class="n">epdlogp</span> <span class="o">*=</span> <span class="n">discounted_epr</span> <span class="c"># modulate the gradient with advantage (PG magic happens right here.)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">policy_backward</span><span class="p">(</span><span class="n">eph</span><span class="p">,</span> <span class="n">epdlogp</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">model</span><span class="p">:</span> <span class="n">grad_buffer</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grad</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="c"># accumulate grad over batch</span>

    <span class="c"># perform rmsprop parameter update every batch_size episodes</span>
    <span class="k">if</span> <span class="n">episode_number</span> <span class="o">%</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">grad_buffer</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="c"># gradient</span>
        <span class="n">rmsprop_cache</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">rmsprop_cache</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">model</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">rmsprop_cache</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
        <span class="n">grad_buffer</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="c"># reset batch gradient buffer</span>

    <span class="c"># boring book-keeping</span>
    <span class="n">running_reward</span> <span class="o">=</span> <span class="n">reward_sum</span> <span class="k">if</span> <span class="n">running_reward</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">running_reward</span> <span class="o">*</span> <span class="mf">0.99</span> <span class="o">+</span> <span class="n">reward_sum</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="k">print</span> <span class="s">&#39;resetting env. episode reward total was </span><span class="si">%f</span><span class="s">. running mean: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">reward_sum</span><span class="p">,</span> <span class="n">running_reward</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">episode_number</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;save.p&#39;</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">))</span>
    <span class="n">reward_sum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span> <span class="c"># reset env</span>
    <span class="n">prev_x</span> <span class="o">=</span> <span class="bp">None</span>

  <span class="k">if</span> <span class="n">reward</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span> <span class="c"># Pong has either +1 or -1 reward exactly when game ends.</span>
</pre></div>
    
    <script src="http://cdn.bootcss.com/jquery/1.11.1/jquery.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script src="http://ai-code.tech/ai_notes_html/js/custom.js"></script>
    <script>
        $(document).ready(function() {
            $('pre').each(function(i, e) {hljs.highlightBlock(e)});
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$","$"]],
                    displayMath: [['$$','$$']],
                    processEscapes: true
                },
                "HTML-CSS": {
                    linebreaks: { automatic: true }
                }
            });
            MathJax.Hub.Startup.onload();
        });
    </script>


</body>
</html>
