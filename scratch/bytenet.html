
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <title>bytenet</title>

    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/color-brewer.min.css">
</head>

<body>

    <h1>Neural Machine Translation in Linear Time. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu.</h1>
<p><a href="https://arxiv.org/pdf/1610.10099v1.pdf">https://arxiv.org/pdf/1610.10099v1.pdf</a></p>
<p>"ByteNet" architecture for neural machine translation which translates in linear time and can handle dependencies over large distances. It consists of two networks, a source (encoder) and a target (decoder).</p>
<p>The source network is formed of one-dimensional convolutional layers that use dilation.</p>
<p>The target network (the "ByteNet Decoder") is formed of one-dimensional convolutional layers that user dilation and are masked. It is "stacked" on the source network and generates variable-length outputs via "dynamic unfolding".</p>
<h2>Dynamic unfolding</h2>
<p>The representation generated by the source network has the same length as the source sequence.</p>
<p>At each step, the target network takes the corresponding column from the source representation and generates an output. This continues until an end-of-sequence (EOS) symbol is produced by the target network. The source representation is automatically zero-padded as the steps go beyond its length and the output is conditioned on the source and target representations accumulated thus far.</p>
<hr />
<h2>Dilated convolutions</h2>
<p><em>Dilated convolutions</em> (also called <em>à trous</em>, "with holes") are a way of integrating knowledge of a larger area (i.e. the global context of an image) while only linearly increasing the number of parameters.</p>
<p>For a dilation size <mathjax>$d$</mathjax>, the kernel is subsampled every <mathjax>$d+1$</mathjax> pixels, so a smaller kernel is "stretched" over a larger area. In this way the receptive field of units grow exponentially across layers, so you require less layers (and parameters) to account for larger contexts. The dilation size <mathjax>$d$</mathjax> is doubled across layers (see the WaveNet graphic below).</p>
<p>It is like pooling or strided convolutions but results in an output with the same size as the input.</p>
<figure><img alt="from &lt;http://mathematica.stackexchange.com/questions/125971/can-someone-explain-how-the-dilation-in-the-convolutionallayer-works&gt;" src="../assets/dilation.png" /><figcaption>from <a href="http://mathematica.stackexchange.com/questions/125971/can-someone-explain-how-the-dilation-in-the-convolutionallayer-works">http://mathematica.stackexchange.com/questions/125971/can-someone-explain-how-the-dilation-in-the-convolutionallayer-works</a></figcaption>
</figure>
<figure><img alt="stacked 1D dilated convolutional layers, from the WaveNet paper" src="../assets/wavenet.png" /><figcaption>stacked 1D dilated convolutional layers, from the WaveNet paper</figcaption>
</figure>
<h3>References</h3>
<ul>
<li><a href="http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/">Dilated Convolutions and Kronecker Factored Convolutions</a>. Ferenc Huszár.</li>
<li><a href="http://mathematica.stackexchange.com/questions/125971/can-someone-explain-how-the-dilation-in-the-convolutionallayer-works">http://mathematica.stackexchange.com/questions/125971/can-someone-explain-how-the-dilation-in-the-convolutionallayer-works</a></li>
<li><a href="https://arxiv.org/pdf/1609.03499v2.pdf">WaveNet: A Generative Model for Raw Audio</a>. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu.</li>
</ul>
<hr />
<h2>Deep Residual Networks (ResNets)</h2>
<p>A <em>deep residual network</em> takes a convolutional neural network and adds shortcuts which take the input of an earlier layer and pass it directly to a later node (i.e. addition) of the network. The unit containing this shortcut is a "residual block". The result is that the other layer feeding into the addition node (in the example below, it is the second convolution layer) just adds a "residual" to the repeated input.</p>
<figure><img alt="A residual block" src="../assets/resnets.png" /><figcaption>A residual block</figcaption>
</figure>
<p>This can help with the vanishing gradient problem that occurs in deep networks.</p>
<p>Another example of a residual block, from <a href="http://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>:</p>
<figure><img alt="" src="../assets/residual_block.png" /><figcaption></figcaption>
</figure>
<h3>References</h3>
<ul>
<li><a href="http://torch.ch/blog/2016/02/04/resnets.html">Training and investigating Residual Nets</a>. Sam Gross and Michael Wilber.</li>
</ul>
    
    <script src="http://cdn.bootcss.com/jquery/1.11.1/jquery.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script>
        $(document).ready(function() {
            $('pre').each(function(i, e) {hljs.highlightBlock(e)});
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$","$"]],
                    displayMath: [['$$','$$']],
                    processEscapes: true
                },
                "HTML-CSS": {
                    linebreaks: { automatic: true }
                }
            });
            MathJax.Hub.Startup.onload();
        });
    </script>


</body>
</html>
